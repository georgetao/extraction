{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outside imports\n",
    "import os\n",
    "import importlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import train\n",
    "import evaluate\n",
    "import train_util\n",
    "import data_util.data\n",
    "import data_util.batcher\n",
    "import data_util.config\n",
    "import data_util.preprocess\n",
    "\n",
    "importlib.reload(data_util.preprocess)\n",
    "importlib.reload(train)\n",
    "importlib.reload(model)\n",
    "importlib.reload(evaluate)\n",
    "importlib.reload(train_util)\n",
    "importlib.reload(data_util.config)\n",
    "importlib.reload(data_util.data)\n",
    "importlib.reload(data_util.batcher)\n",
    "\n",
    "from train import *\n",
    "from evaluate import *\n",
    "from model import *\n",
    "from train_util import *\n",
    "from data_util.data import *\n",
    "from data_util.batcher import *\n",
    "from data_util.preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_train = pd.read_csv(os.path.join(config.sum_path, 'task_train.tsv'), sep = '\\t')\n",
    "wiki_train = pd.read_csv(os.path.join(config.sum_path, 'wiki_train.tsv'), sep = '\\t')\n",
    "wiki_val   = pd.read_csv(os.path.join(config.sum_path, 'wiki_val.tsv'  ), sep = '\\t')\n",
    "task_val   = pd.read_csv(os.path.join(config.sum_path, 'task_val.tsv'  ), sep = '\\t')\n",
    "task_test  = pd.read_csv(os.path.join(config.sum_path, 'task_test.tsv' ), sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NAs with ''\n",
    "for df in [task_train, wiki_train, wiki_val, task_val]:\n",
    "    df.replace(np.nan, '', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3235, 3)\n",
      "(917986, 3)\n",
      "(300, 3)\n",
      "(200, 3)\n",
      "(800, 3)\n"
     ]
    }
   ],
   "source": [
    "# check sizes\n",
    "print(task_train.shape)\n",
    "print(wiki_train.shape)\n",
    "print(wiki_val.shape)\n",
    "print(task_val.shape)\n",
    "print(task_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Duplicated word in vocabulary file: \"\n",
      "Finished constructing vocabulary of 43861 total words. Last word added: jgsm\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary\n",
    "vocab = Vocab(os.path.join(config.vocab_path, 'vocab3.txt'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<data_util.data.Vocab at 0x126130590>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument namespace\n",
    "opt = Namespace(\n",
    "    train_mle = \"yes\", \n",
    "    train_rl = \"no\", \n",
    "    mle_weight = 1., \n",
    "    load_model = None,\n",
    "    new_lr = None, \n",
    "    rl_weight = 0.)\n",
    "\n",
    "# create train and val batchers\n",
    "wiki_batcher = TaskBatcher(\n",
    "    examples=wiki_train.to_dict('records'),\n",
    "    vocab=vocab,\n",
    "    mode='train',\n",
    "    batch_size=16,\n",
    "    single_pass=False\n",
    ")\n",
    "wiki_val_batcher = TaskBatcher( # Batching obj\n",
    "    examples=wiki_val.to_dict('records')[:200],\n",
    "    vocab=vocab, \n",
    "    mode='train', \n",
    "    batch_size=50, \n",
    "    single_pass=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_processor = TaskTrain(vocab, task_batcher, opt, TaskModel, val_task_batcher)\n",
    "wiki_trainer = TaskTrain(vocab, wiki_batcher, opt, TaskModel, wiki_val_batcher)\n",
    "# load pre-trained embedding weights\n",
    "wiki_trainer.model.load_embeddings(\"embedding_43861_200.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1 mle_loss: 8.752 mle_loss_val: -100.0000\n",
      "iter: 2 mle_loss: 8.161 mle_loss_val: -100.0000\n",
      "-------------------Keyboard Interrupt------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/capstone/LSTM_Summarizer/train.py\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(self, n_iters, report_every, save_every)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mmle_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/capstone/LSTM_Summarizer/train.py\u001b[0m in \u001b[0;36mtrain_one_batch\u001b[0;34m(self, batch, iter, no_grad)\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmle_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmle_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrl_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5e4da2398cde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmle_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwiki_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/capstone/LSTM_Summarizer/train.py\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(self, n_iters, report_every, save_every)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------Keyboard Interrupt------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m                 \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mmle_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmle_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "# train\n",
    "mle_losses = wiki_trainer.trainIters(n_iters=15, report_every=1, save_every = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [(1, 8.751328468322754),\n",
       "  (2, 8.145495414733887),\n",
       "  (3, 8.132192611694336),\n",
       "  (4, 7.853020668029785),\n",
       "  (5, 7.729152679443359),\n",
       "  (6, 8.067704200744629),\n",
       "  (7, 7.242686748504639),\n",
       "  (8, 7.274189472198486),\n",
       "  (9, 7.066506385803223),\n",
       "  (10, 6.160511016845703),\n",
       "  (11, 5.582155227661133),\n",
       "  (12, 6.152600288391113),\n",
       "  (13, 6.235994338989258),\n",
       "  (14, 6.4322710037231445),\n",
       "  (15, 5.812689781188965),\n",
       "  (16, 6.197237968444824)],\n",
       " 'val': [(5, 7.81031060218811),\n",
       "  (10, 5.992038726806641),\n",
       "  (15, 5.8313528299331665)]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune the Model to Task data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model at data/saved_models/0000015.tar\n"
     ]
    }
   ],
   "source": [
    "# Retrain namespace\n",
    "opt_retrain = Namespace(\n",
    "    train_mle = \"yes\", \n",
    "    train_rl = \"no\", \n",
    "    mle_weight = 1., \n",
    "    load_model = '0000015.tar',\n",
    "    new_lr = None, \n",
    "    rl_weight = 0.)\n",
    "\n",
    "# create train and val batchers for task data\n",
    "task_batcher = TaskBatcher(\n",
    "    examples=task_train.to_dict('records'),\n",
    "    vocab=vocab,\n",
    "    mode='train',\n",
    "    batch_size=16,\n",
    "    single_pass=False\n",
    ")\n",
    "task_val_batcher = TaskBatcher( # Batching obj\n",
    "    examples=task_val.to_dict('records')[:200],\n",
    "    vocab=vocab, \n",
    "    mode='train', \n",
    "    batch_size=50, \n",
    "    single_pass=False\n",
    ")\n",
    "\n",
    "# create (re)trainer\n",
    "task_trainer = TaskTrain(vocab, task_batcher, opt_retrain, TaskModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1 mle_loss: 5.016 mle_loss_val: -100.0000\n",
      "iter: 2 mle_loss: 4.740 mle_loss_val: -100.0000\n",
      "iter: 3 mle_loss: 4.947 mle_loss_val: -100.0000\n",
      "model saved at: \n",
      " data/saved_models2//0000003.tar\n",
      "iter: 4 mle_loss: 4.371 mle_loss_val: -100.0000\n"
     ]
    }
   ],
   "source": [
    "config.save_model_path = 'data/saved_models2/'\n",
    "mle_losses_task = task_trainer.trainIters(n_iters=3, report_every=1, save_every = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [(1, 5.0163164138793945),\n",
       "  (2, 4.740086078643799),\n",
       "  (3, 4.947429656982422),\n",
       "  (4, 4.370893478393555)],\n",
       " 'val': []}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_losses_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Summaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = os.path.join(config.log_root, \"data/saved_models/0000090.tar\") # model directory\n",
    "\n",
    "# new batcher for evaluation\n",
    "wiki_eval_batcher = TaskBatcher(\n",
    "    examples=wiki_val.to_dict('records'),\n",
    "    vocab=vocab, \n",
    "    mode='train', \n",
    "    batch_size=50,\n",
    "    single_pass=True)\n",
    "\n",
    "task_eval_batcher = TaskBatcher(\n",
    "    examples=task_val.to_dict('records'),\n",
    "    vocab=vocab, \n",
    "    mode='train', \n",
    "    batch_size=50,\n",
    "    single_pass=True)\n",
    "\n",
    "evaluator = TaskEvaluate(vocab, task_eval_batcher, TaskModel, load_model) # Evaluation object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41450470"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_generator completed reading all examples. No more data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-14:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rowancassius/Desktop/capstone/LSTM_Summarizer/data_util/batcher.py\", line 447, in text_generator\n",
      "    example = next(example_generator)\n",
      "StopIteration\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rowancassius/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/rowancassius/opt/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/rowancassius/Desktop/capstone/LSTM_Summarizer/data_util/batcher.py\", line 426, in fill_example_queue\n",
      "    context, task, summary = next(input_gen) # read the next example from file. article and abstract are both strings.\n",
      "RuntimeError: generator raised StopIteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_generator completed reading all examples. No more data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rowancassius/Desktop/capstone/LSTM_Summarizer/data_util/batcher.py\", line 447, in text_generator\n",
      "    example = next(example_generator)\n",
      "StopIteration\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rowancassius/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/rowancassius/opt/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/rowancassius/Desktop/capstone/LSTM_Summarizer/data_util/batcher.py\", line 426, in fill_example_queue\n",
      "    context, task, summary = next(input_gen) # read the next example from file. article and abstract are both strings.\n",
      "RuntimeError: generator raised StopIteration\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# check model parameter count\n",
    "count_parameters(evaluator.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n"
     ]
    }
   ],
   "source": [
    "decoded_sents, ref_sents, task_sents, context_sents = evaluator.evaluate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ref_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decoded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>ref</th>\n",
       "      <th>decoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>Please shout with any questions.</td>\n",
       "      <td>contact SENDER</td>\n",
       "      <td>shout questions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>Per Jay's voice mail to you, could you please take a look at the two exhibits attached to the two documents at the bottom of this email and verify the trades with Jay.</td>\n",
       "      <td>look a exhibits verify trade with jay</td>\n",
       "      <td>examine two documents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>Please call me with any questions that you have,</td>\n",
       "      <td>call SENDER with questions about yesterday 's natural gas curve</td>\n",
       "      <td>call sender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>Daren would you look at the price for 29 and 30th of March 2000 .</td>\n",
       "      <td>examine price for 29 and 30th of March 2000</td>\n",
       "      <td>look at price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>Please rsvp to Judy Cox if you will be attending this meeting.</td>\n",
       "      <td>respond to Judy Cox</td>\n",
       "      <td>rsvp to Judy Cox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>Please call me to discuss when you get a chance.</td>\n",
       "      <td>call SENDER</td>\n",
       "      <td>call sender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>Can you please provide a NYMEX quote and basis quote El Paso San Juan .</td>\n",
       "      <td>send SENDER a NYMEX quote and basis quote</td>\n",
       "      <td>provide NYMEX quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>Please add Janice Moore and Shari Stack to you list and delete Lou Stoler.</td>\n",
       "      <td>add Janice Moore and Shari Stack to list and delete Lou Stoler</td>\n",
       "      <td>add Janice Moore to list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>If you would like to pre-submit a question, please send them to Jeff Skilling directly or fax to him at 713-6468-381 .</td>\n",
       "      <td>send questions to Jeff Skilling or fax him at 713-6468-381</td>\n",
       "      <td>send question to Jeff Skilling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>FaxWave Service faxwave service@callwave.com on 11/29/2000 02:28:57 PM Please respond to This is a send-only email.</td>\n",
       "      <td>respond to SENDER</td>\n",
       "      <td>respond to email</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                        task  \\\n",
       "66   Please shout with any questions.                                                                                                                                          \n",
       "167  Per Jay's voice mail to you, could you please take a look at the two exhibits attached to the two documents at the bottom of this email and verify the trades with Jay.   \n",
       "130  Please call me with any questions that you have,                                                                                                                          \n",
       "95   Daren would you look at the price for 29 and 30th of March 2000 .                                                                                                         \n",
       "113  Please rsvp to Judy Cox if you will be attending this meeting.                                                                                                            \n",
       "133  Please call me to discuss when you get a chance.                                                                                                                          \n",
       "78   Can you please provide a NYMEX quote and basis quote El Paso San Juan .                                                                                                   \n",
       "178  Please add Janice Moore and Shari Stack to you list and delete Lou Stoler.                                                                                                \n",
       "150  If you would like to pre-submit a question, please send them to Jeff Skilling directly or fax to him at 713-6468-381 .                                                    \n",
       "180  FaxWave Service faxwave service@callwave.com on 11/29/2000 02:28:57 PM Please respond to This is a send-only email.                                                       \n",
       "\n",
       "                                                                 ref  \\\n",
       "66   contact SENDER                                                    \n",
       "167  look a exhibits verify trade with jay                             \n",
       "130  call SENDER with questions about yesterday 's natural gas curve   \n",
       "95   examine price for 29 and 30th of March 2000                       \n",
       "113  respond to Judy Cox                                               \n",
       "133  call SENDER                                                       \n",
       "78   send SENDER a NYMEX quote and basis quote                         \n",
       "178  add Janice Moore and Shari Stack to list and delete Lou Stoler    \n",
       "150  send questions to Jeff Skilling or fax him at 713-6468-381        \n",
       "180  respond to SENDER                                                 \n",
       "\n",
       "                            decoded  \n",
       "66   shout questions                 \n",
       "167  examine two documents           \n",
       "130  call sender                     \n",
       "95   look at price                   \n",
       "113  rsvp to Judy Cox                \n",
       "133  call sender                     \n",
       "78   provide NYMEX quote             \n",
       "178  add Janice Moore to list        \n",
       "150  send question to Jeff Skilling  \n",
       "180  respond to email                "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.DataFrame({'task': task_sents,'ref': ref_sents, 'decoded': decoded_sents})\n",
    "df = pd.DataFrame({'task': task_sents, 'ref': ref_sents, 'decoded': decoded_sents})\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
