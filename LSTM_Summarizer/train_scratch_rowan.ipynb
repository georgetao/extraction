{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Outside imports\n",
    "import os\n",
    "import importlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import train\n",
    "import evaluate\n",
    "import train_util\n",
    "import data_util.data\n",
    "import data_util.batcher\n",
    "import data_util.config\n",
    "import data_util.preprocess\n",
    "\n",
    "importlib.reload(data_util.preprocess)\n",
    "importlib.reload(train)\n",
    "importlib.reload(model)\n",
    "importlib.reload(evaluate)\n",
    "importlib.reload(train_util)\n",
    "importlib.reload(data_util.config)\n",
    "importlib.reload(data_util.data)\n",
    "importlib.reload(data_util.batcher)\n",
    "\n",
    "from train import *\n",
    "from evaluate import *\n",
    "from model import *\n",
    "from train_util import *\n",
    "from data_util.data import *\n",
    "from data_util.batcher import *\n",
    "from data_util.preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load real data\n",
    "data_path = os.path.join(config.log_root, 'data/context_task_data.tsv')\n",
    "dat = pd.read_csv(data_path, sep='\\t')\n",
    "\n",
    "# fill nas\n",
    "dat.fillna('', inplace=True)\n",
    "\n",
    "# train/test split\n",
    "np.random.seed(111)\n",
    "dat = dat.sample(frac=1)\n",
    "\n",
    "train_size = int(.8*dat.shape[0])\n",
    "train_data = dat[:train_size]\n",
    "test_data = dat[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the the data\n",
    "def prep_data(df):\n",
    "    df['Context'] = df['Context'].map(lambda x: article_process_text(x))\n",
    "    df['TaskSentence'] = df['TaskSentence'].map(lambda x: article_process_text(x))\n",
    "    df['Summary'] = df['Summary'].map(lambda x: summary_process_text(x))\n",
    "    return df\n",
    "train_data = prep_data(train_data)\n",
    "test_data = prep_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: <s>, </s>, [UNK], [PAD], [START] or [STOP] found in vocab file\n",
      "WARNING: <s>, </s>, [UNK], [PAD], [START] or [STOP] found in vocab file\n",
      "WARNING: <s>, </s>, [UNK], [PAD], [START] or [STOP] found in vocab file\n",
      "WARNING: <s>, </s>, [UNK], [PAD], [START] or [STOP] found in vocab file\n",
      "Finished constructing vocabulary of 6639 total words. Last word added: LANGUAGE\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(os.path.join(config.log_root, 'data/vocab/vocab.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "T.manual_seed(123)\n",
    "if T.cuda.is_available():\n",
    "    T.cuda.manual_seed_all(123)\n",
    "    \n",
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mle = \"yes\"\n",
    "train_rl = \"no\"\n",
    "mle_weight = 1.0\n",
    "load_model = None\n",
    "new_lr = None\n",
    "rl_weight = 1 - mle_weight\n",
    "\n",
    "opt = Namespace(train_mle = train_mle, \n",
    "                train_rl = train_rl, \n",
    "                mle_weight = mle_weight, \n",
    "                load_model = load_model,\n",
    "                new_lr = new_lr, \n",
    "                rl_weight = rl_weight)\n",
    "\n",
    "task_batcher = TaskBatcher(\n",
    "    examples=train_data.to_dict('records'),\n",
    "    vocab=vocab,\n",
    "    mode='train',\n",
    "    batch_size=32,\n",
    "    single_pass=False\n",
    ")\n",
    "\n",
    "val_task_batcher = TaskBatcher( # Batching obj\n",
    "    examples=test_data.to_dict('records')[:200],\n",
    "    vocab=vocab, \n",
    "    mode='train', \n",
    "    batch_size=50, \n",
    "    single_pass=False\n",
    ")\n",
    "\n",
    "\n",
    "train_processor = TaskTrain(vocab, task_batcher, opt, TaskModel, val_task_batcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained embedding weights\n",
    "train_processor.model.load_embeddings(\"embedding_6639_200.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1 mle_loss: 6.149 mle_loss_val: -100.0000\n",
      "iter: 2 mle_loss: 5.980 mle_loss_val: -100.0000\n",
      "iter: 3 mle_loss: 6.558 mle_loss_val: -100.0000\n",
      "iter: 4 mle_loss: 5.892 mle_loss_val: -100.0000\n",
      "iter: 5 mle_loss: 5.856 mle_loss_val: -100.0000\n",
      "iter: 6 mle_loss: 5.299 mle_loss_val: -100.0000\n",
      "iter: 7 mle_loss: 4.500 mle_loss_val: -100.0000\n",
      "iter: 8 mle_loss: 3.893 mle_loss_val: -100.0000\n",
      "iter: 9 mle_loss: 4.333 mle_loss_val: -100.0000\n",
      "iter: 10 mle_loss: 3.549 mle_loss_val: -100.0000\n",
      "iter: 11 mle_loss: 3.579 mle_loss_val: -100.0000\n",
      "iter: 12 mle_loss: 3.731 mle_loss_val: -100.0000\n",
      "iter: 13 mle_loss: 3.141 mle_loss_val: -100.0000\n",
      "iter: 14 mle_loss: 2.942 mle_loss_val: -100.0000\n",
      "iter: 15 mle_loss: 3.059 mle_loss_val: -100.0000\n",
      "iter: 16 mle_loss: 3.126 mle_loss_val: -100.0000\n",
      "iter: 17 mle_loss: 3.153 mle_loss_val: -100.0000\n",
      "iter: 18 mle_loss: 3.091 mle_loss_val: -100.0000\n",
      "iter: 19 mle_loss: 2.932 mle_loss_val: -100.0000\n",
      "iter: 20 mle_loss: 3.393 mle_loss_val: -100.0000\n",
      "iter: 21 mle_loss: 2.895 mle_loss_val: -100.0000\n",
      "iter: 22 mle_loss: 2.814 mle_loss_val: -100.0000\n",
      "iter: 23 mle_loss: 3.023 mle_loss_val: -100.0000\n",
      "iter: 24 mle_loss: 3.030 mle_loss_val: -100.0000\n",
      "iter: 25 mle_loss: 2.949 mle_loss_val: -100.0000\n",
      "iter: 26 mle_loss: 2.525 mle_loss_val: -100.0000\n",
      "iter: 27 mle_loss: 3.003 mle_loss_val: -100.0000\n",
      "iter: 28 mle_loss: 3.054 mle_loss_val: -100.0000\n",
      "iter: 29 mle_loss: 2.961 mle_loss_val: -100.0000\n",
      "iter: 30 mle_loss: 2.849 mle_loss_val: 2.9207\n",
      "model saved at: \n",
      " data/saved_models_2/0000030.tar\n",
      "iter: 31 mle_loss: 3.052 mle_loss_val: 2.9207\n",
      "iter: 32 mle_loss: 2.907 mle_loss_val: 2.9207\n",
      "iter: 33 mle_loss: 2.763 mle_loss_val: 2.9207\n",
      "iter: 34 mle_loss: 2.705 mle_loss_val: 2.9207\n",
      "iter: 35 mle_loss: 2.791 mle_loss_val: 2.9207\n",
      "iter: 36 mle_loss: 2.951 mle_loss_val: 2.9207\n",
      "iter: 37 mle_loss: 2.900 mle_loss_val: 2.9207\n",
      "iter: 38 mle_loss: 2.836 mle_loss_val: 2.9207\n",
      "iter: 39 mle_loss: 3.180 mle_loss_val: 2.9207\n",
      "iter: 40 mle_loss: 2.643 mle_loss_val: 2.9207\n",
      "iter: 41 mle_loss: 2.524 mle_loss_val: 2.9207\n",
      "iter: 42 mle_loss: 2.752 mle_loss_val: 2.9207\n",
      "iter: 43 mle_loss: 2.687 mle_loss_val: 2.9207\n",
      "iter: 44 mle_loss: 2.694 mle_loss_val: 2.9207\n",
      "iter: 45 mle_loss: 2.525 mle_loss_val: 2.9207\n",
      "iter: 46 mle_loss: 2.675 mle_loss_val: 2.9207\n",
      "iter: 47 mle_loss: 2.380 mle_loss_val: 2.9207\n",
      "iter: 48 mle_loss: 2.715 mle_loss_val: 2.9207\n",
      "iter: 49 mle_loss: 2.846 mle_loss_val: 2.9207\n",
      "iter: 50 mle_loss: 2.205 mle_loss_val: 2.9207\n",
      "iter: 51 mle_loss: 2.735 mle_loss_val: 2.9207\n",
      "iter: 52 mle_loss: 2.770 mle_loss_val: 2.9207\n",
      "iter: 53 mle_loss: 2.759 mle_loss_val: 2.9207\n",
      "iter: 54 mle_loss: 2.571 mle_loss_val: 2.9207\n",
      "iter: 55 mle_loss: 2.777 mle_loss_val: 2.9207\n",
      "iter: 56 mle_loss: 2.879 mle_loss_val: 2.9207\n",
      "iter: 57 mle_loss: 2.788 mle_loss_val: 2.9207\n",
      "iter: 58 mle_loss: 2.313 mle_loss_val: 2.9207\n",
      "iter: 59 mle_loss: 2.942 mle_loss_val: 2.9207\n",
      "iter: 60 mle_loss: 2.378 mle_loss_val: 2.5984\n",
      "model saved at: \n",
      " data/saved_models_2/0000060.tar\n",
      "iter: 61 mle_loss: 2.431 mle_loss_val: 2.5984\n",
      "iter: 62 mle_loss: 2.780 mle_loss_val: 2.5984\n",
      "iter: 63 mle_loss: 2.537 mle_loss_val: 2.5984\n",
      "iter: 64 mle_loss: 2.448 mle_loss_val: 2.5984\n",
      "iter: 65 mle_loss: 2.453 mle_loss_val: 2.5984\n",
      "iter: 66 mle_loss: 2.507 mle_loss_val: 2.5984\n",
      "iter: 67 mle_loss: 2.771 mle_loss_val: 2.5984\n",
      "iter: 68 mle_loss: 2.189 mle_loss_val: 2.5984\n",
      "iter: 69 mle_loss: 2.254 mle_loss_val: 2.5984\n",
      "iter: 70 mle_loss: 2.738 mle_loss_val: 2.5984\n",
      "iter: 71 mle_loss: 2.328 mle_loss_val: 2.5984\n",
      "iter: 72 mle_loss: 2.447 mle_loss_val: 2.5984\n",
      "iter: 73 mle_loss: 2.208 mle_loss_val: 2.5984\n",
      "iter: 74 mle_loss: 2.712 mle_loss_val: 2.5984\n",
      "iter: 75 mle_loss: 2.600 mle_loss_val: 2.5984\n",
      "iter: 76 mle_loss: 2.235 mle_loss_val: 2.5984\n",
      "iter: 77 mle_loss: 2.524 mle_loss_val: 2.5984\n",
      "iter: 78 mle_loss: 2.524 mle_loss_val: 2.5984\n",
      "iter: 79 mle_loss: 2.375 mle_loss_val: 2.5984\n",
      "iter: 80 mle_loss: 2.094 mle_loss_val: 2.5984\n",
      "iter: 81 mle_loss: 2.576 mle_loss_val: 2.5984\n",
      "iter: 82 mle_loss: 2.161 mle_loss_val: 2.5984\n",
      "iter: 83 mle_loss: 2.433 mle_loss_val: 2.5984\n",
      "iter: 84 mle_loss: 1.715 mle_loss_val: 2.5984\n",
      "iter: 85 mle_loss: 2.435 mle_loss_val: 2.5984\n",
      "iter: 86 mle_loss: 2.211 mle_loss_val: 2.5984\n",
      "iter: 87 mle_loss: 2.436 mle_loss_val: 2.5984\n",
      "iter: 88 mle_loss: 2.359 mle_loss_val: 2.5984\n",
      "iter: 89 mle_loss: 2.229 mle_loss_val: 2.5984\n",
      "iter: 90 mle_loss: 2.439 mle_loss_val: 2.4340\n",
      "model saved at: \n",
      " data/saved_models_2/0000090.tar\n",
      "iter: 91 mle_loss: 2.169 mle_loss_val: 2.4340\n",
      "iter: 92 mle_loss: 2.248 mle_loss_val: 2.4340\n",
      "iter: 93 mle_loss: 2.131 mle_loss_val: 2.4340\n",
      "iter: 94 mle_loss: 2.044 mle_loss_val: 2.4340\n",
      "iter: 95 mle_loss: 2.717 mle_loss_val: 2.4340\n",
      "iter: 96 mle_loss: 2.377 mle_loss_val: 2.4340\n",
      "iter: 97 mle_loss: 2.079 mle_loss_val: 2.4340\n",
      "iter: 98 mle_loss: 2.267 mle_loss_val: 2.4340\n",
      "iter: 99 mle_loss: 2.536 mle_loss_val: 2.4340\n",
      "iter: 100 mle_loss: 2.148 mle_loss_val: 2.4340\n",
      "iter: 101 mle_loss: 2.323 mle_loss_val: 2.4340\n",
      "iter: 102 mle_loss: 2.485 mle_loss_val: 2.4340\n",
      "iter: 103 mle_loss: 2.140 mle_loss_val: 2.4340\n",
      "iter: 104 mle_loss: 2.355 mle_loss_val: 2.4340\n",
      "iter: 105 mle_loss: 2.096 mle_loss_val: 2.4340\n",
      "iter: 106 mle_loss: 2.248 mle_loss_val: 2.4340\n",
      "iter: 107 mle_loss: 2.203 mle_loss_val: 2.4340\n",
      "iter: 108 mle_loss: 2.563 mle_loss_val: 2.4340\n",
      "iter: 109 mle_loss: 2.570 mle_loss_val: 2.4340\n",
      "iter: 110 mle_loss: 2.274 mle_loss_val: 2.4340\n",
      "iter: 111 mle_loss: 2.022 mle_loss_val: 2.4340\n",
      "iter: 112 mle_loss: 2.403 mle_loss_val: 2.4340\n",
      "iter: 113 mle_loss: 2.135 mle_loss_val: 2.4340\n",
      "iter: 114 mle_loss: 2.479 mle_loss_val: 2.4340\n",
      "iter: 115 mle_loss: 1.905 mle_loss_val: 2.4340\n",
      "iter: 116 mle_loss: 2.052 mle_loss_val: 2.4340\n",
      "iter: 117 mle_loss: 2.270 mle_loss_val: 2.4340\n",
      "iter: 118 mle_loss: 2.164 mle_loss_val: 2.4340\n",
      "iter: 119 mle_loss: 2.410 mle_loss_val: 2.4340\n",
      "iter: 120 mle_loss: 1.979 mle_loss_val: 2.3818\n",
      "model saved at: \n",
      " data/saved_models_2/0000120.tar\n",
      "iter: 121 mle_loss: 2.037 mle_loss_val: 2.3818\n",
      "iter: 122 mle_loss: 2.542 mle_loss_val: 2.3818\n",
      "iter: 123 mle_loss: 2.456 mle_loss_val: 2.3818\n",
      "iter: 124 mle_loss: 2.028 mle_loss_val: 2.3818\n",
      "iter: 125 mle_loss: 2.355 mle_loss_val: 2.3818\n",
      "iter: 126 mle_loss: 2.439 mle_loss_val: 2.3818\n",
      "iter: 127 mle_loss: 2.495 mle_loss_val: 2.3818\n",
      "iter: 128 mle_loss: 2.161 mle_loss_val: 2.3818\n",
      "iter: 129 mle_loss: 2.306 mle_loss_val: 2.3818\n",
      "iter: 130 mle_loss: 2.240 mle_loss_val: 2.3818\n",
      "iter: 131 mle_loss: 2.030 mle_loss_val: 2.3818\n",
      "iter: 132 mle_loss: 2.393 mle_loss_val: 2.3818\n",
      "iter: 133 mle_loss: 2.331 mle_loss_val: 2.3818\n",
      "iter: 134 mle_loss: 2.100 mle_loss_val: 2.3818\n",
      "iter: 135 mle_loss: 2.337 mle_loss_val: 2.3818\n",
      "iter: 136 mle_loss: 1.864 mle_loss_val: 2.3818\n",
      "iter: 137 mle_loss: 2.322 mle_loss_val: 2.3818\n",
      "iter: 138 mle_loss: 2.352 mle_loss_val: 2.3818\n",
      "iter: 139 mle_loss: 1.772 mle_loss_val: 2.3818\n",
      "iter: 140 mle_loss: 1.873 mle_loss_val: 2.3818\n",
      "iter: 141 mle_loss: 2.276 mle_loss_val: 2.3818\n",
      "iter: 142 mle_loss: 2.122 mle_loss_val: 2.3818\n",
      "iter: 143 mle_loss: 2.148 mle_loss_val: 2.3818\n",
      "iter: 144 mle_loss: 2.158 mle_loss_val: 2.3818\n",
      "iter: 145 mle_loss: 2.237 mle_loss_val: 2.3818\n",
      "iter: 146 mle_loss: 2.316 mle_loss_val: 2.3818\n",
      "iter: 147 mle_loss: 2.108 mle_loss_val: 2.3818\n",
      "iter: 148 mle_loss: 2.401 mle_loss_val: 2.3818\n",
      "iter: 149 mle_loss: 2.006 mle_loss_val: 2.3818\n",
      "iter: 150 mle_loss: 1.959 mle_loss_val: 2.3227\n",
      "model saved at: \n",
      " data/saved_models_2/0000150.tar\n",
      "iter: 151 mle_loss: 2.107 mle_loss_val: 2.3227\n",
      "iter: 152 mle_loss: 2.115 mle_loss_val: 2.3227\n",
      "iter: 153 mle_loss: 2.081 mle_loss_val: 2.3227\n",
      "iter: 154 mle_loss: 1.889 mle_loss_val: 2.3227\n",
      "iter: 155 mle_loss: 1.934 mle_loss_val: 2.3227\n",
      "iter: 156 mle_loss: 1.807 mle_loss_val: 2.3227\n",
      "iter: 157 mle_loss: 2.112 mle_loss_val: 2.3227\n",
      "iter: 158 mle_loss: 2.266 mle_loss_val: 2.3227\n",
      "iter: 159 mle_loss: 2.549 mle_loss_val: 2.3227\n",
      "iter: 160 mle_loss: 1.974 mle_loss_val: 2.3227\n",
      "iter: 161 mle_loss: 2.290 mle_loss_val: 2.3227\n",
      "iter: 162 mle_loss: 2.032 mle_loss_val: 2.3227\n",
      "iter: 163 mle_loss: 1.768 mle_loss_val: 2.3227\n",
      "iter: 164 mle_loss: 2.052 mle_loss_val: 2.3227\n",
      "iter: 165 mle_loss: 2.164 mle_loss_val: 2.3227\n",
      "iter: 166 mle_loss: 2.165 mle_loss_val: 2.3227\n",
      "iter: 167 mle_loss: 1.816 mle_loss_val: 2.3227\n",
      "iter: 168 mle_loss: 2.019 mle_loss_val: 2.3227\n",
      "iter: 169 mle_loss: 1.911 mle_loss_val: 2.3227\n",
      "iter: 170 mle_loss: 2.043 mle_loss_val: 2.3227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 171 mle_loss: 1.922 mle_loss_val: 2.3227\n",
      "iter: 172 mle_loss: 2.458 mle_loss_val: 2.3227\n",
      "iter: 173 mle_loss: 2.036 mle_loss_val: 2.3227\n",
      "iter: 174 mle_loss: 1.980 mle_loss_val: 2.3227\n",
      "iter: 175 mle_loss: 1.925 mle_loss_val: 2.3227\n",
      "iter: 176 mle_loss: 2.023 mle_loss_val: 2.3227\n",
      "iter: 177 mle_loss: 2.171 mle_loss_val: 2.3227\n",
      "iter: 178 mle_loss: 2.416 mle_loss_val: 2.3227\n",
      "iter: 179 mle_loss: 2.232 mle_loss_val: 2.3227\n",
      "iter: 180 mle_loss: 2.334 mle_loss_val: 2.2281\n",
      "model saved at: \n",
      " data/saved_models_2/0000180.tar\n",
      "iter: 181 mle_loss: 1.984 mle_loss_val: 2.2281\n",
      "iter: 182 mle_loss: 1.831 mle_loss_val: 2.2281\n",
      "iter: 183 mle_loss: 2.108 mle_loss_val: 2.2281\n",
      "iter: 184 mle_loss: 2.007 mle_loss_val: 2.2281\n",
      "iter: 185 mle_loss: 1.906 mle_loss_val: 2.2281\n",
      "iter: 186 mle_loss: 1.683 mle_loss_val: 2.2281\n",
      "iter: 187 mle_loss: 2.036 mle_loss_val: 2.2281\n",
      "iter: 188 mle_loss: 1.854 mle_loss_val: 2.2281\n",
      "iter: 189 mle_loss: 2.279 mle_loss_val: 2.2281\n",
      "iter: 190 mle_loss: 2.226 mle_loss_val: 2.2281\n",
      "iter: 191 mle_loss: 1.542 mle_loss_val: 2.2281\n",
      "iter: 192 mle_loss: 2.184 mle_loss_val: 2.2281\n",
      "iter: 193 mle_loss: 2.137 mle_loss_val: 2.2281\n",
      "iter: 194 mle_loss: 2.044 mle_loss_val: 2.2281\n",
      "iter: 195 mle_loss: 1.928 mle_loss_val: 2.2281\n",
      "iter: 196 mle_loss: 2.175 mle_loss_val: 2.2281\n",
      "iter: 197 mle_loss: 2.233 mle_loss_val: 2.2281\n",
      "iter: 198 mle_loss: 2.159 mle_loss_val: 2.2281\n",
      "iter: 199 mle_loss: 1.623 mle_loss_val: 2.2281\n",
      "iter: 200 mle_loss: 2.164 mle_loss_val: 2.2281\n",
      "iter: 201 mle_loss: 1.873 mle_loss_val: 2.2281\n",
      "iter: 202 mle_loss: 1.903 mle_loss_val: 2.2281\n",
      "iter: 203 mle_loss: 2.232 mle_loss_val: 2.2281\n",
      "iter: 204 mle_loss: 1.955 mle_loss_val: 2.2281\n",
      "iter: 205 mle_loss: 2.047 mle_loss_val: 2.2281\n",
      "iter: 206 mle_loss: 2.030 mle_loss_val: 2.2281\n",
      "iter: 207 mle_loss: 1.827 mle_loss_val: 2.2281\n",
      "iter: 208 mle_loss: 2.288 mle_loss_val: 2.2281\n",
      "iter: 209 mle_loss: 1.624 mle_loss_val: 2.2281\n",
      "iter: 210 mle_loss: 1.676 mle_loss_val: 2.2262\n",
      "model saved at: \n",
      " data/saved_models_2/0000210.tar\n",
      "iter: 211 mle_loss: 2.102 mle_loss_val: 2.2262\n",
      "iter: 212 mle_loss: 1.911 mle_loss_val: 2.2262\n",
      "iter: 213 mle_loss: 1.889 mle_loss_val: 2.2262\n",
      "iter: 214 mle_loss: 1.825 mle_loss_val: 2.2262\n",
      "iter: 215 mle_loss: 2.221 mle_loss_val: 2.2262\n",
      "iter: 216 mle_loss: 2.214 mle_loss_val: 2.2262\n",
      "iter: 217 mle_loss: 1.781 mle_loss_val: 2.2262\n",
      "iter: 218 mle_loss: 2.156 mle_loss_val: 2.2262\n",
      "iter: 219 mle_loss: 2.163 mle_loss_val: 2.2262\n",
      "iter: 220 mle_loss: 1.876 mle_loss_val: 2.2262\n",
      "iter: 221 mle_loss: 1.688 mle_loss_val: 2.2262\n",
      "iter: 222 mle_loss: 2.184 mle_loss_val: 2.2262\n",
      "iter: 223 mle_loss: 1.819 mle_loss_val: 2.2262\n",
      "iter: 224 mle_loss: 2.012 mle_loss_val: 2.2262\n",
      "iter: 225 mle_loss: 1.325 mle_loss_val: 2.2262\n",
      "iter: 226 mle_loss: 2.040 mle_loss_val: 2.2262\n",
      "iter: 227 mle_loss: 1.728 mle_loss_val: 2.2262\n",
      "iter: 228 mle_loss: 1.996 mle_loss_val: 2.2262\n",
      "iter: 229 mle_loss: 1.971 mle_loss_val: 2.2262\n",
      "iter: 230 mle_loss: 1.817 mle_loss_val: 2.2262\n",
      "iter: 231 mle_loss: 2.025 mle_loss_val: 2.2262\n",
      "iter: 232 mle_loss: 1.831 mle_loss_val: 2.2262\n",
      "iter: 233 mle_loss: 1.876 mle_loss_val: 2.2262\n",
      "iter: 234 mle_loss: 1.684 mle_loss_val: 2.2262\n",
      "iter: 235 mle_loss: 1.619 mle_loss_val: 2.2262\n",
      "iter: 236 mle_loss: 2.272 mle_loss_val: 2.2262\n",
      "iter: 237 mle_loss: 1.975 mle_loss_val: 2.2262\n",
      "iter: 238 mle_loss: 1.651 mle_loss_val: 2.2262\n",
      "iter: 239 mle_loss: 1.811 mle_loss_val: 2.2262\n",
      "iter: 240 mle_loss: 1.878 mle_loss_val: 2.2178\n",
      "model saved at: \n",
      " data/saved_models_2/0000240.tar\n",
      "iter: 241 mle_loss: 1.726 mle_loss_val: 2.2178\n",
      "iter: 242 mle_loss: 1.971 mle_loss_val: 2.2178\n",
      "iter: 243 mle_loss: 2.051 mle_loss_val: 2.2178\n",
      "iter: 244 mle_loss: 1.745 mle_loss_val: 2.2178\n",
      "iter: 245 mle_loss: 1.958 mle_loss_val: 2.2178\n",
      "iter: 246 mle_loss: 1.712 mle_loss_val: 2.2178\n",
      "iter: 247 mle_loss: 1.849 mle_loss_val: 2.2178\n",
      "iter: 248 mle_loss: 2.056 mle_loss_val: 2.2178\n",
      "iter: 249 mle_loss: 2.127 mle_loss_val: 2.2178\n",
      "iter: 250 mle_loss: 2.235 mle_loss_val: 2.2178\n",
      "iter: 251 mle_loss: 1.879 mle_loss_val: 2.2178\n",
      "iter: 252 mle_loss: 1.748 mle_loss_val: 2.2178\n",
      "iter: 253 mle_loss: 1.987 mle_loss_val: 2.2178\n",
      "iter: 254 mle_loss: 1.779 mle_loss_val: 2.2178\n",
      "iter: 255 mle_loss: 1.963 mle_loss_val: 2.2178\n",
      "iter: 256 mle_loss: 1.663 mle_loss_val: 2.2178\n",
      "iter: 257 mle_loss: 1.643 mle_loss_val: 2.2178\n",
      "iter: 258 mle_loss: 1.869 mle_loss_val: 2.2178\n",
      "iter: 259 mle_loss: 1.842 mle_loss_val: 2.2178\n",
      "iter: 260 mle_loss: 2.154 mle_loss_val: 2.2178\n",
      "iter: 261 mle_loss: 1.724 mle_loss_val: 2.2178\n",
      "iter: 262 mle_loss: 1.704 mle_loss_val: 2.2178\n",
      "iter: 263 mle_loss: 2.091 mle_loss_val: 2.2178\n",
      "iter: 264 mle_loss: 2.022 mle_loss_val: 2.2178\n",
      "iter: 265 mle_loss: 1.633 mle_loss_val: 2.2178\n",
      "iter: 266 mle_loss: 1.951 mle_loss_val: 2.2178\n",
      "iter: 267 mle_loss: 2.002 mle_loss_val: 2.2178\n",
      "iter: 268 mle_loss: 2.193 mle_loss_val: 2.2178\n",
      "iter: 269 mle_loss: 1.931 mle_loss_val: 2.2178\n",
      "iter: 270 mle_loss: 1.870 mle_loss_val: 2.1669\n",
      "model saved at: \n",
      " data/saved_models_2/0000270.tar\n",
      "iter: 271 mle_loss: 1.709 mle_loss_val: 2.1669\n",
      "iter: 272 mle_loss: 1.518 mle_loss_val: 2.1669\n",
      "iter: 273 mle_loss: 2.024 mle_loss_val: 2.1669\n",
      "iter: 274 mle_loss: 2.040 mle_loss_val: 2.1669\n",
      "iter: 275 mle_loss: 1.814 mle_loss_val: 2.1669\n",
      "iter: 276 mle_loss: 2.119 mle_loss_val: 2.1669\n",
      "iter: 277 mle_loss: 1.554 mle_loss_val: 2.1669\n",
      "iter: 278 mle_loss: 2.011 mle_loss_val: 2.1669\n",
      "iter: 279 mle_loss: 1.943 mle_loss_val: 2.1669\n",
      "iter: 280 mle_loss: 1.448 mle_loss_val: 2.1669\n",
      "iter: 281 mle_loss: 1.666 mle_loss_val: 2.1669\n",
      "iter: 282 mle_loss: 1.996 mle_loss_val: 2.1669\n",
      "iter: 283 mle_loss: 1.992 mle_loss_val: 2.1669\n",
      "iter: 284 mle_loss: 1.752 mle_loss_val: 2.1669\n",
      "iter: 285 mle_loss: 1.865 mle_loss_val: 2.1669\n",
      "iter: 286 mle_loss: 1.915 mle_loss_val: 2.1669\n",
      "iter: 287 mle_loss: 1.909 mle_loss_val: 2.1669\n",
      "iter: 288 mle_loss: 1.840 mle_loss_val: 2.1669\n",
      "iter: 289 mle_loss: 2.089 mle_loss_val: 2.1669\n",
      "iter: 290 mle_loss: 1.732 mle_loss_val: 2.1669\n",
      "iter: 291 mle_loss: 1.722 mle_loss_val: 2.1669\n",
      "iter: 292 mle_loss: 1.806 mle_loss_val: 2.1669\n",
      "iter: 293 mle_loss: 1.831 mle_loss_val: 2.1669\n",
      "iter: 294 mle_loss: 1.757 mle_loss_val: 2.1669\n",
      "iter: 295 mle_loss: 1.703 mle_loss_val: 2.1669\n",
      "iter: 296 mle_loss: 1.691 mle_loss_val: 2.1669\n",
      "iter: 297 mle_loss: 1.528 mle_loss_val: 2.1669\n",
      "iter: 298 mle_loss: 1.712 mle_loss_val: 2.1669\n",
      "iter: 299 mle_loss: 1.886 mle_loss_val: 2.1669\n",
      "iter: 300 mle_loss: 1.714 mle_loss_val: 2.1154\n",
      "model saved at: \n",
      " data/saved_models_2/0000300.tar\n",
      "iter: 301 mle_loss: 1.623 mle_loss_val: 2.1154\n",
      "iter: 302 mle_loss: 2.010 mle_loss_val: 2.1154\n",
      "iter: 303 mle_loss: 1.730 mle_loss_val: 2.1154\n",
      "iter: 304 mle_loss: 1.410 mle_loss_val: 2.1154\n",
      "iter: 305 mle_loss: 1.711 mle_loss_val: 2.1154\n",
      "iter: 306 mle_loss: 1.819 mle_loss_val: 2.1154\n",
      "iter: 307 mle_loss: 1.833 mle_loss_val: 2.1154\n",
      "iter: 308 mle_loss: 1.502 mle_loss_val: 2.1154\n",
      "iter: 309 mle_loss: 1.642 mle_loss_val: 2.1154\n",
      "iter: 310 mle_loss: 1.572 mle_loss_val: 2.1154\n",
      "iter: 311 mle_loss: 1.705 mle_loss_val: 2.1154\n",
      "iter: 312 mle_loss: 1.617 mle_loss_val: 2.1154\n",
      "iter: 313 mle_loss: 1.878 mle_loss_val: 2.1154\n",
      "iter: 314 mle_loss: 1.677 mle_loss_val: 2.1154\n",
      "iter: 315 mle_loss: 1.817 mle_loss_val: 2.1154\n",
      "iter: 316 mle_loss: 1.543 mle_loss_val: 2.1154\n",
      "iter: 317 mle_loss: 1.715 mle_loss_val: 2.1154\n",
      "iter: 318 mle_loss: 1.848 mle_loss_val: 2.1154\n",
      "iter: 319 mle_loss: 1.867 mle_loss_val: 2.1154\n",
      "iter: 320 mle_loss: 1.950 mle_loss_val: 2.1154\n",
      "iter: 321 mle_loss: 1.878 mle_loss_val: 2.1154\n",
      "iter: 322 mle_loss: 1.858 mle_loss_val: 2.1154\n",
      "iter: 323 mle_loss: 1.553 mle_loss_val: 2.1154\n",
      "iter: 324 mle_loss: 1.892 mle_loss_val: 2.1154\n",
      "iter: 325 mle_loss: 1.546 mle_loss_val: 2.1154\n",
      "iter: 326 mle_loss: 1.597 mle_loss_val: 2.1154\n",
      "iter: 327 mle_loss: 1.403 mle_loss_val: 2.1154\n",
      "iter: 328 mle_loss: 1.790 mle_loss_val: 2.1154\n",
      "iter: 329 mle_loss: 1.614 mle_loss_val: 2.1154\n",
      "iter: 330 mle_loss: 1.979 mle_loss_val: 2.2192\n",
      "model saved at: \n",
      " data/saved_models_2/0000330.tar\n",
      "iter: 331 mle_loss: 1.935 mle_loss_val: 2.2192\n",
      "iter: 332 mle_loss: 1.499 mle_loss_val: 2.2192\n",
      "iter: 333 mle_loss: 1.907 mle_loss_val: 2.2192\n",
      "iter: 334 mle_loss: 1.846 mle_loss_val: 2.2192\n",
      "iter: 335 mle_loss: 1.728 mle_loss_val: 2.2192\n",
      "iter: 336 mle_loss: 1.715 mle_loss_val: 2.2192\n",
      "iter: 337 mle_loss: 1.917 mle_loss_val: 2.2192\n",
      "iter: 338 mle_loss: 2.129 mle_loss_val: 2.2192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 339 mle_loss: 1.957 mle_loss_val: 2.2192\n",
      "iter: 340 mle_loss: 1.374 mle_loss_val: 2.2192\n",
      "iter: 341 mle_loss: 1.766 mle_loss_val: 2.2192\n",
      "iter: 342 mle_loss: 1.481 mle_loss_val: 2.2192\n",
      "iter: 343 mle_loss: 1.551 mle_loss_val: 2.2192\n",
      "iter: 344 mle_loss: 1.827 mle_loss_val: 2.2192\n",
      "iter: 345 mle_loss: 1.796 mle_loss_val: 2.2192\n",
      "iter: 346 mle_loss: 1.816 mle_loss_val: 2.2192\n",
      "iter: 347 mle_loss: 1.753 mle_loss_val: 2.2192\n",
      "iter: 348 mle_loss: 1.531 mle_loss_val: 2.2192\n",
      "iter: 349 mle_loss: 1.910 mle_loss_val: 2.2192\n",
      "iter: 350 mle_loss: 1.387 mle_loss_val: 2.2192\n",
      "iter: 351 mle_loss: 1.484 mle_loss_val: 2.2192\n",
      "iter: 352 mle_loss: 1.711 mle_loss_val: 2.2192\n",
      "iter: 353 mle_loss: 1.713 mle_loss_val: 2.2192\n",
      "iter: 354 mle_loss: 1.635 mle_loss_val: 2.2192\n",
      "iter: 355 mle_loss: 1.595 mle_loss_val: 2.2192\n",
      "iter: 356 mle_loss: 1.910 mle_loss_val: 2.2192\n",
      "iter: 357 mle_loss: 1.750 mle_loss_val: 2.2192\n",
      "iter: 358 mle_loss: 1.698 mle_loss_val: 2.2192\n",
      "iter: 359 mle_loss: 1.894 mle_loss_val: 2.2192\n",
      "iter: 360 mle_loss: 1.812 mle_loss_val: 2.1117\n",
      "model saved at: \n",
      " data/saved_models_2/0000360.tar\n",
      "iter: 361 mle_loss: 1.667 mle_loss_val: 2.1117\n",
      "iter: 362 mle_loss: 1.393 mle_loss_val: 2.1117\n",
      "iter: 363 mle_loss: 1.954 mle_loss_val: 2.1117\n",
      "iter: 364 mle_loss: 1.523 mle_loss_val: 2.1117\n",
      "iter: 365 mle_loss: 1.761 mle_loss_val: 2.1117\n",
      "iter: 366 mle_loss: 1.113 mle_loss_val: 2.1117\n",
      "iter: 367 mle_loss: 1.798 mle_loss_val: 2.1117\n",
      "iter: 368 mle_loss: 1.440 mle_loss_val: 2.1117\n",
      "iter: 369 mle_loss: 1.650 mle_loss_val: 2.1117\n",
      "iter: 370 mle_loss: 1.835 mle_loss_val: 2.1117\n",
      "iter: 371 mle_loss: 1.586 mle_loss_val: 2.1117\n",
      "iter: 372 mle_loss: 1.765 mle_loss_val: 2.1117\n",
      "iter: 373 mle_loss: 1.392 mle_loss_val: 2.1117\n",
      "iter: 374 mle_loss: 1.624 mle_loss_val: 2.1117\n",
      "iter: 375 mle_loss: 1.386 mle_loss_val: 2.1117\n",
      "iter: 376 mle_loss: 1.360 mle_loss_val: 2.1117\n",
      "iter: 377 mle_loss: 1.970 mle_loss_val: 2.1117\n",
      "iter: 378 mle_loss: 1.663 mle_loss_val: 2.1117\n",
      "iter: 379 mle_loss: 1.314 mle_loss_val: 2.1117\n",
      "iter: 380 mle_loss: 1.526 mle_loss_val: 2.1117\n",
      "iter: 381 mle_loss: 1.599 mle_loss_val: 2.1117\n",
      "iter: 382 mle_loss: 1.435 mle_loss_val: 2.1117\n",
      "iter: 383 mle_loss: 1.555 mle_loss_val: 2.1117\n",
      "iter: 384 mle_loss: 1.697 mle_loss_val: 2.1117\n",
      "iter: 385 mle_loss: 1.521 mle_loss_val: 2.1117\n",
      "iter: 386 mle_loss: 1.590 mle_loss_val: 2.1117\n",
      "iter: 387 mle_loss: 1.393 mle_loss_val: 2.1117\n",
      "iter: 388 mle_loss: 1.487 mle_loss_val: 2.1117\n",
      "iter: 389 mle_loss: 1.756 mle_loss_val: 2.1117\n",
      "iter: 390 mle_loss: 1.743 mle_loss_val: 2.2547\n",
      "model saved at: \n",
      " data/saved_models_2/0000390.tar\n",
      "iter: 391 mle_loss: 1.989 mle_loss_val: 2.2547\n",
      "iter: 392 mle_loss: 1.727 mle_loss_val: 2.2547\n",
      "iter: 393 mle_loss: 1.548 mle_loss_val: 2.2547\n",
      "iter: 394 mle_loss: 1.748 mle_loss_val: 2.2547\n",
      "iter: 395 mle_loss: 1.476 mle_loss_val: 2.2547\n",
      "iter: 396 mle_loss: 1.602 mle_loss_val: 2.2547\n",
      "iter: 397 mle_loss: 1.265 mle_loss_val: 2.2547\n",
      "iter: 398 mle_loss: 1.277 mle_loss_val: 2.2547\n",
      "iter: 399 mle_loss: 1.656 mle_loss_val: 2.2547\n",
      "iter: 400 mle_loss: 1.722 mle_loss_val: 2.2547\n",
      "iter: 401 mle_loss: 1.929 mle_loss_val: 2.2547\n"
     ]
    }
   ],
   "source": [
    "config.save_model_path = \"data/saved_models_2\"\n",
    "\n",
    "mle_losses = train_processor.trainIters(n_iters=400, report_every=1, save_every = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"validate\"\n",
    "\n",
    "load_model = os.path.join(config.log_root, \"data/saved_models/0000180.tar\") # model directory\n",
    "\n",
    "opt = Namespace(task = task, load_model = load_model) # opt\n",
    "\n",
    "\n",
    "# new batcher for evaluation\n",
    "task_batcher = TaskBatcher( # Batching obj\n",
    "    examples=test_data.to_dict('records'),\n",
    "    vocab=vocab, \n",
    "    mode='train', \n",
    "    batch_size=188, \n",
    "    single_pass=True)\n",
    "\n",
    "eval_processor = TaskEvaluate(vocab, task_batcher, opt, TaskModel) # Evaluation object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(eval_processor.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sents, ref_sents, task_sents, context_sents = eval_processor.evaluate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ref_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(decoded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = Rouge().get_scores(decoded_sents, ref_sents, avg = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'ref': ref_sents, 'decoded': decoded_sents})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(config.log_root, 'data/test_results_2.csv'), sep = '\\t', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'packet' in vocab._word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'interview' in vocab._word_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking certain words for presence in the vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"ensure\", \"indicate\", \"turn\", \"open\", \"add\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w in vocab._word_to_id for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_words = train_data['Summary'].map(lambda x: x.split(' ', 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_words.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vocab = action_words.map(lambda w: w in vocab._word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(in_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vocab.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_words[~in_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[~in_vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2240 of 2256 verbs to start summaries are in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "action_words[~in_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[~in_vocab]['Labeler'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[~in_vocab]['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
