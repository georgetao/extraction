{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Outside imports\n",
    "import os\n",
    "import importlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import train\n",
    "import evaluate\n",
    "import train_util\n",
    "import data_util.data\n",
    "import data_util.batcher\n",
    "import data_util.config\n",
    "import data_util.preprocess\n",
    "\n",
    "importlib.reload(train)\n",
    "importlib.reload(model)\n",
    "importlib.reload(evaluate)\n",
    "importlib.reload(train_util)\n",
    "importlib.reload(data_util.config)\n",
    "importlib.reload(data_util.data)\n",
    "importlib.reload(data_util.batcher)\n",
    "importlib.reload(data_util.preprocess)\n",
    "\n",
    "from train import *\n",
    "from evaluate import *\n",
    "from model import *\n",
    "from train_util import *\n",
    "from data_util.data import *\n",
    "from data_util.batcher import *\n",
    "from data_util.preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load real data\n",
    "data_path = '/Users/rowancassius/Desktop/capstone/data/context_task_data_fresh.tsv'\n",
    "dat = pd.read_csv(data_path, sep='\\t')\n",
    "\n",
    "# fill nas\n",
    "dat.fillna('', inplace=True)\n",
    "\n",
    "# train/test split\n",
    "np.random.seed(111)\n",
    "dat = dat.sample(frac=1)\n",
    "\n",
    "train_size = int(.8*dat.shape[0])\n",
    "train_data = dat[:train_size]\n",
    "test_data = dat[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the the data\n",
    "def prep_data(df):\n",
    "    df['Context'] = df['Context'].map(lambda x: article_process_text(x))\n",
    "    df['TaskSentence'] = df['TaskSentence'].map(lambda x: article_process_text(x))\n",
    "    df['Summary'] = df['Summary'].map(lambda x: summary_process_text(x))\n",
    "    return df\n",
    "train_data = prep_data(train_data)\n",
    "test_data = prep_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Task</th>\n",
       "      <th>Context</th>\n",
       "      <th>TaskSentence</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Labeler</th>\n",
       "      <th>NoRequestInContext</th>\n",
       "      <th>Urgent</th>\n",
       "      <th>NotRequest</th>\n",
       "      <th>Unsure/Discuss</th>\n",
       "      <th>RandomNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1083</td>\n",
       "      <td>5187</td>\n",
       "      <td>Please give me a shout with</td>\n",
       "      <td>Attached is the file that I use from Storey to...</td>\n",
       "      <td>Please give me a shout with any questions.</td>\n",
       "      <td>contact SENDER with questions</td>\n",
       "      <td>Natalie</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.053197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>385</td>\n",
       "      <td>Please respond whether we have your most updat...</td>\n",
       "      <td>Hi, I'm Shawna with Icon. I am updating our da...</td>\n",
       "      <td>Please respond whether we have your most updat...</td>\n",
       "      <td>check whether information is updated</td>\n",
       "      <td>Rowan</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.282344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1348</td>\n",
       "      <td>5452</td>\n",
       "      <td>plz call when you can.</td>\n",
       "      <td></td>\n",
       "      <td>plz call when you can 415-7827-822 .</td>\n",
       "      <td>call SENDER at 415-7827-822</td>\n",
       "      <td>Natalie</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0.779285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>1727</td>\n",
       "      <td>Karen, please call me when you receive this em...</td>\n",
       "      <td></td>\n",
       "      <td>Karen, please call me when you receive this em...</td>\n",
       "      <td>call SENDER</td>\n",
       "      <td>Percy</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.929963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694</td>\n",
       "      <td>3461</td>\n",
       "      <td>Please look over the attached at your convenie...</td>\n",
       "      <td>I have them revised the document to reflect th...</td>\n",
       "      <td>Please look over the attached at your convenie...</td>\n",
       "      <td>look over attached documents</td>\n",
       "      <td>George</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.989056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1241</td>\n",
       "      <td>5345</td>\n",
       "      <td>IMAGE To receive our e-mails in a text-only fo...</td>\n",
       "      <td>Your kind of clothes. Online. All the time. IM...</td>\n",
       "      <td>please reply to this message and type change t...</td>\n",
       "      <td>reply to SENDER and type change to text</td>\n",
       "      <td>Natalie</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.308842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>306</td>\n",
       "      <td>In light of this, please make sure your Confir...</td>\n",
       "      <td>Diane or Patrick, I need help from one of you ...</td>\n",
       "      <td>In light of this, please make sure your Confir...</td>\n",
       "      <td>ensure confirmation copy includes the GTC</td>\n",
       "      <td>Rowan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.580914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>248</td>\n",
       "      <td>Please review the attached non-standard discou...</td>\n",
       "      <td>I'm OK on this, with one small change. Please ...</td>\n",
       "      <td>Please review the attached non-standard discou...</td>\n",
       "      <td>review attached letter</td>\n",
       "      <td>Rowan</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.149520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "      <td>Please, check the logic of my</td>\n",
       "      <td>Headcount gives the allocations by major busin...</td>\n",
       "      <td>Please, check the logic of my modifications I ...</td>\n",
       "      <td>check logic of modifications</td>\n",
       "      <td>Rowan</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.305235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>268</td>\n",
       "      <td>Can you call me on this one?</td>\n",
       "      <td>The first one looks ok to me as long as someon...</td>\n",
       "      <td>Can you call me on this one?</td>\n",
       "      <td>call SENDER</td>\n",
       "      <td>Rowan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.030236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1157 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         #                                               Task  \\\n",
       "1083  5187                        Please give me a shout with   \n",
       "384    385  Please respond whether we have your most updat...   \n",
       "1348  5452                             plz call when you can.   \n",
       "535   1727  Karen, please call me when you receive this em...   \n",
       "694   3461  Please look over the attached at your convenie...   \n",
       "...    ...                                                ...   \n",
       "1241  5345  IMAGE To receive our e-mails in a text-only fo...   \n",
       "305    306  In light of this, please make sure your Confir...   \n",
       "247    248  Please review the attached non-standard discou...   \n",
       "181    182                      Please, check the logic of my   \n",
       "267    268                       Can you call me on this one?   \n",
       "\n",
       "                                                Context  \\\n",
       "1083  Attached is the file that I use from Storey to...   \n",
       "384   Hi, I'm Shawna with Icon. I am updating our da...   \n",
       "1348                                                      \n",
       "535                                                       \n",
       "694   I have them revised the document to reflect th...   \n",
       "...                                                 ...   \n",
       "1241  Your kind of clothes. Online. All the time. IM...   \n",
       "305   Diane or Patrick, I need help from one of you ...   \n",
       "247   I'm OK on this, with one small change. Please ...   \n",
       "181   Headcount gives the allocations by major busin...   \n",
       "267   The first one looks ok to me as long as someon...   \n",
       "\n",
       "                                           TaskSentence  \\\n",
       "1083         Please give me a shout with any questions.   \n",
       "384   Please respond whether we have your most updat...   \n",
       "1348               plz call when you can 415-7827-822 .   \n",
       "535   Karen, please call me when you receive this em...   \n",
       "694   Please look over the attached at your convenie...   \n",
       "...                                                 ...   \n",
       "1241  please reply to this message and type change t...   \n",
       "305   In light of this, please make sure your Confir...   \n",
       "247   Please review the attached non-standard discou...   \n",
       "181   Please, check the logic of my modifications I ...   \n",
       "267                        Can you call me on this one?   \n",
       "\n",
       "                                        Summary  Labeler NoRequestInContext  \\\n",
       "1083              contact SENDER with questions  Natalie                  1   \n",
       "384        check whether information is updated    Rowan                      \n",
       "1348                call SENDER at 415-7827-822  Natalie                  1   \n",
       "535                                 call SENDER    Percy                      \n",
       "694                look over attached documents   George                      \n",
       "...                                         ...      ...                ...   \n",
       "1241    reply to SENDER and type change to text  Natalie                  1   \n",
       "305   ensure confirmation copy includes the GTC    Rowan                  1   \n",
       "247                      review attached letter    Rowan                      \n",
       "181                check logic of modifications    Rowan                      \n",
       "267                                 call SENDER    Rowan                  1   \n",
       "\n",
       "     Urgent NotRequest Unsure/Discuss  RandomNumber  \n",
       "1083      0          1                     0.053197  \n",
       "384       0                                0.282344  \n",
       "1348      0          0                     0.779285  \n",
       "535                  1                     0.929963  \n",
       "694                                        0.989056  \n",
       "...     ...        ...            ...           ...  \n",
       "1241      0          1                     0.308842  \n",
       "305       0                                0.580914  \n",
       "247       0                                0.149520  \n",
       "181                                        0.305235  \n",
       "267       0                                0.030236  \n",
       "\n",
       "[1157 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished constructing vocabulary of 4654 total words. Last word added: included\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab.from_vocab_file(os.path.join(config.log_root, 'data/vocab.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2732"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.word2id('now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "T.manual_seed(123)\n",
    "if T.cuda.is_available():\n",
    "    T.cuda.manual_seed_all(123)\n",
    "    \n",
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mle = \"yes\"\n",
    "train_rl = \"no\"\n",
    "mle_weight = 1.0\n",
    "load_model = None\n",
    "new_lr = None\n",
    "rl_weight = 1 - mle_weight\n",
    "\n",
    "opt = Namespace(train_mle = train_mle, \n",
    "                train_rl = train_rl, \n",
    "                mle_weight = mle_weight, \n",
    "                load_model = load_model,\n",
    "                new_lr = new_lr, \n",
    "                rl_weight = rl_weight)\n",
    "\n",
    "task_batcher = TaskBatcher(\n",
    "    examples=train_data.to_dict('records'),\n",
    "    vocab=vocab,\n",
    "    mode='train',\n",
    "    batch_size=32,\n",
    "    single_pass=False\n",
    ")\n",
    "\n",
    "val_task_batcher = TaskBatcher( # Batching obj\n",
    "    examples=train_data.to_dict('records')[:200],\n",
    "    vocab=vocab, \n",
    "    mode='train', \n",
    "    batch_size=50, \n",
    "    single_pass=False\n",
    ")\n",
    "\n",
    "\n",
    "train_processor = TaskTrain(vocab, task_batcher, opt, TaskModel, val_task_batcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1 mle_loss: 6.290 mle_loss_val: -100.0000\n",
      "iter: 2 mle_loss: 5.847 mle_loss_val: -100.0000\n",
      "iter: 3 mle_loss: 6.003 mle_loss_val: -100.0000\n",
      "iter: 4 mle_loss: 5.632 mle_loss_val: -100.0000\n",
      "iter: 5 mle_loss: 5.439 mle_loss_val: -100.0000\n",
      "iter: 6 mle_loss: 5.186 mle_loss_val: -100.0000\n",
      "iter: 7 mle_loss: 4.832 mle_loss_val: -100.0000\n",
      "iter: 8 mle_loss: 4.664 mle_loss_val: -100.0000\n",
      "iter: 9 mle_loss: 4.020 mle_loss_val: -100.0000\n",
      "-------------------Keyboard Interrupt------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/capstone/LSTM_Summarizer/train.py\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(self, n_iters, report_every, save_every)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m                 \u001b[0mmle_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/capstone/LSTM_Summarizer/train.py\u001b[0m in \u001b[0;36mtrain_one_batch\u001b[0;34m(self, batch, iter, no_grad)\u001b[0m\n\u001b[1;32m    327\u001b[0m             mle_loss = self.train_batch_MLE(enc_out, enc_hidden, enc_padding_mask, context, \n\u001b[0;32m--> 328\u001b[0;31m                                             extra_zeros, enc_batch_extend_vocab, batch)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mno_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/capstone/LSTM_Summarizer/train.py\u001b[0m in \u001b[0;36mtrain_batch_MLE\u001b[0;34m(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, batch)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0msum_temporal_srcs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mprev_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/capstone/LSTM_Summarizer/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_t, s_t, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mct_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_s\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m#intra-decoder attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/capstone/LSTM_Summarizer/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, s_t, prev_s)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# Standard attention technique (eq 1 in https://arxiv.org/pdf/1704.04368.pdf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0met\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_prev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_s\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m# bs,t-1,n_hid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0mdec_fea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# bs,1,n_hid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7c6d5f1a3406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/lstm_seg_ent_2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmle_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/capstone/LSTM_Summarizer/train.py\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(self, n_iters, report_every, save_every)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------Keyboard Interrupt------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m                 \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mmle_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmle_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "config.save_model_path = \"data/lstm_seg_ent_2\"\n",
    "\n",
    "mle_losses = train_processor.trainIters(n_iters=200, report_every=1, save_every = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_path /Users/rowancassius/Desktop/capstone/LSTM_Summarizer/data/lstm_seg_ent_2/0000200.tar\n",
      "example_generator completed reading all examples. No more data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-20:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rowancassius/Desktop/capstone/LSTM_Summarizer/data_util/batcher.py\", line 443, in text_generator\n",
      "    example = next(example_generator)\n",
      "StopIteration\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rowancassius/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/rowancassius/opt/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/rowancassius/Desktop/capstone/LSTM_Summarizer/data_util/batcher.py\", line 424, in fill_example_queue\n",
      "    context, task, summary = next(input_gen) # read the next example from file. article and abstract are both strings.\n",
      "RuntimeError: generator raised StopIteration\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config.save_model_path = \"data/lstm_seg_ent_2\"\n",
    "\n",
    "task = \"validate\"\n",
    "\n",
    "load_model = os.path.join(config.log_root, \"data/lstm_seg_ent_2/0000200.tar\") # model directory\n",
    "\n",
    "opt = Namespace(task = task, load_model = load_model) # opt\n",
    "\n",
    "\n",
    "# new batcher for evaluation\n",
    "task_batcher = TaskBatcher( # Batching obj\n",
    "    examples=test_data.to_dict('records'),\n",
    "    vocab=vocab, \n",
    "    mode='train', \n",
    "    batch_size=10, \n",
    "    single_pass=True)\n",
    "\n",
    "eval_processor = TaskEvaluate(vocab, task_batcher, opt, TaskModel) # Evaluation object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14413103"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(eval_processor.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.lstm.weight_ih_l0',\n",
       "              tensor([[-0.0954,  0.0502,  0.1390,  ...,  0.0518, -0.0395,  0.0514],\n",
       "                      [ 0.0487,  0.0290, -0.0217,  ...,  0.0148, -0.0193,  0.0253],\n",
       "                      [-0.0725,  0.0572,  0.1141,  ...,  0.0124, -0.0181,  0.0087],\n",
       "                      ...,\n",
       "                      [-0.1301,  0.1141,  0.1222,  ...,  0.0576, -0.0336,  0.0744],\n",
       "                      [-0.1076,  0.0807,  0.0551,  ..., -0.0027, -0.0076,  0.0451],\n",
       "                      [-0.1604,  0.1239,  0.1439,  ...,  0.0289, -0.0160,  0.0375]])),\n",
       "             ('encoder.lstm.weight_hh_l0',\n",
       "              tensor([[ 0.0335, -0.0072, -0.0085,  ..., -0.0220, -0.0239,  0.0236],\n",
       "                      [ 0.0034, -0.0056,  0.0198,  ...,  0.0085, -0.0093,  0.0221],\n",
       "                      [-0.0015,  0.0040,  0.0262,  ..., -0.0231,  0.0076, -0.0066],\n",
       "                      ...,\n",
       "                      [ 0.0188, -0.0020, -0.0154,  ..., -0.0133, -0.0255,  0.0245],\n",
       "                      [-0.0251, -0.0021,  0.0064,  ...,  0.0062,  0.0011, -0.0055],\n",
       "                      [ 0.0141, -0.0161,  0.0261,  ..., -0.0114, -0.0083,  0.0102]])),\n",
       "             ('encoder.lstm.bias_ih_l0',\n",
       "              tensor([ 0.0183,  0.0016, -0.0013,  ...,  0.0036,  0.0032, -0.0209])),\n",
       "             ('encoder.lstm.bias_hh_l0',\n",
       "              tensor([ 0.0183,  0.0016, -0.0013,  ...,  0.0036,  0.0032, -0.0209])),\n",
       "             ('encoder.lstm.weight_ih_l0_reverse',\n",
       "              tensor([[-0.0296,  0.0018,  0.0323,  ...,  0.0010,  0.0120, -0.0229],\n",
       "                      [-0.0560,  0.0189,  0.0676,  ...,  0.0041, -0.0198,  0.0577],\n",
       "                      [-0.0274,  0.0123, -0.0156,  ...,  0.0307, -0.0343, -0.0254],\n",
       "                      ...,\n",
       "                      [-0.0160,  0.0576,  0.0566,  ...,  0.0024, -0.0161,  0.0259],\n",
       "                      [ 0.0212, -0.0131, -0.0400,  ..., -0.0144,  0.0080, -0.0101],\n",
       "                      [-0.0136,  0.0336,  0.0090,  ...,  0.0245, -0.0099, -0.0104]])),\n",
       "             ('encoder.lstm.weight_hh_l0_reverse',\n",
       "              tensor([[-0.0107, -0.0202, -0.0167,  ..., -0.0122,  0.0024, -0.0062],\n",
       "                      [-0.0445,  0.0323,  0.0066,  ..., -0.0197,  0.0184,  0.0196],\n",
       "                      [-0.0314,  0.0031, -0.0082,  ..., -0.0051,  0.0172, -0.0099],\n",
       "                      ...,\n",
       "                      [-0.0005,  0.0149, -0.0015,  ..., -0.0086,  0.0222,  0.0015],\n",
       "                      [-0.0094,  0.0121,  0.0124,  ..., -0.0233, -0.0189,  0.0054],\n",
       "                      [-0.0282,  0.0096,  0.0210,  ..., -0.0043,  0.0104, -0.0027]])),\n",
       "             ('encoder.lstm.bias_ih_l0_reverse',\n",
       "              tensor([-0.0061,  0.0152, -0.0110,  ..., -0.0043, -0.0091, -0.0096])),\n",
       "             ('encoder.lstm.bias_hh_l0_reverse',\n",
       "              tensor([-0.0061,  0.0152, -0.0110,  ..., -0.0043, -0.0091, -0.0096])),\n",
       "             ('encoder.reduce_h.weight',\n",
       "              tensor([[ 2.8476e-03, -1.7751e-02,  1.9323e-02,  ...,  1.3647e-02,\n",
       "                        1.8669e-03,  3.1974e-03],\n",
       "                      [-1.3045e-02, -8.4241e-03,  9.8124e-03,  ...,  1.2153e-02,\n",
       "                        9.1335e-03, -1.1545e-02],\n",
       "                      [ 2.1842e-02, -1.6221e-02,  1.8016e-02,  ...,  1.3562e-02,\n",
       "                        5.1675e-03, -3.6073e-03],\n",
       "                      ...,\n",
       "                      [-6.7098e-03, -9.9436e-03,  8.7237e-03,  ..., -7.8329e-03,\n",
       "                        1.1370e-02, -9.0368e-03],\n",
       "                      [-8.4507e-03,  1.5760e-03, -1.3997e-02,  ..., -8.1149e-03,\n",
       "                        2.0610e-03, -3.7648e-03],\n",
       "                      [-1.8836e-02, -7.8512e-03,  4.9325e-03,  ...,  5.3522e-03,\n",
       "                        3.3608e-03,  5.8930e-05]])),\n",
       "             ('encoder.reduce_h.bias',\n",
       "              tensor([-1.9218e-02, -3.7959e-03, -2.4055e-02,  2.2090e-03, -8.6016e-03,\n",
       "                       1.4408e-02,  1.6196e-02, -3.3903e-02, -9.7897e-03,  2.6086e-03,\n",
       "                       9.9128e-04,  1.3366e-02, -3.0456e-02,  7.5852e-03,  1.1792e-02,\n",
       "                      -1.6315e-02, -1.2337e-02,  8.9947e-03,  2.0682e-02,  1.6836e-02,\n",
       "                      -1.7823e-03,  5.9097e-03, -7.0571e-03, -1.0336e-02,  1.0288e-02,\n",
       "                      -1.6078e-02, -9.6047e-03, -1.7718e-02, -2.1102e-02, -2.8777e-02,\n",
       "                      -6.1569e-03, -7.4230e-03, -8.4734e-03,  2.6646e-02, -1.4756e-02,\n",
       "                      -2.7266e-02, -1.6558e-02, -2.7502e-02, -1.1426e-02, -8.7004e-03,\n",
       "                      -2.2931e-03, -5.7909e-03, -9.5529e-03, -1.1911e-02, -1.0576e-02,\n",
       "                      -8.1837e-03,  2.2679e-02, -4.7283e-03, -9.7631e-03,  3.4507e-02,\n",
       "                       1.3445e-02, -6.3536e-03,  1.2984e-02,  3.5472e-02,  2.0568e-02,\n",
       "                      -2.0262e-03, -2.5949e-03,  2.7129e-02, -2.1351e-02,  1.9169e-03,\n",
       "                       1.9819e-02,  1.6225e-02,  1.4579e-02, -2.7281e-03,  2.2364e-02,\n",
       "                      -2.0708e-02, -2.4186e-02, -5.7825e-03, -6.5052e-04, -2.2252e-03,\n",
       "                       1.5639e-02, -1.9955e-03, -6.1157e-03,  6.7723e-03,  4.3326e-03,\n",
       "                       1.1966e-02, -1.3742e-02,  1.7496e-02,  6.3467e-04,  1.6874e-02,\n",
       "                      -1.0291e-03, -1.3414e-02, -1.6689e-02,  9.1700e-03, -1.7114e-04,\n",
       "                       1.7499e-02, -4.2570e-03,  6.6057e-03, -1.1238e-02, -1.1696e-02,\n",
       "                       1.4251e-02,  1.0914e-02,  1.5872e-02, -1.3729e-03,  3.6229e-02,\n",
       "                      -1.2341e-02, -1.0873e-02, -1.3393e-02, -5.0387e-03,  3.3153e-03,\n",
       "                      -1.7321e-02,  3.1169e-02, -7.1207e-03, -1.3360e-03, -1.4775e-02,\n",
       "                       1.1907e-03, -9.2157e-04,  2.5860e-02,  1.0151e-02, -1.4414e-02,\n",
       "                       7.3167e-04, -1.9564e-02, -6.0388e-03, -2.1898e-03, -1.1547e-02,\n",
       "                      -8.2129e-03,  6.3618e-03,  6.0380e-03, -7.1688e-03,  2.5923e-03,\n",
       "                       1.1532e-02, -1.1798e-02, -1.4808e-02,  1.8427e-02, -1.4865e-02,\n",
       "                       4.7784e-03,  3.8177e-03, -8.9226e-03, -1.4705e-03, -1.2043e-02,\n",
       "                      -7.6682e-03, -7.2540e-03,  1.9903e-02, -1.9755e-02,  1.9477e-02,\n",
       "                       8.2544e-04, -5.2914e-03, -2.4668e-02,  2.0791e-02, -6.4789e-03,\n",
       "                      -5.9043e-03,  1.5254e-02, -1.4788e-03, -1.3214e-02, -2.1580e-03,\n",
       "                       1.3590e-03, -2.6748e-03,  7.9397e-03, -2.0173e-03,  1.1352e-02,\n",
       "                      -9.9577e-03, -2.2966e-02, -1.7690e-02,  2.0386e-02,  7.7583e-03,\n",
       "                      -3.2424e-02,  1.1400e-02, -1.0173e-02, -1.7243e-02, -1.1999e-02,\n",
       "                      -4.9093e-03, -2.2443e-02, -1.3325e-03, -2.1824e-02,  3.7551e-03,\n",
       "                      -8.1209e-03, -1.1198e-02, -2.6597e-02, -2.1636e-02,  1.9172e-02,\n",
       "                      -6.6023e-03, -5.2619e-03,  2.4575e-02, -9.6662e-03,  2.3721e-02,\n",
       "                      -5.3721e-03, -1.6337e-02, -2.3628e-02,  5.7788e-03, -1.0150e-02,\n",
       "                       2.1885e-02,  1.6095e-02, -7.3037e-03, -9.7794e-03,  3.3371e-02,\n",
       "                       2.0568e-02, -9.7613e-03,  2.0282e-02, -8.0428e-03,  5.6884e-03,\n",
       "                      -1.5165e-02,  2.4295e-03, -2.6126e-03, -3.7380e-03, -5.1925e-03,\n",
       "                       5.5043e-03,  1.1311e-03, -1.8952e-02, -1.0807e-02, -1.4667e-02,\n",
       "                       2.1793e-02,  8.1251e-03,  5.1830e-04,  8.4640e-03, -1.5857e-03,\n",
       "                       2.5019e-02, -5.5693e-03, -5.8741e-03, -6.5237e-03,  1.7232e-02,\n",
       "                       1.0005e-02, -1.1883e-02, -1.4450e-02, -1.8596e-03,  2.0640e-02,\n",
       "                       1.6492e-04,  2.6905e-02, -1.3925e-02,  1.4365e-02, -1.1676e-02,\n",
       "                       2.0695e-02, -2.1112e-02, -1.0208e-02,  1.0756e-02,  4.0065e-03,\n",
       "                       2.0645e-02, -4.8008e-03, -2.2520e-02,  4.2699e-03, -2.2049e-02,\n",
       "                      -2.8816e-02, -9.4445e-03,  8.1777e-03, -6.7783e-03,  3.7729e-04,\n",
       "                      -1.0402e-02, -1.0532e-02, -1.2268e-02,  2.1525e-02,  3.3908e-03,\n",
       "                      -2.7847e-02, -2.8424e-03, -2.3259e-02,  2.0321e-03, -1.3833e-02,\n",
       "                      -2.7809e-02, -9.1426e-03, -1.1297e-02,  8.1607e-03, -1.0020e-02,\n",
       "                      -4.4386e-04, -2.0403e-02, -8.5911e-03,  2.9049e-02, -8.0414e-03,\n",
       "                       1.3403e-03,  1.4237e-02, -5.3932e-03,  1.9029e-02,  1.7665e-02,\n",
       "                       1.9407e-02, -2.0508e-02,  3.9802e-03, -1.0206e-02, -9.3598e-03,\n",
       "                      -5.0260e-04, -3.9918e-03,  1.6779e-03, -1.3107e-02, -1.4158e-02,\n",
       "                       1.5938e-02,  1.2144e-02, -1.1479e-02, -1.1496e-02, -1.1520e-02,\n",
       "                      -8.3644e-03,  2.2525e-02,  6.2712e-03, -1.9331e-02,  1.0485e-02,\n",
       "                       5.2668e-03,  1.9119e-03,  1.4478e-02,  3.0561e-02, -8.2335e-03,\n",
       "                       2.1978e-04, -9.3805e-03,  2.5268e-02, -1.7370e-02, -2.3013e-02,\n",
       "                      -9.9622e-03, -8.8737e-03,  1.0617e-03, -1.1339e-02,  6.3808e-03,\n",
       "                      -1.0365e-02,  7.1723e-03, -7.9647e-03,  1.2270e-03, -5.4447e-03,\n",
       "                      -2.2648e-02,  4.1313e-03, -1.8554e-02,  1.0149e-02,  2.8593e-02,\n",
       "                      -1.8215e-02,  2.7693e-02, -7.8401e-03,  1.3898e-02,  1.3338e-02,\n",
       "                      -2.4400e-03,  9.3977e-03, -1.1904e-02, -6.2277e-03, -1.9864e-02,\n",
       "                      -2.5356e-03,  1.4045e-02, -2.0379e-03,  5.0263e-03, -5.8125e-03,\n",
       "                      -1.2600e-02, -4.9822e-03, -5.6980e-03, -9.4688e-03, -7.7823e-03,\n",
       "                      -1.8197e-02, -8.0446e-03,  9.8651e-03, -7.0150e-03,  5.7873e-03,\n",
       "                      -2.0433e-02, -8.9066e-03,  1.6668e-02, -4.1029e-03,  2.6659e-03,\n",
       "                      -2.2484e-02, -4.1836e-03,  1.9123e-02, -7.2818e-03, -1.8085e-03,\n",
       "                      -6.7989e-03, -4.4689e-03, -1.2291e-02,  9.8600e-03, -9.7275e-03,\n",
       "                       3.0601e-02,  1.1970e-02, -8.4850e-03, -9.8956e-03, -1.6098e-03,\n",
       "                      -1.4256e-02, -1.2524e-02, -1.9582e-02,  3.0148e-03, -1.5768e-02,\n",
       "                       9.0329e-03, -2.2968e-02,  2.6122e-02, -2.3731e-02,  1.0121e-02,\n",
       "                      -1.6387e-02,  9.3590e-03, -5.8549e-03, -8.4807e-04, -1.2330e-02,\n",
       "                       1.8777e-02,  1.7606e-02, -3.1658e-02, -1.0162e-02, -1.0425e-02,\n",
       "                      -2.7745e-02,  4.1762e-03, -1.2246e-02, -5.6923e-03,  5.8228e-04,\n",
       "                       1.7882e-02, -9.9374e-03, -8.7857e-03, -2.8033e-03,  3.5307e-02,\n",
       "                       1.3517e-02, -9.5622e-03, -1.1142e-02, -5.4476e-03,  6.1053e-03,\n",
       "                       4.3413e-03, -8.6609e-03,  2.1019e-02,  1.5351e-02,  3.2641e-03,\n",
       "                       7.5320e-03,  2.4290e-02, -3.6983e-03, -4.3710e-03, -4.9644e-05,\n",
       "                      -6.9653e-03, -2.3027e-02, -3.8433e-03,  1.0519e-02, -6.5260e-03,\n",
       "                      -1.8448e-03, -2.0704e-02, -1.5019e-02, -5.8667e-03, -1.2851e-02,\n",
       "                       3.0346e-03, -4.2094e-03, -2.0604e-02, -1.4546e-03,  4.4095e-03,\n",
       "                      -3.3514e-02,  4.1549e-03, -4.8891e-03, -6.3490e-03, -8.1778e-03,\n",
       "                       9.8866e-04, -1.2689e-02, -8.1318e-03, -2.5307e-03, -8.4862e-03,\n",
       "                      -1.0827e-02, -1.4343e-02, -7.1656e-04, -8.6418e-03, -8.4322e-03,\n",
       "                      -4.1407e-04,  1.0256e-02,  1.0457e-02,  2.3893e-02,  1.6261e-02,\n",
       "                      -9.3602e-03,  2.7441e-02, -1.0735e-02,  2.7876e-02, -1.0689e-02,\n",
       "                      -1.8521e-03,  2.8265e-02, -8.8792e-03, -3.9358e-03, -2.0061e-02,\n",
       "                      -1.3261e-02,  1.4199e-02,  2.0645e-02, -3.1604e-02, -1.0303e-02,\n",
       "                       1.2586e-02,  5.8580e-03, -1.7256e-02, -1.4699e-02, -4.5597e-03,\n",
       "                       1.0953e-02, -4.8458e-03, -7.1770e-03,  3.0233e-02,  3.3601e-02,\n",
       "                      -6.4800e-03, -2.7179e-03, -5.7194e-03,  2.0605e-02,  7.5979e-03,\n",
       "                       2.8107e-02, -3.7091e-03, -2.7783e-03, -2.3691e-02, -6.6993e-04,\n",
       "                      -1.2042e-02,  5.4830e-03, -9.8456e-03, -6.8049e-03,  2.3109e-03,\n",
       "                      -6.3609e-03, -7.9813e-03,  2.1994e-02, -9.4731e-03, -9.7449e-03,\n",
       "                       3.0360e-03, -1.7647e-02, -7.4577e-03, -6.6051e-03, -2.8810e-02,\n",
       "                       1.1812e-02, -1.7207e-02, -8.7951e-04,  2.8229e-02, -9.7797e-03,\n",
       "                      -2.6001e-03, -2.1455e-02,  2.8133e-02, -3.2671e-02, -1.0603e-04,\n",
       "                      -9.3079e-03, -1.4881e-02,  8.4103e-03,  2.5204e-02, -3.6883e-03,\n",
       "                      -1.0780e-02, -6.1527e-03, -7.7959e-03,  2.5851e-02, -6.9971e-03,\n",
       "                       1.3230e-03,  8.2659e-04, -1.8859e-02, -3.1391e-03,  1.0251e-02,\n",
       "                       1.0697e-02, -4.2570e-03,  3.7962e-03,  2.3776e-02,  3.9858e-03,\n",
       "                       2.1052e-02,  1.6247e-02])),\n",
       "             ('encoder.reduce_c.weight',\n",
       "              tensor([[ 0.0099, -0.0053,  0.0082,  ...,  0.0036, -0.0033,  0.0053],\n",
       "                      [-0.0095,  0.0033, -0.0012,  ..., -0.0088, -0.0043,  0.0024],\n",
       "                      [-0.0247,  0.0025, -0.0057,  ..., -0.0034, -0.0038,  0.0042],\n",
       "                      ...,\n",
       "                      [ 0.0021, -0.0001,  0.0014,  ...,  0.0057,  0.0054, -0.0055],\n",
       "                      [ 0.0094, -0.0105,  0.0095,  ...,  0.0012,  0.0046, -0.0041],\n",
       "                      [ 0.0026, -0.0011,  0.0025,  ...,  0.0053,  0.0057, -0.0046]])),\n",
       "             ('encoder.reduce_c.bias',\n",
       "              tensor([-1.1657e-02,  3.6295e-03,  2.7088e-02,  9.6647e-03,  1.4049e-02,\n",
       "                      -1.6839e-02, -1.1079e-04, -1.1568e-02, -6.0518e-03, -1.2829e-02,\n",
       "                      -1.2631e-02,  1.5637e-02, -7.4831e-03,  8.2013e-03, -1.1666e-02,\n",
       "                      -4.3714e-03,  2.3781e-02, -4.7200e-03,  1.8537e-02, -1.6686e-02,\n",
       "                      -7.4791e-03,  1.7304e-02, -1.0456e-03,  1.8013e-02, -5.3587e-03,\n",
       "                      -1.1045e-02, -1.4831e-02, -1.2020e-02, -1.5087e-02, -9.3381e-03,\n",
       "                       2.0158e-02, -7.1677e-03,  1.7478e-02, -5.3877e-03, -5.7382e-03,\n",
       "                      -1.5281e-02, -3.6982e-03, -8.0942e-03,  7.7390e-03,  4.5311e-03,\n",
       "                       2.7662e-03,  9.3379e-03, -1.8995e-02,  3.5678e-03, -6.4509e-03,\n",
       "                      -1.1036e-02, -2.0449e-02,  2.5858e-02,  2.7625e-02, -5.8035e-03,\n",
       "                       1.7139e-02, -1.2620e-02, -1.7394e-02, -1.0043e-02,  7.7215e-03,\n",
       "                       2.3374e-02, -2.0197e-02, -5.3881e-03,  7.1947e-03,  1.0089e-02,\n",
       "                       2.6215e-02, -2.8082e-03, -1.0497e-02, -1.3303e-02, -1.0368e-02,\n",
       "                      -6.1177e-03, -1.3436e-02, -1.2915e-02,  2.6735e-02,  1.6147e-02,\n",
       "                       2.4534e-02, -5.3589e-03, -8.8520e-03, -1.5568e-02, -8.6352e-03,\n",
       "                       6.3120e-03, -1.1130e-02,  8.1797e-03,  2.6341e-02,  2.9508e-02,\n",
       "                       1.8316e-02, -1.0949e-02,  1.9981e-02, -5.5905e-03, -8.1650e-03,\n",
       "                       1.3186e-02,  1.9117e-02,  1.3319e-02,  1.2227e-02, -1.7193e-02,\n",
       "                      -3.9503e-03,  2.7039e-02, -6.0022e-03, -1.4050e-02, -1.8910e-02,\n",
       "                      -2.5942e-02,  2.0071e-02, -1.3495e-02, -1.1397e-02, -4.5937e-03,\n",
       "                       1.8301e-02, -1.1533e-02, -1.1413e-02,  2.0949e-02, -3.1773e-03,\n",
       "                      -1.0493e-02, -2.8631e-03, -2.9934e-02,  3.0372e-02, -2.8851e-03,\n",
       "                      -1.1480e-02, -1.1394e-02, -3.4827e-03, -7.8718e-03, -8.5477e-03,\n",
       "                      -1.7610e-02, -1.1590e-02, -1.8675e-02, -6.5031e-03,  1.7017e-03,\n",
       "                      -1.0251e-02,  2.5920e-02,  3.3811e-04,  1.1503e-02,  1.2598e-03,\n",
       "                      -2.3410e-02, -1.1035e-02, -1.0547e-02,  2.5943e-02,  3.7237e-03,\n",
       "                      -5.4870e-03, -3.9062e-03,  2.6424e-02, -1.7895e-02, -2.7789e-02,\n",
       "                       4.6434e-05,  1.6669e-02, -6.5603e-03, -8.8537e-03, -1.1710e-02,\n",
       "                       9.9689e-03,  2.4638e-02, -4.9735e-03, -7.9132e-03,  2.0789e-02,\n",
       "                       5.9782e-03, -5.0871e-03,  2.3708e-03,  2.5641e-02, -1.2205e-02,\n",
       "                      -1.0966e-02, -1.1675e-02, -1.1162e-02, -1.1593e-02, -6.0787e-03,\n",
       "                      -1.8437e-02, -4.5914e-03, -6.2329e-03,  1.7462e-02, -2.0163e-02,\n",
       "                       2.6197e-02,  7.9304e-03,  2.6446e-02, -1.6775e-02, -1.0818e-02,\n",
       "                      -8.9001e-03, -2.6500e-03, -5.8319e-03, -6.8531e-03, -1.2889e-02,\n",
       "                       5.1276e-03,  3.2295e-02, -1.1552e-02, -1.3534e-02, -1.7921e-03,\n",
       "                      -7.7622e-03, -2.1884e-02, -1.2693e-02, -5.1929e-03, -1.1348e-02,\n",
       "                      -5.6302e-03,  1.3754e-02, -1.3762e-02, -1.8908e-04, -5.0152e-03,\n",
       "                       3.9388e-03,  1.8965e-02, -7.5917e-03,  4.7552e-03, -1.4844e-02,\n",
       "                       9.9741e-03, -1.9076e-02, -5.7165e-03,  1.8656e-02, -1.4257e-03,\n",
       "                      -1.1755e-02, -2.3879e-02, -1.5948e-02, -8.6192e-03, -1.1159e-02,\n",
       "                      -2.2250e-02, -3.3941e-04,  9.9900e-03, -1.3370e-03,  2.4029e-02,\n",
       "                      -5.9478e-03, -9.5794e-03, -1.1071e-02,  9.3319e-04, -5.2112e-03,\n",
       "                      -9.6277e-03, -8.7715e-03,  3.2276e-02,  2.7312e-03,  1.0313e-02,\n",
       "                       2.7661e-02, -1.5755e-02, -2.0643e-02, -1.1670e-02,  8.8330e-03,\n",
       "                       2.4565e-02, -5.6368e-03,  2.5746e-02, -1.7841e-02,  2.6553e-02,\n",
       "                      -7.0103e-03, -5.0191e-03, -1.1596e-02, -1.4012e-02, -6.8230e-03,\n",
       "                      -1.0598e-02,  2.0567e-02,  7.7147e-03, -1.0862e-02,  2.1667e-02,\n",
       "                       6.3956e-04, -1.1602e-02,  4.4368e-03,  2.5320e-02,  1.4299e-02,\n",
       "                      -1.1459e-02,  1.7417e-02,  2.9617e-03, -1.3501e-02,  1.3387e-02,\n",
       "                      -1.7359e-02,  2.2050e-02, -1.7020e-02,  8.6534e-03, -1.1875e-02,\n",
       "                       7.3421e-03, -2.1075e-02,  1.1244e-02, -1.1735e-02, -6.3095e-03,\n",
       "                      -1.1169e-02, -3.5682e-05, -4.0731e-03, -6.1998e-03,  2.1719e-02,\n",
       "                      -1.2432e-02, -5.7766e-03, -6.9114e-03,  2.5734e-03,  2.2369e-03,\n",
       "                      -1.1343e-02,  2.7762e-02,  2.8371e-02,  1.1766e-02, -1.1641e-02,\n",
       "                      -1.2014e-04, -7.8985e-03, -1.1626e-02, -1.1137e-02,  2.6662e-04,\n",
       "                      -7.4582e-03, -2.0204e-02, -1.9658e-02, -1.2900e-02, -1.5751e-02,\n",
       "                      -2.3159e-03, -8.1777e-03, -8.4404e-03, -5.9278e-03, -1.9455e-03,\n",
       "                      -2.4354e-03,  2.1116e-02, -2.7820e-02, -1.1698e-02, -2.0308e-02,\n",
       "                      -9.9751e-03,  2.8808e-02, -1.5704e-02,  1.7528e-02, -5.1816e-03,\n",
       "                      -5.7662e-03, -3.6238e-03, -8.9637e-04, -1.2040e-02, -4.6022e-03,\n",
       "                      -1.4867e-02,  2.6427e-02,  5.1178e-03, -1.1308e-02,  8.0796e-03,\n",
       "                      -1.1385e-02, -5.8277e-03, -1.1421e-02, -1.2848e-02, -1.1808e-02,\n",
       "                      -9.9486e-03,  9.1277e-04, -1.0581e-02, -1.1579e-02, -2.9650e-03,\n",
       "                      -1.1691e-02,  1.3088e-03,  2.7676e-02, -7.8983e-03, -2.7626e-02,\n",
       "                       6.1742e-03, -9.1365e-03, -4.2418e-03, -5.7443e-03,  1.2851e-02,\n",
       "                       1.7228e-02,  2.5798e-02,  2.9230e-02, -1.1413e-02,  2.4432e-02,\n",
       "                      -1.8492e-02, -9.8963e-03, -1.0004e-02,  2.3876e-02, -5.9334e-03,\n",
       "                      -5.9176e-03,  2.7166e-02, -9.9894e-03, -5.2747e-03, -1.0233e-02,\n",
       "                      -1.2556e-02,  2.2649e-02,  4.3384e-03, -3.8474e-03,  2.0113e-02,\n",
       "                      -1.0386e-02, -1.0769e-02,  2.6993e-02, -1.1293e-02, -9.0226e-03,\n",
       "                       2.6585e-02, -8.0073e-03,  9.8176e-03, -7.9529e-03, -5.6329e-03,\n",
       "                       2.7001e-03,  3.5463e-02, -1.8398e-02,  3.4172e-03,  1.9968e-05,\n",
       "                      -6.3451e-03, -2.1609e-02,  1.2277e-02, -1.4787e-02,  6.4032e-03,\n",
       "                      -5.0618e-03,  2.7031e-02, -6.8245e-03,  1.7611e-02, -1.1703e-02,\n",
       "                      -4.1241e-03, -9.5304e-03, -1.0733e-02,  6.7606e-04,  2.3757e-02,\n",
       "                      -1.1441e-02, -1.1715e-02,  1.0931e-02, -2.1087e-02, -6.8668e-03,\n",
       "                       7.9715e-03,  2.6054e-02, -7.8539e-03,  2.4566e-02,  2.7594e-02,\n",
       "                       2.0528e-02, -9.1500e-03,  2.4733e-02, -7.5709e-03,  1.9342e-02,\n",
       "                       2.9589e-02, -1.8049e-02,  1.7287e-02, -2.1812e-02,  5.4885e-03,\n",
       "                       2.6975e-02, -2.7627e-02, -1.3418e-02,  2.6307e-02, -1.1129e-02,\n",
       "                      -1.1236e-02,  1.1106e-03, -7.5357e-03, -3.7111e-03, -1.0754e-02,\n",
       "                       2.5653e-02, -1.0298e-02, -1.0788e-02, -4.6747e-03, -1.6233e-02,\n",
       "                      -1.1519e-02, -9.9589e-03,  2.9315e-02,  2.5973e-02,  1.5715e-02,\n",
       "                       1.1456e-02, -9.0303e-03, -9.3031e-03,  1.8623e-02, -1.5349e-02,\n",
       "                      -9.8446e-03, -1.1316e-02,  2.4283e-02,  4.6892e-03,  1.2031e-02,\n",
       "                      -1.1452e-02,  1.5447e-02, -3.7706e-03, -5.2008e-03,  2.7486e-02,\n",
       "                       2.6367e-02, -1.5054e-02, -2.8898e-03, -1.0291e-02,  2.3494e-02,\n",
       "                       2.4837e-02, -1.2340e-02, -1.2303e-03, -9.3430e-03,  8.4357e-03,\n",
       "                      -5.7457e-03, -1.1629e-02,  1.9358e-02, -1.1825e-02, -8.8089e-03,\n",
       "                      -2.4056e-03, -4.9596e-03, -1.6737e-02,  6.0252e-04, -9.1330e-03,\n",
       "                       1.2258e-02, -5.7213e-03,  5.9866e-03, -2.7435e-02, -1.0980e-02,\n",
       "                       3.2681e-02, -7.6327e-03, -1.1446e-02, -1.5491e-02,  1.0319e-02,\n",
       "                      -1.2153e-02, -2.2702e-02, -5.9948e-03, -1.2075e-02,  9.8331e-03,\n",
       "                       9.4908e-03, -1.3039e-02,  1.2599e-02, -1.0619e-02, -9.5532e-03,\n",
       "                       7.9226e-03, -9.3450e-04, -7.1266e-03, -7.4481e-03, -9.2542e-03,\n",
       "                      -8.5091e-03, -1.4524e-02,  8.3444e-03,  2.5057e-03, -2.0426e-02,\n",
       "                      -4.0471e-03, -5.4797e-03,  1.3932e-02, -1.3761e-02, -9.9968e-03,\n",
       "                       2.7822e-02, -1.8876e-03, -1.8191e-02, -1.6764e-02,  2.3856e-03,\n",
       "                      -5.9898e-03, -2.4985e-02,  2.3708e-02, -1.1403e-02, -1.1801e-02,\n",
       "                      -1.4967e-03, -7.8470e-03,  2.2069e-02,  5.2433e-03,  7.2271e-04,\n",
       "                      -1.0210e-02,  2.6771e-02, -8.1345e-03,  3.0693e-03, -1.0558e-02,\n",
       "                      -9.2004e-03,  2.8364e-02, -6.5825e-03, -4.6129e-03, -4.9706e-03,\n",
       "                      -6.1102e-03,  7.0403e-03])),\n",
       "             ('decoder.enc_attention.W_h.weight',\n",
       "              tensor([[ 0.0504,  0.0337, -0.0348,  ...,  0.0229, -0.0082, -0.0009],\n",
       "                      [-0.0297,  0.0285, -0.0163,  ..., -0.0272, -0.0368, -0.0132],\n",
       "                      [-0.0421,  0.0177,  0.0296,  ...,  0.0236, -0.0061, -0.0116],\n",
       "                      ...,\n",
       "                      [-0.0057, -0.0031,  0.0135,  ..., -0.0305, -0.0425,  0.0139],\n",
       "                      [ 0.0414,  0.0109,  0.0114,  ...,  0.0301, -0.0067, -0.0395],\n",
       "                      [-0.0170,  0.0177, -0.0257,  ...,  0.0288,  0.0041, -0.0196]])),\n",
       "             ('decoder.enc_attention.W_s.weight',\n",
       "              tensor([[-0.0336,  0.0119, -0.0070,  ...,  0.0058, -0.0161, -0.0056],\n",
       "                      [ 0.0102,  0.0300, -0.0015,  ...,  0.0065, -0.0170,  0.0155],\n",
       "                      [-0.0078, -0.0088,  0.0107,  ...,  0.0177, -0.0091, -0.0381],\n",
       "                      ...,\n",
       "                      [ 0.0201, -0.0124,  0.0023,  ..., -0.0182, -0.0364, -0.0313],\n",
       "                      [ 0.0020,  0.0074,  0.0236,  ..., -0.0066, -0.0180, -0.0091],\n",
       "                      [ 0.0280, -0.0004,  0.0055,  ..., -0.0227,  0.0105, -0.0297]])),\n",
       "             ('decoder.enc_attention.W_s.bias',\n",
       "              tensor([-0.0143,  0.0057, -0.0207,  ..., -0.0107,  0.0294,  0.0184])),\n",
       "             ('decoder.enc_attention.v.weight',\n",
       "              tensor([[ 0.0129, -0.0086,  0.0489,  ..., -0.0056,  0.0242,  0.0435]])),\n",
       "             ('decoder.dec_attention.W_prev.weight',\n",
       "              tensor([[ 1.5094e-02,  2.1943e-04,  5.3776e-03,  ...,  1.4964e-02,\n",
       "                       -8.0060e-04,  2.8060e-02],\n",
       "                      [-1.4776e-02, -1.0756e-03,  6.3321e-02,  ..., -1.6003e-02,\n",
       "                       -5.6640e-02,  5.6452e-02],\n",
       "                      [-2.0585e-02, -2.2773e-03, -7.8592e-02,  ..., -1.0346e-02,\n",
       "                        3.0829e-02, -4.4071e-03],\n",
       "                      ...,\n",
       "                      [ 2.1247e-02, -5.5173e-03,  2.4530e-02,  ..., -2.7989e-02,\n",
       "                        2.6380e-02, -2.5443e-02],\n",
       "                      [-4.8055e-03, -4.2620e-02, -6.4563e-02,  ...,  5.9376e-03,\n",
       "                        1.3129e-02, -2.0664e-02],\n",
       "                      [-7.8048e-05, -4.6942e-02,  2.5324e-02,  ..., -8.8277e-03,\n",
       "                       -2.1926e-02,  1.5631e-02]])),\n",
       "             ('decoder.dec_attention.W_s.weight',\n",
       "              tensor([[-0.0072, -0.0267, -0.0175,  ..., -0.0377,  0.0070, -0.0163],\n",
       "                      [-0.0565,  0.0009, -0.0396,  ..., -0.0277,  0.0130,  0.0506],\n",
       "                      [ 0.0417,  0.0234, -0.0272,  ..., -0.0231, -0.0541,  0.0296],\n",
       "                      ...,\n",
       "                      [-0.0032, -0.0465, -0.0224,  ...,  0.0451,  0.0196, -0.0435],\n",
       "                      [ 0.0312,  0.0273,  0.0286,  ..., -0.0068,  0.0209, -0.0304],\n",
       "                      [ 0.0012, -0.0472, -0.0003,  ...,  0.0194,  0.0275, -0.0148]])),\n",
       "             ('decoder.dec_attention.W_s.bias',\n",
       "              tensor([-3.2714e-02, -3.2533e-02, -6.7279e-03, -5.3349e-02,  3.4735e-02,\n",
       "                      -1.2599e-02, -3.0009e-02,  5.1469e-02, -3.4055e-02, -4.0300e-02,\n",
       "                       3.1439e-02,  4.9789e-03,  4.4328e-02, -4.6701e-02,  3.9648e-02,\n",
       "                      -3.5611e-02, -3.8942e-02,  2.3114e-02,  2.5190e-02,  3.0001e-02,\n",
       "                       3.3821e-02,  6.0900e-03,  4.3908e-02,  1.0358e-02, -4.2972e-02,\n",
       "                      -3.1288e-02, -2.6685e-02,  8.2303e-03, -8.2097e-03,  4.1357e-02,\n",
       "                      -8.4939e-03,  3.5697e-02,  2.6707e-02,  2.1093e-02,  9.2856e-04,\n",
       "                       2.2208e-02,  3.1770e-03, -1.0876e-02,  2.5741e-02,  1.9159e-02,\n",
       "                       8.3004e-03,  3.0025e-03,  3.6336e-02,  2.7171e-02,  1.5951e-02,\n",
       "                       4.0684e-03,  3.1454e-02,  3.6711e-02, -6.1928e-03,  4.7277e-02,\n",
       "                      -3.8940e-02, -5.0149e-03,  2.3876e-02, -2.6238e-02,  1.2074e-02,\n",
       "                      -7.4604e-03,  3.3982e-02,  4.6051e-02, -3.7591e-02,  1.9683e-02,\n",
       "                       2.1742e-02,  3.5952e-02,  1.0764e-02,  5.0989e-02, -4.7494e-03,\n",
       "                       4.6350e-02,  6.6073e-03,  8.9629e-03,  3.0678e-02,  4.7005e-02,\n",
       "                      -3.8687e-02,  1.7180e-02,  2.9378e-02,  2.4769e-02,  3.0886e-02,\n",
       "                      -4.4271e-02,  3.4231e-02,  3.6294e-02,  3.9632e-02, -3.7845e-02,\n",
       "                       1.4553e-02, -2.6952e-02, -2.0785e-02, -2.7679e-02, -1.7049e-02,\n",
       "                      -2.8148e-02, -1.7687e-02,  1.3640e-02,  5.1405e-02,  2.6682e-02,\n",
       "                      -4.1757e-02,  1.6080e-02,  1.1903e-03, -2.6849e-02,  1.8976e-02,\n",
       "                       1.7359e-02, -1.9908e-02, -2.5099e-02,  7.1478e-03,  1.6159e-02,\n",
       "                      -4.3691e-03, -4.7102e-03, -2.9662e-02, -2.7685e-02, -1.1086e-02,\n",
       "                      -1.9094e-02,  3.5568e-02, -1.1647e-02, -3.0664e-02,  2.6812e-02,\n",
       "                       1.6095e-03,  4.2892e-02, -3.0675e-02,  1.7751e-02, -2.2401e-02,\n",
       "                       4.7326e-02, -3.7759e-02,  3.0255e-02,  1.6029e-02,  7.2843e-03,\n",
       "                      -1.9556e-02,  2.5936e-02, -1.4961e-03,  2.3917e-02,  9.9406e-03,\n",
       "                       2.2168e-03, -2.8816e-02, -5.9742e-03, -1.9492e-02, -1.4242e-02,\n",
       "                       1.8653e-02,  3.2740e-02,  1.6165e-02, -3.7734e-02, -1.6210e-02,\n",
       "                      -5.8577e-03,  8.2063e-03, -2.4806e-02, -3.2531e-02, -2.4797e-03,\n",
       "                       3.6606e-02, -1.4709e-03,  3.1986e-02,  8.7794e-03, -2.7176e-02,\n",
       "                      -1.0460e-02,  3.1823e-02,  2.1107e-02, -1.8682e-02,  5.0061e-02,\n",
       "                      -2.0859e-02,  1.3781e-02,  1.9505e-02,  1.9483e-02,  2.5855e-02,\n",
       "                       1.6785e-04,  2.6612e-02,  7.5097e-03, -3.2698e-02, -1.6728e-02,\n",
       "                      -4.2728e-02, -1.0212e-03, -1.3653e-02, -3.7313e-02,  7.9799e-03,\n",
       "                      -2.7508e-02, -3.5375e-02,  3.8686e-02,  1.0597e-03,  3.2839e-02,\n",
       "                       3.5215e-02, -1.8460e-02,  5.7857e-02,  1.8041e-02,  4.2305e-02,\n",
       "                       5.6643e-03, -1.3614e-02, -8.0445e-03,  2.3582e-02,  2.1871e-02,\n",
       "                       3.2631e-03,  2.3117e-02,  1.2620e-02,  3.1105e-02, -1.1550e-02,\n",
       "                       3.3383e-02,  8.2246e-03,  5.2145e-02, -1.3091e-02, -2.4295e-02,\n",
       "                      -3.2441e-02, -1.7514e-02,  3.5460e-03, -2.0986e-02,  4.0658e-02,\n",
       "                       1.8465e-03, -1.9159e-02, -2.0487e-03,  2.3911e-02, -7.2946e-03,\n",
       "                       5.0681e-02,  1.3651e-02, -1.8244e-03, -1.6210e-02, -2.6071e-02,\n",
       "                      -1.9938e-02,  3.2970e-02, -6.8496e-03,  1.4954e-02, -1.5612e-02,\n",
       "                      -1.9586e-02, -2.4341e-02,  1.7195e-02, -4.6066e-02,  3.3175e-02,\n",
       "                       2.0783e-02,  1.5505e-02, -2.7952e-02, -7.4961e-03,  7.1714e-03,\n",
       "                      -2.5444e-02,  3.3536e-02, -1.3632e-02,  1.7337e-02,  8.5386e-03,\n",
       "                       3.9239e-02, -4.1903e-04, -7.6186e-04,  3.7560e-03,  1.6516e-03,\n",
       "                       1.7951e-03,  1.0452e-02, -1.1870e-03, -7.2464e-04,  7.7997e-03,\n",
       "                       7.1622e-03, -4.0383e-03, -2.0825e-02,  2.2107e-03, -8.6614e-03,\n",
       "                      -1.5558e-02,  7.9736e-03,  1.5780e-02,  2.2144e-02,  4.1950e-02,\n",
       "                      -5.5684e-03, -6.8857e-03,  1.3809e-02,  1.6689e-02,  4.1708e-02,\n",
       "                       3.8051e-02, -6.5801e-03,  4.5398e-02, -5.1838e-02, -6.7915e-03,\n",
       "                       1.5648e-02,  4.2930e-02,  1.8156e-02, -1.4389e-02,  3.1903e-02,\n",
       "                      -1.8629e-02,  4.1535e-02, -2.5687e-02, -1.7637e-02,  1.5492e-03,\n",
       "                       2.5063e-02,  2.7109e-02, -3.7469e-02, -3.2719e-02,  2.7542e-02,\n",
       "                      -3.7124e-02,  9.0260e-03,  6.1271e-03,  3.6024e-02, -6.2293e-02,\n",
       "                       5.5659e-02,  1.8151e-02,  1.1932e-03,  3.7855e-02, -2.4901e-02,\n",
       "                       2.4391e-02, -1.2698e-02,  2.8086e-02,  1.6488e-02,  3.2551e-04,\n",
       "                      -1.9819e-02,  8.1793e-03,  2.6964e-02,  2.2570e-02, -1.5655e-02,\n",
       "                      -1.7271e-02,  2.7032e-02,  1.6283e-02, -1.2728e-02, -3.5229e-02,\n",
       "                       2.3310e-02, -1.5393e-02, -3.7807e-02, -3.4871e-03,  4.3359e-03,\n",
       "                       5.1587e-02, -1.9228e-02,  3.3206e-03,  3.2505e-02,  2.6190e-03,\n",
       "                       1.9724e-02, -2.6157e-02, -2.6083e-02,  3.3072e-05,  6.4396e-03,\n",
       "                       1.1539e-02, -6.3151e-03, -9.1462e-03, -1.0095e-03,  3.0754e-02,\n",
       "                       6.0476e-02, -2.0536e-02, -1.5161e-02,  9.8575e-03,  2.9014e-02,\n",
       "                       6.6569e-03,  3.0803e-02,  1.3542e-02,  2.1616e-02,  1.9559e-02,\n",
       "                      -2.7509e-02, -1.2984e-02, -2.5382e-02,  4.4648e-02,  1.2323e-02,\n",
       "                      -7.5315e-03,  4.4075e-02,  1.8301e-02, -4.3020e-02,  2.1084e-02,\n",
       "                      -1.1126e-02, -3.0776e-02, -1.6625e-02,  5.3334e-03, -1.3104e-02,\n",
       "                       1.7496e-02, -3.2143e-02,  2.0569e-02, -2.3210e-02, -8.8169e-03,\n",
       "                      -4.6270e-02, -1.4851e-03,  3.0564e-02, -2.2501e-03,  3.6973e-02,\n",
       "                      -6.0386e-03,  4.5424e-02,  2.2855e-02,  2.8906e-02,  2.5078e-02,\n",
       "                       5.4107e-02, -2.5772e-02,  7.5727e-03, -1.1538e-02,  4.1240e-02,\n",
       "                      -9.7759e-03, -5.7294e-03, -4.6277e-02,  1.1981e-02, -3.2381e-02,\n",
       "                       1.5992e-02,  2.9337e-03, -4.6958e-02,  1.9800e-02,  1.0694e-02,\n",
       "                      -4.8423e-03, -3.4283e-02,  4.9904e-02,  3.0701e-02,  5.4613e-03,\n",
       "                       4.0239e-02, -3.2105e-02,  1.1015e-02, -3.4659e-03, -4.4337e-02,\n",
       "                      -4.3649e-02, -2.5184e-02, -4.1793e-02, -2.8391e-03,  5.6268e-02,\n",
       "                      -1.7820e-02,  4.5836e-02,  7.7898e-03, -5.4924e-02,  4.0713e-02,\n",
       "                       3.3434e-02,  4.5503e-02, -1.1298e-02,  8.4898e-03,  1.2869e-02,\n",
       "                       4.3587e-02, -1.8451e-02,  3.0677e-02,  3.0346e-03,  2.0432e-03,\n",
       "                      -1.0200e-02, -2.5712e-02, -3.4746e-02, -5.0181e-03,  2.8922e-02,\n",
       "                       2.0027e-02, -1.3713e-03,  1.6420e-02,  9.8079e-04, -3.4362e-02,\n",
       "                      -2.1111e-02,  2.4584e-02, -8.3494e-03,  2.6988e-02, -8.9489e-03,\n",
       "                      -3.4254e-02,  2.7616e-02, -2.8200e-02,  3.2625e-02,  2.3518e-02,\n",
       "                       3.1107e-03,  2.3399e-02,  7.7339e-03,  6.2850e-03,  3.5181e-02,\n",
       "                       3.5321e-03, -4.2751e-02, -7.4012e-03,  1.7613e-02, -6.5823e-03,\n",
       "                      -2.2135e-02, -3.7646e-02,  1.4056e-02,  3.0609e-02, -4.7437e-02,\n",
       "                       2.8955e-03,  4.0832e-02, -4.4636e-02,  1.5084e-02, -2.0889e-02,\n",
       "                      -2.2261e-02, -2.9580e-02, -3.0605e-02,  2.2260e-02, -2.3478e-02,\n",
       "                      -1.0426e-02,  4.7102e-02,  5.2827e-03,  6.6748e-03, -4.2959e-02,\n",
       "                       1.5231e-02,  4.6870e-03, -2.9346e-02, -4.1109e-02,  2.4076e-02,\n",
       "                      -1.0752e-03, -1.4599e-03,  2.5873e-02,  1.0736e-02,  1.6433e-02,\n",
       "                      -2.6141e-02, -1.3319e-02, -3.5492e-04,  1.1504e-02,  2.5155e-02,\n",
       "                      -6.9054e-03,  3.5431e-02, -2.7816e-03,  4.5027e-02,  3.4190e-03,\n",
       "                       7.4205e-03,  3.1117e-02,  3.9559e-03,  3.8093e-02,  2.7792e-02,\n",
       "                       7.5039e-03,  6.0084e-03,  9.4579e-03, -4.0724e-03,  3.3599e-02,\n",
       "                      -8.4536e-03, -7.4027e-03,  1.3223e-02,  4.2784e-02, -1.3148e-02,\n",
       "                      -8.3278e-03,  5.1883e-03, -4.9172e-03, -3.3992e-02,  2.2475e-03,\n",
       "                       1.9104e-02, -3.7693e-02,  3.6698e-02, -1.8575e-02,  2.4238e-02,\n",
       "                      -1.2642e-02,  4.5467e-02,  3.8781e-02, -3.6538e-02,  1.1639e-02,\n",
       "                       3.3191e-02, -2.6421e-02,  5.3204e-02,  1.7801e-04, -5.0052e-02,\n",
       "                       3.2632e-02, -2.2227e-02, -3.9088e-02,  2.5253e-02,  4.0397e-02,\n",
       "                      -4.0069e-03,  2.7821e-02])),\n",
       "             ('decoder.dec_attention.v.weight',\n",
       "              tensor([[ 0.0632,  0.0222, -0.0380,  0.0177,  0.0362, -0.0094,  0.0391,  0.0187,\n",
       "                       -0.0301, -0.0227,  0.0347,  0.0356,  0.0446, -0.0075,  0.0534, -0.0214,\n",
       "                       -0.0649,  0.0412, -0.0242,  0.0392,  0.0213,  0.0366,  0.0210, -0.0481,\n",
       "                        0.0185,  0.0408, -0.0396,  0.0270, -0.0404, -0.0293, -0.0215,  0.0191,\n",
       "                        0.0367,  0.0451,  0.0231,  0.0191, -0.0496,  0.0372, -0.0334, -0.0219,\n",
       "                        0.0536,  0.0383,  0.0610, -0.0601, -0.0272,  0.0684,  0.0458, -0.0323,\n",
       "                        0.0089,  0.0361,  0.0348,  0.0226,  0.0579, -0.0277,  0.0426, -0.0447,\n",
       "                        0.0310,  0.0360, -0.0460,  0.0585, -0.0339,  0.0377, -0.0253, -0.0500,\n",
       "                        0.0181, -0.0460, -0.0444,  0.0239, -0.0114,  0.0388, -0.0236, -0.0579,\n",
       "                        0.0329, -0.0600, -0.0158,  0.0302, -0.0490,  0.0256,  0.0233, -0.0587,\n",
       "                       -0.0421, -0.0227, -0.0443,  0.0219,  0.0583, -0.0420,  0.0541,  0.0321,\n",
       "                       -0.0172, -0.0420, -0.0487, -0.0332,  0.0375,  0.0353, -0.0221,  0.0360,\n",
       "                        0.0235, -0.0294, -0.0233, -0.0428, -0.0245, -0.0520, -0.0650,  0.0391,\n",
       "                       -0.0388,  0.0314, -0.0402, -0.0493, -0.0228, -0.0404, -0.0620,  0.0587,\n",
       "                        0.0573, -0.0503,  0.0470, -0.0420, -0.0224, -0.0652, -0.0548,  0.0585,\n",
       "                        0.0280, -0.0282, -0.0490, -0.0415, -0.0342,  0.0213,  0.0471, -0.0436,\n",
       "                        0.0227, -0.0586,  0.0417, -0.0355,  0.0180, -0.0434, -0.0113, -0.0232,\n",
       "                        0.0290, -0.0638,  0.0565, -0.0519,  0.0457,  0.0624,  0.0517,  0.0627,\n",
       "                        0.0330, -0.0125,  0.0256, -0.0285,  0.0365, -0.0452, -0.0434, -0.0146,\n",
       "                       -0.0222, -0.0319, -0.0365,  0.0336,  0.0342, -0.0302,  0.0323, -0.0545,\n",
       "                        0.0291,  0.0252,  0.0497,  0.0257,  0.0376, -0.0540,  0.0267, -0.0537,\n",
       "                       -0.0335,  0.0253, -0.0430, -0.0429,  0.0083,  0.0159,  0.0321, -0.0233,\n",
       "                       -0.0409, -0.0399, -0.0152,  0.0191, -0.0252,  0.0463,  0.0173,  0.0094,\n",
       "                       -0.0264,  0.0610, -0.0617,  0.0248, -0.0122, -0.0582,  0.0568,  0.0479,\n",
       "                        0.0270,  0.0599, -0.0427, -0.0465, -0.0291, -0.0267, -0.0278, -0.0191,\n",
       "                        0.0392, -0.0428, -0.0663, -0.0492, -0.0315, -0.0534,  0.0266, -0.0229,\n",
       "                        0.0169,  0.0467, -0.0283,  0.0222, -0.0514, -0.0080,  0.0338,  0.0568,\n",
       "                       -0.0601, -0.0547,  0.0529,  0.0211, -0.0281, -0.0326, -0.0412,  0.0337,\n",
       "                       -0.0443, -0.0230,  0.0288,  0.0442, -0.0592, -0.0415, -0.0462, -0.0604,\n",
       "                        0.0593,  0.0192,  0.0401, -0.0488, -0.0463,  0.0547, -0.0266,  0.0358,\n",
       "                       -0.0072,  0.0446, -0.0335,  0.0324,  0.0050,  0.0142, -0.0277,  0.0228,\n",
       "                       -0.0197,  0.0467,  0.0266, -0.0509, -0.0494, -0.0313, -0.0380, -0.0396,\n",
       "                       -0.0326,  0.0337, -0.0281, -0.0587, -0.0011, -0.0651, -0.0251, -0.0327,\n",
       "                        0.0355,  0.0422,  0.0149,  0.0705,  0.0310, -0.0328, -0.0310, -0.0154,\n",
       "                        0.0528,  0.0531,  0.0500, -0.0444,  0.0282,  0.0414,  0.0346,  0.0256,\n",
       "                       -0.0191,  0.0302, -0.0557,  0.0408,  0.0308,  0.0466, -0.0192, -0.0177,\n",
       "                       -0.0345, -0.0068, -0.0627, -0.0514,  0.0454,  0.0524, -0.0285,  0.0480,\n",
       "                        0.0554, -0.0391,  0.0447,  0.0552, -0.0202,  0.0211,  0.0213,  0.0287,\n",
       "                       -0.0448,  0.0244,  0.0269,  0.0242, -0.0575, -0.0564, -0.0313, -0.0462,\n",
       "                        0.0219, -0.0492,  0.0444, -0.0182, -0.0351, -0.0185,  0.0365, -0.0255,\n",
       "                        0.0575, -0.0464,  0.0138, -0.0363,  0.0449,  0.0540,  0.0142, -0.0405,\n",
       "                        0.0488,  0.0541, -0.0618,  0.0600,  0.0161,  0.0510,  0.0567, -0.0526,\n",
       "                       -0.0588,  0.0416,  0.0339, -0.0238, -0.0439, -0.0484,  0.0518,  0.0152,\n",
       "                       -0.0059,  0.0317, -0.0378, -0.0306,  0.0172, -0.0430,  0.0243, -0.0373,\n",
       "                       -0.0254,  0.0362, -0.0332, -0.0382, -0.0457, -0.0567,  0.0294,  0.0244,\n",
       "                       -0.0366,  0.0362,  0.0370, -0.0344, -0.0319,  0.0139, -0.0348, -0.0307,\n",
       "                        0.0304, -0.0463, -0.0499, -0.0479,  0.0510, -0.0301, -0.0496, -0.0367,\n",
       "                        0.0149, -0.0189,  0.0436,  0.0433,  0.0034,  0.0329,  0.0550, -0.0492,\n",
       "                        0.0378, -0.0465, -0.0273, -0.0201,  0.0558, -0.0326,  0.0458, -0.0364,\n",
       "                        0.0389,  0.0564,  0.0379, -0.0348, -0.0217, -0.0238, -0.0279,  0.0203,\n",
       "                        0.0340,  0.0274,  0.0259, -0.0392, -0.0291,  0.0517,  0.0186,  0.0527,\n",
       "                       -0.0290,  0.0393, -0.0270, -0.0576,  0.0245,  0.0222,  0.0277, -0.0188,\n",
       "                       -0.0653, -0.0236, -0.0311, -0.0504,  0.0196, -0.0490, -0.0461,  0.0352,\n",
       "                       -0.0213,  0.0518,  0.0346,  0.0604,  0.0530, -0.0515, -0.0226, -0.0097,\n",
       "                       -0.0423,  0.0545, -0.0221,  0.0497,  0.0223,  0.0506,  0.0475,  0.0649,\n",
       "                       -0.0442,  0.0305,  0.0316,  0.0475, -0.0304,  0.0245,  0.0176, -0.0221,\n",
       "                        0.0582, -0.0485, -0.0239,  0.0499,  0.0545,  0.0232, -0.0363,  0.0424,\n",
       "                       -0.0651, -0.0317,  0.0353,  0.0295,  0.0551,  0.0391,  0.0600,  0.0500,\n",
       "                       -0.0492,  0.0423, -0.0478, -0.0470, -0.0258,  0.0141, -0.0063, -0.0420,\n",
       "                        0.0577, -0.0301,  0.0491,  0.0175, -0.0281, -0.0320,  0.0695, -0.0539,\n",
       "                       -0.0069,  0.0551,  0.0478, -0.0375, -0.0449,  0.0602,  0.0010, -0.0160,\n",
       "                       -0.0336,  0.0359, -0.0598,  0.0538, -0.0063, -0.0414, -0.0346,  0.0478,\n",
       "                        0.0289, -0.0044, -0.0481, -0.0121,  0.0318,  0.0347, -0.0551,  0.0219,\n",
       "                       -0.0188, -0.0405, -0.0173,  0.0090, -0.0535,  0.0314, -0.0635,  0.0206]])),\n",
       "             ('decoder.x_context.weight',\n",
       "              tensor([[-0.0701,  0.0268,  0.0159,  ..., -0.0087,  0.0057,  0.0067],\n",
       "                      [-0.0150,  0.0513, -0.0183,  ...,  0.0106, -0.0192, -0.0111],\n",
       "                      [-0.0891, -0.0099,  0.0456,  ..., -0.0139,  0.0171, -0.0090],\n",
       "                      ...,\n",
       "                      [-0.0683, -0.0581,  0.0460,  ..., -0.0245, -0.0073,  0.0115],\n",
       "                      [-0.0969, -0.0140,  0.0387,  ...,  0.0289,  0.0071,  0.0006],\n",
       "                      [ 0.0701, -0.0062, -0.0054,  ...,  0.0241, -0.0219, -0.0178]])),\n",
       "             ('decoder.x_context.bias',\n",
       "              tensor([ 0.0323,  0.0171, -0.0057,  0.0319,  0.0187,  0.0337, -0.0097,  0.0154,\n",
       "                       0.0373, -0.0424,  0.0373, -0.0191,  0.0343,  0.0308, -0.0028,  0.0109,\n",
       "                      -0.0171, -0.0141,  0.0337, -0.0420, -0.0244, -0.0355,  0.0151, -0.0110,\n",
       "                      -0.0292,  0.0222,  0.0414,  0.0324, -0.0285, -0.0229, -0.0325,  0.0361,\n",
       "                      -0.0137,  0.0047,  0.0179,  0.0128, -0.0390,  0.0219,  0.0364,  0.0173,\n",
       "                       0.0105, -0.0324, -0.0304, -0.0043,  0.0219,  0.0089,  0.0396,  0.0192,\n",
       "                      -0.0126, -0.0331,  0.0309,  0.0354,  0.0126, -0.0376,  0.0309, -0.0207,\n",
       "                      -0.0011,  0.0177, -0.0464, -0.0018,  0.0267,  0.0331,  0.0117,  0.0243,\n",
       "                      -0.0409, -0.0455, -0.0176, -0.0234,  0.0122, -0.0025,  0.0095, -0.0197,\n",
       "                      -0.0191, -0.0265, -0.0047,  0.0229, -0.0216, -0.0227,  0.0242,  0.0284,\n",
       "                      -0.0364, -0.0124, -0.0326, -0.0174, -0.0130, -0.0031, -0.0433,  0.0160,\n",
       "                      -0.0148, -0.0318,  0.0273, -0.0160,  0.0242,  0.0269,  0.0184, -0.0046,\n",
       "                       0.0211, -0.0293, -0.0212,  0.0332,  0.0329,  0.0381, -0.0421, -0.0106,\n",
       "                       0.0143,  0.0101, -0.0213,  0.0175,  0.0217,  0.0148, -0.0193,  0.0362,\n",
       "                       0.0328,  0.0310, -0.0178, -0.0265, -0.0327, -0.0240,  0.0166,  0.0412,\n",
       "                       0.0210,  0.0276, -0.0292,  0.0145,  0.0198,  0.0062,  0.0262,  0.0317,\n",
       "                      -0.0282,  0.0047,  0.0273, -0.0144,  0.0120,  0.0204,  0.0445,  0.0140,\n",
       "                       0.0256, -0.0323,  0.0239, -0.0302,  0.0354, -0.0315, -0.0366,  0.0028,\n",
       "                      -0.0047,  0.0153, -0.0457,  0.0219, -0.0110, -0.0432,  0.0232,  0.0273,\n",
       "                       0.0304, -0.0077, -0.0326, -0.0133, -0.0124, -0.0305,  0.0226, -0.0181,\n",
       "                       0.0021,  0.0016,  0.0248,  0.0081,  0.0114,  0.0244,  0.0233, -0.0324,\n",
       "                       0.0370,  0.0254, -0.0421,  0.0200,  0.0298, -0.0278, -0.0275,  0.0205,\n",
       "                       0.0277, -0.0203,  0.0291, -0.0423,  0.0147, -0.0357, -0.0274, -0.0286,\n",
       "                      -0.0236, -0.0138, -0.0308, -0.0162, -0.0218, -0.0012,  0.0287, -0.0192,\n",
       "                      -0.0175, -0.0099,  0.0086, -0.0384, -0.0176,  0.0434, -0.0205, -0.0048,\n",
       "                      -0.0418,  0.0111,  0.0335,  0.0260,  0.0169,  0.0320,  0.0264,  0.0348,\n",
       "                      -0.0081, -0.0105, -0.0224, -0.0118,  0.0231,  0.0424, -0.0394,  0.0450,\n",
       "                       0.0156, -0.0151, -0.0225, -0.0317, -0.0419,  0.0122,  0.0187, -0.0390,\n",
       "                      -0.0338, -0.0349, -0.0278, -0.0316,  0.0228,  0.0076,  0.0281,  0.0170,\n",
       "                       0.0095, -0.0211, -0.0004, -0.0303, -0.0021,  0.0166,  0.0164,  0.0210,\n",
       "                       0.0384,  0.0270,  0.0141, -0.0026, -0.0013,  0.0322, -0.0385, -0.0291,\n",
       "                       0.0240,  0.0262, -0.0242, -0.0025,  0.0072, -0.0218, -0.0055,  0.0132])),\n",
       "             ('decoder.lstm.weight_ih',\n",
       "              tensor([[ 0.0253,  0.0221, -0.0274,  ..., -0.0349, -0.0255,  0.0123],\n",
       "                      [-0.0086, -0.0008, -0.0024,  ..., -0.0103,  0.0014, -0.0069],\n",
       "                      [ 0.0277,  0.0231, -0.0042,  ..., -0.0341,  0.0037,  0.0143],\n",
       "                      ...,\n",
       "                      [ 0.0086,  0.0021,  0.0521,  ...,  0.0618,  0.0919, -0.0471],\n",
       "                      [ 0.0029, -0.0078, -0.0252,  ..., -0.0072, -0.0091, -0.0135],\n",
       "                      [ 0.0072,  0.0012,  0.0536,  ...,  0.0927,  0.0753, -0.0618]])),\n",
       "             ('decoder.lstm.weight_hh',\n",
       "              tensor([[ 0.0137, -0.0219, -0.0230,  ..., -0.0204, -0.0120, -0.0164],\n",
       "                      [-0.0220, -0.0207,  0.0003,  ..., -0.0281,  0.0093, -0.0247],\n",
       "                      [-0.0211, -0.0062,  0.0117,  ..., -0.0152,  0.0142, -0.0315],\n",
       "                      ...,\n",
       "                      [ 0.0017,  0.0058, -0.0139,  ..., -0.0282, -0.0131, -0.0450],\n",
       "                      [-0.0146, -0.0051, -0.0108,  ...,  0.0158,  0.0075,  0.0216],\n",
       "                      [-0.0223, -0.0266, -0.0118,  ..., -0.0546, -0.0611, -0.0448]])),\n",
       "             ('decoder.lstm.bias_ih',\n",
       "              tensor([-0.0228, -0.0083, -0.0103,  ..., -0.0056,  0.0129, -0.0036])),\n",
       "             ('decoder.lstm.bias_hh',\n",
       "              tensor([-0.0228, -0.0083, -0.0103,  ..., -0.0056,  0.0129, -0.0036])),\n",
       "             ('decoder.p_gen_linear.weight',\n",
       "              tensor([[-0.0037, -0.0035,  0.0210,  ...,  0.0132,  0.0178, -0.0020]])),\n",
       "             ('decoder.p_gen_linear.bias', tensor([0.0043])),\n",
       "             ('decoder.V.weight',\n",
       "              tensor([[-0.0049,  0.0177,  0.0086,  ..., -0.0044, -0.0113, -0.0162],\n",
       "                      [-0.0023, -0.0190, -0.0286,  ..., -0.0228, -0.0058,  0.0231],\n",
       "                      [-0.0073, -0.0194, -0.0065,  ...,  0.0013,  0.0230,  0.0093],\n",
       "                      ...,\n",
       "                      [-0.0278,  0.0054, -0.0253,  ...,  0.0019,  0.0119,  0.0177],\n",
       "                      [-0.0462, -0.0100, -0.0090,  ..., -0.0039, -0.0071,  0.0153],\n",
       "                      [-0.0025,  0.0019,  0.0115,  ...,  0.0152,  0.0184,  0.0119]])),\n",
       "             ('decoder.V.bias',\n",
       "              tensor([-0.0258, -0.0435, -0.0367, -0.0362,  0.0254,  0.0374,  0.0274, -0.0228,\n",
       "                      -0.0356,  0.0287, -0.0265,  0.0314, -0.0113, -0.0379, -0.0443, -0.0466,\n",
       "                      -0.0375, -0.0302,  0.0098,  0.0292, -0.0322, -0.0370, -0.0389, -0.0402,\n",
       "                       0.0411,  0.0340,  0.0230,  0.0381, -0.0233,  0.0263,  0.0202, -0.0418,\n",
       "                       0.0253,  0.0386,  0.0322,  0.0360,  0.0235, -0.0162, -0.0393,  0.0400,\n",
       "                      -0.0303,  0.0327, -0.0245,  0.0292,  0.0124, -0.0291, -0.0430, -0.0275,\n",
       "                       0.0280, -0.0326,  0.0427,  0.0242,  0.0326, -0.0210,  0.0169, -0.0215,\n",
       "                      -0.0310, -0.0286, -0.0321,  0.0276, -0.0350,  0.0281, -0.0253, -0.0295,\n",
       "                       0.0251, -0.0341,  0.0341, -0.0240, -0.0415, -0.0232, -0.0388, -0.0386,\n",
       "                      -0.0167,  0.0466, -0.0361, -0.0379,  0.0413,  0.0344,  0.0334,  0.0464,\n",
       "                       0.0428, -0.0249,  0.0457, -0.0352,  0.0279,  0.0323, -0.0400, -0.0440,\n",
       "                       0.0281,  0.0280,  0.0356,  0.0298, -0.0392,  0.0293,  0.0210, -0.0312,\n",
       "                       0.0407, -0.0288,  0.0230, -0.0372, -0.0452, -0.0227, -0.0263, -0.0411,\n",
       "                       0.0263,  0.0384, -0.0263,  0.0281, -0.0386,  0.0261, -0.0375,  0.0340,\n",
       "                      -0.0362, -0.0305,  0.0334, -0.0267, -0.0334, -0.0331, -0.0363, -0.0254,\n",
       "                       0.0343, -0.0318,  0.0383, -0.0227, -0.0357,  0.0306,  0.0311, -0.0290,\n",
       "                      -0.0226, -0.0298,  0.0363, -0.0395, -0.0397,  0.0301,  0.0397,  0.0355,\n",
       "                      -0.0288, -0.0316,  0.0415,  0.0274,  0.0288, -0.0343, -0.0319,  0.0150,\n",
       "                      -0.0360, -0.0376,  0.0349,  0.0273, -0.0395, -0.0356, -0.0316,  0.0391,\n",
       "                       0.0333, -0.0375, -0.0325,  0.0230,  0.0242, -0.0225,  0.0347, -0.0313,\n",
       "                      -0.0377, -0.0180,  0.0318,  0.0402, -0.0394,  0.0178, -0.0236,  0.0136,\n",
       "                      -0.0226, -0.0398,  0.0374,  0.0288, -0.0385, -0.0127,  0.0415, -0.0400,\n",
       "                      -0.0325, -0.0187, -0.0431,  0.0102, -0.0296, -0.0213, -0.0349, -0.0238,\n",
       "                      -0.0424, -0.0172,  0.0249, -0.0321, -0.0307, -0.0163,  0.0441, -0.0370,\n",
       "                       0.0223, -0.0388,  0.0187,  0.0241, -0.0303, -0.0078, -0.0412, -0.0282,\n",
       "                      -0.0428,  0.0295,  0.0118,  0.0357,  0.0417,  0.0375,  0.0363, -0.0353,\n",
       "                      -0.0424, -0.0261, -0.0372, -0.0290, -0.0244,  0.0249, -0.0296, -0.0300,\n",
       "                       0.0268,  0.0334,  0.0335,  0.0201, -0.0323,  0.0410, -0.0271, -0.0391,\n",
       "                       0.0245,  0.0212,  0.0242,  0.0364,  0.0329,  0.0378, -0.0375, -0.0185,\n",
       "                      -0.0193, -0.0362,  0.0377, -0.0356, -0.0217, -0.0347,  0.0406, -0.0369,\n",
       "                       0.0400,  0.0315,  0.0346,  0.0471, -0.0385,  0.0354, -0.0263,  0.0345,\n",
       "                       0.0375,  0.0349,  0.0355,  0.0271, -0.0340, -0.0201,  0.0459, -0.0226,\n",
       "                       0.0272,  0.0309, -0.0420,  0.0263, -0.0335, -0.0214,  0.0455,  0.0359,\n",
       "                      -0.0342, -0.0397,  0.0387, -0.0437,  0.0247, -0.0283,  0.0365, -0.0386,\n",
       "                       0.0390, -0.0321, -0.0299,  0.0459,  0.0257,  0.0362, -0.0187,  0.0263,\n",
       "                      -0.0290, -0.0150, -0.0397, -0.0358,  0.0317,  0.0225, -0.0284, -0.0396,\n",
       "                      -0.0253,  0.0410,  0.0320, -0.0347,  0.0347,  0.0241,  0.0410,  0.0400,\n",
       "                      -0.0298,  0.0300, -0.0235, -0.0361, -0.0342, -0.0428, -0.0424, -0.0323,\n",
       "                       0.0288,  0.0383,  0.0284,  0.0300, -0.0439, -0.0258,  0.0276, -0.0224,\n",
       "                       0.0409,  0.0420, -0.0327, -0.0296,  0.0209,  0.0345, -0.0360, -0.0227,\n",
       "                       0.0342, -0.0427,  0.0278, -0.0282,  0.0495,  0.0268,  0.0320, -0.0338,\n",
       "                       0.0380, -0.0218,  0.0387, -0.0352,  0.0299, -0.0430,  0.0377, -0.0374,\n",
       "                      -0.0379, -0.0432,  0.0304,  0.0289, -0.0302,  0.0395, -0.0261, -0.0378,\n",
       "                       0.0339, -0.0426, -0.0301,  0.0362,  0.0383, -0.0221, -0.0246,  0.0259,\n",
       "                      -0.0329, -0.0259,  0.0392,  0.0417, -0.0439,  0.0261, -0.0362,  0.0200,\n",
       "                       0.0217, -0.0310, -0.0253,  0.0217,  0.0331,  0.0347,  0.0393,  0.0365,\n",
       "                      -0.0345, -0.0358,  0.0406,  0.0265,  0.0428, -0.0349, -0.0382, -0.0348,\n",
       "                      -0.0312, -0.0246,  0.0173, -0.0394, -0.0374,  0.0257,  0.0249, -0.0239,\n",
       "                       0.0443, -0.0440,  0.0263, -0.0249, -0.0403, -0.0376, -0.0368,  0.0312,\n",
       "                      -0.0206,  0.0400, -0.0386,  0.0286, -0.0237, -0.0324,  0.0321,  0.0262,\n",
       "                       0.0269,  0.0330,  0.0434,  0.0391, -0.0405, -0.0318, -0.0373, -0.0253,\n",
       "                       0.0396, -0.0342,  0.0182, -0.0376,  0.0199, -0.0259, -0.0372, -0.0348,\n",
       "                       0.0379, -0.0329, -0.0326,  0.0347,  0.0444, -0.0400,  0.0290,  0.0156,\n",
       "                      -0.0273, -0.0304,  0.0317, -0.0399, -0.0142, -0.0281,  0.0205,  0.0076,\n",
       "                       0.0334, -0.0399,  0.0400,  0.0427,  0.0359,  0.0226,  0.0261, -0.0320,\n",
       "                      -0.0250, -0.0306, -0.0290,  0.0259,  0.0375, -0.0423,  0.0281,  0.0287,\n",
       "                      -0.0167, -0.0388, -0.0411,  0.0369,  0.0315,  0.0321, -0.0418, -0.0244,\n",
       "                      -0.0479, -0.0297,  0.0261,  0.0283, -0.0271, -0.0438,  0.0440,  0.0206,\n",
       "                       0.0266, -0.0283, -0.0421, -0.0179,  0.0283,  0.0254, -0.0349, -0.0315,\n",
       "                       0.0270, -0.0440,  0.0421, -0.0438,  0.0437, -0.0272,  0.0409,  0.0393,\n",
       "                       0.0427,  0.0373,  0.0251, -0.0211,  0.0196,  0.0261, -0.0308, -0.0328,\n",
       "                      -0.0371,  0.0399, -0.0452,  0.0294, -0.0159,  0.0248,  0.0361, -0.0338,\n",
       "                      -0.0356,  0.0394, -0.0375,  0.0320,  0.0285, -0.0387, -0.0243,  0.0436,\n",
       "                      -0.0264, -0.0384,  0.0310,  0.0335, -0.0378, -0.0394, -0.0210,  0.0229])),\n",
       "             ('decoder.V1.weight',\n",
       "              tensor([[-0.0128, -0.0203, -0.0124,  ..., -0.0155, -0.0023,  0.0106],\n",
       "                      [ 0.0332,  0.0365,  0.0386,  ...,  0.0373,  0.0344, -0.0316],\n",
       "                      [ 0.0334,  0.0365,  0.0388,  ...,  0.0375,  0.0347, -0.0317],\n",
       "                      ...,\n",
       "                      [ 0.0333,  0.0366,  0.0389,  ...,  0.0376,  0.0343, -0.0314],\n",
       "                      [ 0.0334,  0.0364,  0.0389,  ...,  0.0374,  0.0342, -0.0315],\n",
       "                      [ 0.0332,  0.0366,  0.0390,  ...,  0.0374,  0.0343, -0.0317]])),\n",
       "             ('decoder.V1.bias',\n",
       "              tensor([ 0.0385, -0.0254, -0.0255,  ..., -0.0255, -0.0254, -0.0253])),\n",
       "             ('embeds.weight',\n",
       "              tensor([[-0.0120, -0.0246,  0.0134,  ...,  0.0164,  0.0046,  0.0288],\n",
       "                      [ 0.0048,  0.0052, -0.0050,  ..., -0.0051,  0.0052,  0.0051],\n",
       "                      [ 0.0009,  0.0179, -0.0115,  ..., -0.0096,  0.0046,  0.0083],\n",
       "                      ...,\n",
       "                      [ 0.0025, -0.0140, -0.0168,  ...,  0.0077,  0.0124,  0.0195],\n",
       "                      [ 0.0144, -0.0291, -0.0344,  ...,  0.0257,  0.0233,  0.0260],\n",
       "                      [-0.0006, -0.0285,  0.0151,  ...,  0.0187,  0.0349,  0.0386]])),\n",
       "             ('seg_embeds.weight',\n",
       "              tensor([[ 2.1368e-02, -1.8749e-02, -1.9032e-02, -2.1389e-02, -4.6480e-03,\n",
       "                       -2.0587e-02, -1.8439e-02,  2.1182e-02,  1.6102e-02, -1.7140e-02,\n",
       "                       -1.9363e-02, -4.6016e-03,  1.9646e-02, -1.7515e-02, -2.0666e-02,\n",
       "                       -1.8103e-02, -2.0681e-02, -1.8046e-02, -1.8569e-02,  2.0919e-02,\n",
       "                        2.0410e-02, -1.5428e-02,  2.1034e-02, -1.7228e-02, -1.7810e-02,\n",
       "                       -1.8651e-02,  1.7843e-02,  1.7579e-02, -2.1402e-02, -1.7353e-02,\n",
       "                        2.2318e-03,  1.8733e-02,  6.5614e-03,  1.7788e-02,  7.5445e-03,\n",
       "                        2.0987e-02, -2.1122e-02,  1.9066e-02, -2.4110e-03, -1.6764e-02,\n",
       "                        1.4946e-02,  1.2784e-02,  2.4483e-03,  1.7145e-02, -2.0654e-02,\n",
       "                        1.6976e-02, -2.0366e-02,  1.8684e-02,  1.9495e-02,  2.1138e-02,\n",
       "                       -1.9459e-02,  2.6706e-04, -2.1416e-02, -1.3774e-02,  1.7218e-02,\n",
       "                        1.5972e-02, -2.0499e-02, -2.1099e-02, -2.2062e-02,  1.9759e-02,\n",
       "                        1.1962e-02, -1.1995e-02, -1.9229e-02, -2.0582e-02, -1.7466e-02,\n",
       "                        1.3147e-02,  2.1202e-02,  2.1681e-02, -2.1277e-02,  2.1472e-03,\n",
       "                       -2.4960e-03,  1.8563e-02,  2.0602e-02, -2.0720e-02, -1.7487e-02,\n",
       "                        6.4381e-03,  2.1399e-02,  2.0112e-02,  1.7476e-02,  1.9826e-02,\n",
       "                        2.1136e-02,  1.9429e-02,  2.0881e-02,  1.9371e-02, -1.9844e-02,\n",
       "                       -1.6407e-02,  1.6668e-02, -3.1765e-04,  5.8918e-03,  1.2119e-02,\n",
       "                       -1.8692e-02, -2.0575e-02,  1.9417e-02,  2.1367e-02,  1.9082e-02,\n",
       "                        1.1782e-02, -2.0827e-02, -1.7520e-02,  2.0729e-02, -1.1300e-03,\n",
       "                        2.0142e-02,  1.7866e-02,  2.0798e-02,  1.3557e-02, -2.2391e-02,\n",
       "                        2.0652e-02,  1.9610e-02,  2.1577e-02,  1.7280e-02,  1.7094e-02,\n",
       "                       -1.8742e-02, -2.0490e-02,  2.0250e-02, -1.7779e-02,  2.1233e-02,\n",
       "                        2.0408e-02, -1.2238e-03,  3.9923e-03, -1.9397e-02,  1.8134e-02,\n",
       "                        2.0357e-02, -2.0873e-02,  1.3402e-02,  1.7048e-02,  9.7008e-03,\n",
       "                       -2.2428e-02, -1.7912e-02, -1.9508e-02, -2.1564e-02, -1.9268e-02,\n",
       "                        2.0336e-02,  1.5438e-02,  2.0519e-02,  2.1071e-02,  2.0745e-02,\n",
       "                       -1.7557e-02,  1.1620e-03,  1.8005e-02,  2.0246e-02, -2.0792e-02,\n",
       "                        1.5805e-02,  1.9355e-02, -1.1092e-02,  1.8621e-02, -1.7347e-02,\n",
       "                        2.0312e-02, -1.7623e-03,  1.9087e-02,  5.9599e-03, -1.6885e-03,\n",
       "                        1.8215e-02,  2.1183e-02,  6.2198e-03, -2.0155e-02,  4.7321e-03,\n",
       "                        1.9710e-02, -1.5245e-02, -1.6912e-02, -2.1343e-02, -2.0753e-02,\n",
       "                       -1.0941e-03, -2.0463e-02, -8.8706e-03,  2.0274e-02,  1.9503e-02,\n",
       "                        1.0408e-02, -2.0651e-02,  1.2895e-02, -2.1262e-02,  1.3694e-02,\n",
       "                        1.7417e-02,  2.0998e-02, -2.1548e-02, -1.7296e-02,  1.7666e-02,\n",
       "                        1.7538e-02, -2.1026e-02, -1.6552e-02,  6.4801e-03,  2.0378e-02,\n",
       "                        2.0891e-02,  1.9817e-02, -1.5667e-02,  1.6146e-02,  2.1680e-02,\n",
       "                       -1.4354e-02,  2.1359e-02, -2.0158e-02,  1.6455e-02,  1.8296e-02,\n",
       "                        2.1204e-02,  9.3256e-03, -1.8544e-02, -1.7398e-02,  7.8509e-03,\n",
       "                       -9.3538e-03,  2.1013e-02,  1.7261e-02, -7.2848e-03,  1.3576e-02,\n",
       "                       -6.6608e-03,  1.8560e-02,  1.6459e-02,  2.0191e-02, -1.9557e-02,\n",
       "                       -2.1198e-02,  1.6914e-02,  1.4912e-02, -2.0709e-02,  6.0911e-03,\n",
       "                        2.0417e-02,  1.8326e-02,  1.9100e-02, -2.0274e-02, -1.7043e-02,\n",
       "                        9.7107e-03, -2.0001e-02, -2.0931e-02, -1.1079e-02,  2.0818e-02,\n",
       "                        1.9852e-02,  1.9535e-02,  1.9400e-02,  2.0611e-02,  8.0191e-03,\n",
       "                       -2.0434e-02, -1.5603e-02,  1.9024e-02,  2.0961e-02,  1.8717e-02,\n",
       "                        1.8097e-02, -1.8551e-02,  2.1620e-02, -1.9084e-02, -2.0260e-02,\n",
       "                        1.7970e-02,  1.8129e-02,  2.1026e-02,  2.1703e-02,  2.0315e-02,\n",
       "                       -1.9782e-02,  1.7033e-02,  4.6256e-03, -9.3314e-03, -2.0351e-02,\n",
       "                        3.7132e-03, -1.8554e-02,  2.0439e-02, -1.6174e-02, -2.0891e-02,\n",
       "                        2.1238e-02, -1.8856e-03, -8.8061e-03, -1.8842e-02,  1.7909e-02,\n",
       "                       -2.0296e-02],\n",
       "                      [-5.6843e-03, -5.8583e-03, -6.3866e-03,  2.9219e-03,  1.9263e-02,\n",
       "                        4.8469e-04, -8.1520e-03, -2.1159e-03, -1.8358e-02,  9.3711e-03,\n",
       "                       -6.0245e-03, -1.3828e-02,  2.9434e-03,  3.5973e-03, -7.0308e-04,\n",
       "                       -5.6233e-03,  4.3411e-03, -7.8103e-03, -7.6832e-03, -3.1850e-04,\n",
       "                        1.4727e-03,  1.5588e-02, -6.5917e-03, -7.6879e-03,  8.2320e-03,\n",
       "                       -3.6586e-03, -9.7427e-04, -9.7053e-04,  1.9388e-03,  1.2991e-02,\n",
       "                        1.6455e-02,  3.6116e-03,  4.5931e-03,  7.6639e-03, -1.7765e-02,\n",
       "                       -4.2090e-03,  1.1223e-03,  4.2622e-03,  1.8431e-02,  1.6902e-02,\n",
       "                       -1.3552e-02, -1.6783e-02, -1.6881e-02, -1.3860e-02,  5.1986e-03,\n",
       "                        9.8423e-03,  7.5884e-03,  7.8702e-03,  3.4668e-03, -6.6960e-03,\n",
       "                       -4.0892e-03,  1.1446e-02,  4.3621e-03,  1.2254e-02,  8.3805e-03,\n",
       "                        1.0876e-02,  5.4434e-03,  2.4618e-03,  9.4639e-03,  5.5519e-04,\n",
       "                       -1.7021e-02,  1.9993e-02,  8.5560e-03, -1.5249e-03,  1.2269e-02,\n",
       "                        6.1649e-03, -2.7140e-03, -2.5157e-03,  2.4226e-03, -1.8230e-02,\n",
       "                        1.9872e-02, -8.9851e-03, -7.3049e-04,  5.1300e-03, -6.7204e-03,\n",
       "                       -1.9436e-02, -7.8674e-03, -4.8218e-03, -5.0746e-03,  2.5587e-03,\n",
       "                       -1.7309e-03, -5.3194e-03, -6.5478e-03,  5.4379e-03,  3.4146e-03,\n",
       "                        1.9624e-02,  9.2156e-03, -2.3989e-03, -1.8323e-02, -1.9426e-02,\n",
       "                       -3.8980e-03,  3.8141e-03,  1.0568e-04, -7.2826e-03,  5.4125e-03,\n",
       "                        1.2518e-02,  4.5191e-03, -8.0192e-03, -6.9391e-03,  9.9145e-03,\n",
       "                       -3.3130e-03,  8.8661e-03, -4.3854e-03, -1.6676e-02,  7.7908e-03,\n",
       "                       -5.3945e-03,  1.7946e-03, -8.1108e-03,  9.2337e-03,  9.0386e-03,\n",
       "                       -7.4046e-03,  5.2457e-03, -5.5013e-04, -4.3745e-03, -2.7774e-03,\n",
       "                       -6.2743e-03, -9.7136e-03, -1.9644e-02, -4.1179e-03,  7.4561e-03,\n",
       "                       -5.4473e-03,  2.2760e-03, -1.8716e-02, -1.7430e-02, -1.5224e-02,\n",
       "                        8.5976e-03, -6.7853e-03,  6.5224e-03,  3.5516e-03, -2.5449e-03,\n",
       "                        3.9739e-04, -1.8566e-02, -4.1245e-03, -5.9153e-03, -6.4309e-04,\n",
       "                        1.5675e-02, -2.1425e-02,  8.6603e-03,  1.1814e-03,  4.8066e-03,\n",
       "                        1.0364e-02,  6.5369e-03,  1.4132e-02,  7.5950e-03,  5.5606e-03,\n",
       "                        1.3520e-03, -2.4613e-03, -1.1440e-02, -1.7939e-02,  4.2140e-03,\n",
       "                       -1.4235e-03,  9.7377e-04, -1.3995e-02, -1.8330e-03, -1.6996e-02,\n",
       "                       -8.9357e-03, -1.0628e-02, -6.7145e-03,  6.5790e-03,  2.4261e-05,\n",
       "                        1.7405e-02,  9.8306e-03,  1.9918e-02, -6.1140e-03, -6.3841e-03,\n",
       "                       -1.9657e-02,  3.2105e-03, -1.9647e-02,  3.2003e-03, -1.8234e-02,\n",
       "                       -1.1455e-02, -6.7856e-03,  4.7100e-03, -7.2070e-03, -1.4563e-02,\n",
       "                        7.9773e-03,  1.7264e-03,  1.4878e-02, -1.5257e-02, -5.3676e-03,\n",
       "                       -1.6236e-03,  3.8265e-03, -1.0886e-02, -1.7189e-02, -6.7239e-03,\n",
       "                        1.6851e-02, -3.1230e-03,  5.1521e-03,  8.7196e-03, -1.0820e-02,\n",
       "                       -4.5142e-04,  1.5523e-02, -5.4834e-03, -9.1691e-03,  5.5793e-03,\n",
       "                        2.0236e-02, -1.9393e-03,  8.6346e-03,  1.3400e-02, -2.0792e-02,\n",
       "                       -1.4845e-02,  2.6019e-03, -8.0893e-03, -4.4732e-03,  6.6419e-03,\n",
       "                        2.1591e-03,  8.5360e-03, -8.4844e-03, -9.4966e-04, -1.9451e-02,\n",
       "                       -4.4343e-03,  6.7123e-03,  6.6253e-03, -1.9737e-03, -7.4564e-03,\n",
       "                       -1.3727e-02,  4.6967e-03,  4.3144e-03, -1.0703e-03, -3.9171e-03,\n",
       "                        2.6417e-03,  3.0676e-03, -5.1197e-03, -6.4731e-04, -1.9503e-02,\n",
       "                        2.7063e-03,  1.5081e-02, -7.2407e-03, -1.4303e-03,  5.4245e-03,\n",
       "                       -1.6120e-02, -8.2914e-03, -3.0941e-03, -4.1870e-04,  4.5454e-03,\n",
       "                       -9.7916e-03,  9.1575e-03, -1.1559e-03, -2.5802e-03, -4.1216e-03,\n",
       "                       -1.8023e-03, -1.3898e-02,  1.6197e-02,  1.5533e-02,  5.8619e-03,\n",
       "                        1.3594e-02, -6.8221e-03, -5.4275e-03,  1.5347e-02,  1.7676e-03,\n",
       "                       -1.5446e-03, -1.8459e-02,  1.1246e-02, -2.3661e-03,  2.4660e-03,\n",
       "                        6.0681e-03],\n",
       "                      [-6.8090e-05, -6.2587e-06, -1.7641e-04,  1.1374e-04, -8.4347e-05,\n",
       "                       -1.0219e-04,  1.7634e-04,  3.6192e-05, -2.7344e-05,  1.3934e-04,\n",
       "                       -4.9293e-05, -9.8910e-05, -7.2856e-05, -2.7893e-05,  2.4776e-04,\n",
       "                        1.7086e-04,  8.8263e-05, -7.7383e-05, -9.2670e-05,  4.1027e-05,\n",
       "                       -3.1119e-05,  2.0099e-04,  7.4249e-05,  1.0046e-04, -2.1793e-04,\n",
       "                        2.7378e-04,  2.8925e-05,  1.4964e-04,  2.9156e-04,  3.0206e-04,\n",
       "                        8.6826e-05, -7.1696e-06,  2.2987e-05,  1.0619e-04,  2.7840e-05,\n",
       "                        1.0251e-05, -6.6382e-05,  5.1242e-05,  8.5113e-05,  5.1718e-05,\n",
       "                        4.4923e-05, -1.1246e-04, -4.8532e-05, -1.6043e-04, -8.3282e-06,\n",
       "                       -1.1025e-04,  1.5301e-04, -4.0346e-05, -6.6252e-05,  1.7235e-04,\n",
       "                        3.7982e-05, -3.7738e-07, -9.9622e-05, -3.8212e-05, -8.2622e-05,\n",
       "                        8.0928e-05, -2.9823e-04, -1.1257e-05, -5.9971e-05,  8.4405e-06,\n",
       "                       -1.4863e-04, -5.7398e-05,  2.9206e-05,  1.6418e-04,  6.2922e-05,\n",
       "                       -6.0477e-05, -1.3522e-04,  8.4589e-05, -4.1095e-05,  7.2603e-05,\n",
       "                       -1.2845e-04,  1.0412e-04, -1.3838e-05,  1.9779e-04,  1.9137e-04,\n",
       "                        3.4995e-05,  5.7370e-05,  4.8798e-05,  1.0116e-04,  1.4095e-04,\n",
       "                        6.3876e-05,  1.6201e-05,  3.5668e-05, -1.5721e-04,  2.2695e-05,\n",
       "                       -6.0854e-05, -1.8383e-04,  2.0297e-04, -3.6616e-05, -9.7233e-05,\n",
       "                        1.6455e-04, -1.0747e-04, -7.0114e-05, -1.8432e-04, -1.2238e-04,\n",
       "                       -5.5499e-05, -1.6640e-04,  5.0317e-05,  3.6698e-05, -6.9883e-05,\n",
       "                       -2.2088e-05,  1.1667e-04,  7.2238e-05, -8.6830e-05,  8.8220e-05,\n",
       "                        4.0853e-05,  1.4282e-05,  1.2917e-04,  1.2808e-04,  6.2025e-05,\n",
       "                        1.9146e-05,  5.8351e-05, -1.4580e-04,  2.0964e-04, -1.7574e-04,\n",
       "                       -6.6640e-05,  6.0956e-05,  8.3943e-05, -7.7681e-06,  6.4053e-05,\n",
       "                        1.9425e-05, -1.3066e-04, -7.6187e-05,  1.5998e-04, -3.6413e-05,\n",
       "                        3.2494e-05, -9.6112e-06,  2.2240e-04,  7.2622e-05, -4.7422e-05,\n",
       "                       -1.4310e-04,  3.9394e-05,  1.9489e-05, -1.7302e-04,  6.5821e-05,\n",
       "                       -1.5791e-04, -5.5435e-05,  2.4000e-04,  1.0365e-04, -6.6008e-05,\n",
       "                        1.1678e-04, -1.0251e-04,  6.6113e-05, -7.7515e-05, -6.2848e-06,\n",
       "                        3.1835e-04, -1.0726e-04, -1.3224e-04, -2.3765e-05,  1.8525e-04,\n",
       "                       -2.1832e-04,  6.1980e-05,  2.2376e-05,  7.0632e-05, -6.2497e-05,\n",
       "                       -3.6544e-05,  1.4290e-04, -6.4816e-05,  9.6773e-05, -4.4076e-05,\n",
       "                        5.5853e-05,  7.7803e-05, -1.3405e-04,  7.1462e-05,  4.2263e-05,\n",
       "                       -5.1310e-05,  1.4295e-05,  1.2027e-04,  1.1992e-04, -2.2810e-05,\n",
       "                       -5.4727e-05,  4.4560e-05,  4.6799e-05,  1.1847e-04, -4.2035e-05,\n",
       "                        9.8752e-05, -5.7853e-05,  4.2603e-05,  2.0144e-04,  2.1927e-05,\n",
       "                        3.2375e-05,  3.4728e-05,  1.7254e-04, -6.2477e-05,  1.6162e-04,\n",
       "                       -1.4862e-04, -8.5017e-05, -7.8775e-05,  1.0717e-05,  4.0452e-05,\n",
       "                        7.5040e-05,  4.4358e-05,  1.1357e-05,  4.5774e-05, -1.1825e-04,\n",
       "                       -6.1811e-05,  2.5819e-05,  9.6345e-05,  2.2520e-05, -1.5038e-04,\n",
       "                       -1.3766e-04, -1.0139e-04, -8.3685e-06, -5.5659e-05, -4.9848e-05,\n",
       "                        2.3962e-05,  9.6219e-06,  2.4952e-05, -6.9218e-06,  2.1985e-04,\n",
       "                       -3.3492e-05,  3.1061e-05,  8.6174e-05,  5.4537e-05,  6.9379e-05,\n",
       "                       -1.1767e-04, -2.6545e-05, -1.4193e-04, -1.0936e-04, -1.6347e-04,\n",
       "                        1.6163e-05,  1.4483e-04, -7.5558e-05, -2.3919e-04, -6.4091e-05,\n",
       "                        4.0993e-05,  4.7933e-05, -1.6497e-04, -3.1438e-05,  1.5690e-04,\n",
       "                        5.4295e-06, -2.6365e-05,  4.2563e-05,  3.2573e-07, -1.0980e-04,\n",
       "                       -6.2036e-07, -7.0685e-06, -6.8603e-05,  7.9204e-05,  7.7297e-05,\n",
       "                        3.1872e-05, -3.0354e-05,  1.0133e-04,  6.7367e-05, -5.0063e-05,\n",
       "                        4.6849e-05,  1.4024e-04, -1.6152e-04,  1.6382e-04,  5.4815e-05,\n",
       "                       -1.6118e-04,  9.3097e-05, -1.5246e-04, -2.0083e-04, -8.3299e-05,\n",
       "                        6.8501e-05]]))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_processor.model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "example_generator completed reading all examples. No more data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rowancassius/Desktop/capstone/LSTM_Summarizer/data_util/batcher.py\", line 443, in text_generator\n",
      "    example = next(example_generator)\n",
      "StopIteration\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rowancassius/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/rowancassius/opt/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/rowancassius/Desktop/capstone/LSTM_Summarizer/data_util/batcher.py\", line 424, in fill_example_queue\n",
      "    context, task, summary = next(input_gen) # read the next example from file. article and abstract are both strings.\n",
      "RuntimeError: generator raised StopIteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "Summarizing Batch...\n",
      "INFO:tensorflow:Finished reading dataset in single_pass mode.\n"
     ]
    }
   ],
   "source": [
    "decoded_sents, ref_sents, article_sents = eval_processor.evaluate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ref_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decoded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = Rouge().get_scores(decoded_sents, ref_sents, avg = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.0871131137765286,\n",
       "  'p': 0.13652983032293375,\n",
       "  'r': 0.07235935126401657},\n",
       " 'rouge-2': {'f': 0.02123266716332826,\n",
       "  'p': 0.034140667761357414,\n",
       "  'r': 0.017825412135756964},\n",
       " 'rouge-l': {'f': 0.09327039381614881,\n",
       "  'p': 0.15944581280788178,\n",
       "  'r': 0.07215619629412733}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref</th>\n",
       "      <th>decoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>tell Tom to clarify the cp</td>\n",
       "      <td>Kate Tom Alonso please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>determine how renewal contract work will be coordinated</td>\n",
       "      <td>with with CDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>forward information to beneficial people</td>\n",
       "      <td>Cynthia Kase Cynthia Kase please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>review draft of 20/20 Term Sheet</td>\n",
       "      <td>be a Letter of Intent please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>forward things to SENDER</td>\n",
       "      <td>John John Sam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       ref  \\\n",
       "0  tell Tom to clarify the cp                                \n",
       "1  determine how renewal contract work will be coordinated   \n",
       "2  forward information to beneficial people                  \n",
       "3  review draft of 20/20 Term Sheet                          \n",
       "4  forward things to SENDER                                  \n",
       "\n",
       "                            decoded  \n",
       "0  Kate Tom Alonso please            \n",
       "1  with with CDC                     \n",
       "2  Cynthia Kase Cynthia Kase please  \n",
       "3  be a Letter of Intent please      \n",
       "4  John John Sam                     "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'ref': ref_sents, 'decoded': decoded_sents})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(config.log_root, 'data/test_results_2.csv'), sep = '\\t', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'packet' in vocab._word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'interview' in vocab._word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some training examples\n",
    "pd.DataFrame({'ref': ref_sents, 'decoded': decoded_sents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..\n",
    "..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in nltk.corpus.wordnet.words(): print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(nltk.corpus.wordnet.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
