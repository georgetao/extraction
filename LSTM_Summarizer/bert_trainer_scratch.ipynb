{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Outside imports\n",
    "import os\n",
    "import importlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train\n",
    "import model\n",
    "import evaluate\n",
    "import beam_search\n",
    "import data_util.data\n",
    "import data_util.batcher\n",
    "import data_util.config\n",
    "\n",
    "\n",
    "importlib.reload(model)\n",
    "importlib.reload(train)\n",
    "importlib.reload(evaluate)\n",
    "importlib.reload(beam_search)\n",
    "importlib.reload(data_util.config)\n",
    "importlib.reload(data_util.data)\n",
    "importlib.reload(data_util.batcher)\n",
    "\n",
    "from model import *\n",
    "from train import *\n",
    "from beam_search import *\n",
    "from evaluate import *\n",
    "from data_util.data import *\n",
    "from data_util.batcher import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load real data\n",
    "data_path = '/Users/rowancassius/Desktop/capstone/data/context_task_data.tsv'\n",
    "dat = pd.read_csv(data_path, sep='\\t')\n",
    "\n",
    "# test/train split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    dat.TaskSentence.values, \n",
    "    dat.Summary.values,\n",
    "    test_size=0.2,\n",
    "    random_state=0,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# fit tokenizer\n",
    "tokenizer = Tokenizer(lower=True, filters='!\"#$%&()*+,./:;<=>?[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(list(x_train)+list(y_train))\n",
    "\n",
    "# lowercase the data\n",
    "x_train = tokenizer.sequences_to_texts(tokenizer.texts_to_sequences(x_train))\n",
    "y_train = tokenizer.sequences_to_texts(tokenizer.texts_to_sequences(y_train))\n",
    "\n",
    "examples = list(zip(x_train, y_train))\n",
    "# examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "T.manual_seed(123)\n",
    "if T.cuda.is_available():\n",
    "    T.cuda.manual_seed_all(123)\n",
    "    \n",
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_size of vocab was specified as 2829; we now have 2829 words. Stopping reading.\n",
      "Finished constructing vocabulary of 2829 total words. Last word added: foundation\n",
      "INFO:tensorflow:Bucket queue size: 0, Input queue size: 439\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n"
     ]
    }
   ],
   "source": [
    "# Standards\n",
    "# Make Vocab\n",
    "vocab = data.Vocab(\n",
    "    words = tokenizer.word_index.keys(), \n",
    "    max_size=len(tokenizer.word_index))\n",
    "\n",
    "# Make Batcher\n",
    "batcher = Batcher(\n",
    "    examples=examples,\n",
    "    vocab=vocab, \n",
    "    mode='train', \n",
    "    batch_size=32, \n",
    "    single_pass=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mle = \"yes\"\n",
    "train_rl = \"no\"\n",
    "mle_weight = 1.0\n",
    "load_model = None\n",
    "new_lr = None\n",
    "rl_weight = 1 - mle_weight\n",
    "\n",
    "opt = Namespace(train_mle = train_mle, \n",
    "                train_rl = train_rl, \n",
    "                mle_weight = mle_weight, \n",
    "                load_model = load_model,\n",
    "                new_lr = new_lr, \n",
    "                rl_weight = rl_weight)\n",
    "\n",
    "\n",
    "train_processor = Train(vocab, batcher, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_processor.setup_train(model=Model)\n",
    "# train_processor.train_one_batch(train_processor.batcher.next_batch(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_generator completed reading all examples. No more data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-38:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rowancassius/Desktop/capstone/LSTM_Summarizer/data_util/batcher.py\", line 273, in text_generator\n",
      "    e = next(example_generator) # e is a tf.Example\n",
      "StopIteration\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rowancassius/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/rowancassius/opt/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/rowancassius/Desktop/capstone/LSTM_Summarizer/data_util/batcher.py\", line 207, in fill_example_queue\n",
      "    (article, abstract) = next(input_gen) # read the next example from file. article and abstract are both strings.\n",
      "RuntimeError: generator raised StopIteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n"
     ]
    }
   ],
   "source": [
    "task = \"validate\"\n",
    "load_model = os.path.join(config.log_root, \"data/saved_models/0000500.tar\")\n",
    "\n",
    "opt = Namespace(task = task, load_model = load_model)\n",
    "\n",
    "# new batcher for evaluation\n",
    "batcher = Batcher(\n",
    "    examples=examples[:20],\n",
    "    vocab=vocab, \n",
    "    mode='train', \n",
    "    batch_size=10, \n",
    "    single_pass=True)\n",
    "\n",
    "eval_processor = Evaluate(vocab, batcher, opt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "Summarizing Batch...\n",
      "x_t before: torch.Size([40])\n",
      "x_t: torch.Size([40, 256])\n",
      "x_t before: torch.Size([40])\n",
      "x_t: torch.Size([40, 256])\n",
      "x_t before: torch.Size([40])\n",
      "x_t: torch.Size([40, 256])\n",
      "x_t before: torch.Size([24])\n",
      "x_t: torch.Size([24, 256])\n",
      "x_t before: torch.Size([16])\n",
      "x_t: torch.Size([16, 256])\n",
      "Summarizing Batch...\n",
      "x_t before: torch.Size([40])\n",
      "x_t: torch.Size([40, 256])\n",
      "x_t before: torch.Size([40])\n",
      "x_t: torch.Size([40, 256])\n",
      "x_t before: torch.Size([40])\n",
      "x_t: torch.Size([40, 256])\n",
      "x_t before: torch.Size([32])\n",
      "x_t: torch.Size([32, 256])\n",
      "x_t before: torch.Size([32])\n",
      "x_t: torch.Size([32, 256])\n",
      "x_t before: torch.Size([16])\n",
      "x_t: torch.Size([16, 256])\n",
      "x_t before: torch.Size([12])\n",
      "x_t: torch.Size([12, 256])\n",
      "x_t before: torch.Size([12])\n",
      "x_t: torch.Size([12, 256])\n",
      "x_t before: torch.Size([8])\n",
      "x_t: torch.Size([8, 256])\n",
      "INFO:tensorflow:Finished reading dataset in single_pass mode.\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n"
     ]
    }
   ],
   "source": [
    "decoded_sents, ref_sents = eval_processor.evaluate_batch(model=Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Repeating with the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Bucket queue size: 18, Input queue size: 111\n"
     ]
    }
   ],
   "source": [
    "# Make Vocab\n",
    "bert_vocab = BertVocab('bert-base-uncased')\n",
    "\n",
    "# Make Batcher\n",
    "bert_batcher = Batcher(\n",
    "    examples=examples,\n",
    "    vocab=bert_vocab, \n",
    "    mode='train', \n",
    "    batch_size=32, \n",
    "    single_pass=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n"
     ]
    }
   ],
   "source": [
    "train_mle = \"yes\"\n",
    "train_rl = \"no\"\n",
    "mle_weight = 1.0\n",
    "load_model = None\n",
    "new_lr = None\n",
    "rl_weight = 1 - mle_weight\n",
    "\n",
    "opt = Namespace(train_mle = train_mle, \n",
    "                train_rl = train_rl, \n",
    "                mle_weight = mle_weight, \n",
    "                load_model = load_model,\n",
    "                new_lr = new_lr, \n",
    "                rl_weight = rl_weight)\n",
    "\n",
    "\n",
    "bert_train_processor = Train(bert_vocab, bert_batcher, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_train_processor.setup_train(model=BertSummarizer)\n",
    "# bert_train_processor.train_one_batch(bert_train_processor.batcher.next_batch(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "1\n",
      "2\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "3\n",
      "4\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "iter: 5 mle_loss: 6.355 reward: 0.0000\n",
      "5\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "6\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "7\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "8\n",
      "9\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "iter: 10 mle_loss: 4.720 reward: 0.0000\n",
      "model saved at: \n",
      " data/saved_models/0000010.tar\n",
      "10\n",
      "11\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "12\n",
      "13\n",
      "14\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "iter: 15 mle_loss: 4.161 reward: 0.0000\n",
      "15\n",
      "16\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "17\n",
      "18\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "19\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "iter: 20 mle_loss: 3.603 reward: 0.0000\n",
      "model saved at: \n",
      " data/saved_models/0000020.tar\n",
      "20\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "21\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "22\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "23\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "24\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "iter: 25 mle_loss: 3.509 reward: 0.0000\n",
      "25\n",
      "26\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "27\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "28\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "29\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "iter: 30 mle_loss: 3.375 reward: 0.0000\n",
      "model saved at: \n",
      " data/saved_models/0000030.tar\n",
      "30\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "31\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n",
      "32\n",
      "-------------------Keyboard Interrupt------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/capstone/LSTM_Summarizer/train.py\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(self, n_iters, model, report_every, save_every)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m                 \u001b[0mmle_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/capstone/LSTM_Summarizer/train.py\u001b[0m in \u001b[0;36mtrain_one_batch\u001b[0;34m(self, batch, iter)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmle_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmle_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrl_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c6acc22427e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_train_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBertSummarizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/capstone/LSTM_Summarizer/train.py\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(self, n_iters, model, report_every, save_every)\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------Keyboard Interrupt------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mmle_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmle_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "bert_train_processor.trainIters(n_iters=50, model=BertSummarizer, report_every=5, save_every = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_generator completed reading all examples. No more data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-33:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rowancassius/Desktop/capstone/LSTM_Summarizer/data_util/batcher.py\", line 273, in text_generator\n",
      "    e = next(example_generator) # e is a tf.Example\n",
      "StopIteration\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rowancassius/opt/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/rowancassius/opt/anaconda3/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/rowancassius/Desktop/capstone/LSTM_Summarizer/data_util/batcher.py\", line 207, in fill_example_queue\n",
      "    (article, abstract) = next(input_gen) # read the next example from file. article and abstract are both strings.\n",
      "RuntimeError: generator raised StopIteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n"
     ]
    }
   ],
   "source": [
    "task = \"validate\"\n",
    "load_model = os.path.join(config.log_root, \"data/saved_models/0000020.tar\")\n",
    "\n",
    "opt = Namespace(task = task, load_model = load_model)\n",
    "\n",
    "# new batcher for evaluation\n",
    "bert_decode_batcher = Batcher(\n",
    "    examples=examples[:20],\n",
    "    vocab=bert_vocab, \n",
    "    mode='train', \n",
    "    batch_size=10, \n",
    "    single_pass=True)\n",
    "\n",
    "eval_processor = Evaluate(bert_vocab, bert_decode_batcher, opt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished reading dataset in single_pass mode.\n"
     ]
    }
   ],
   "source": [
    "decoded_sents, ref_sents = eval_processor.evaluate_batch(model=BertSummarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 32000\n"
     ]
    }
   ],
   "source": [
    "decoded_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
