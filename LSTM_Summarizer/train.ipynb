{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\georg\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\georg\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\georg\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\georg\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\georg\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\georg\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"    #Set cuda device\n",
    "\n",
    "import time\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from model import Model\n",
    "\n",
    "from data_util import config, data\n",
    "from data_util.batcher import Batcher\n",
    "from data_util.data import Vocab\n",
    "from train_util import *\n",
    "from torch.distributions import Categorical\n",
    "from rouge import Rouge\n",
    "from numpy import random\n",
    "import argparse;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "T.manual_seed(123)\n",
    "if T.cuda.is_available():\n",
    "    T.cuda.manual_seed_all(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train(object):\n",
    "    def __init__(self, opt):\n",
    "        self.vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "        self.batcher = Batcher(config.train_data_path, self.vocab, mode='train',\n",
    "                               batch_size=config.batch_size, single_pass=False)\n",
    "        self.opt = opt\n",
    "        self.start_id = self.vocab.word2id(data.START_DECODING)\n",
    "        self.end_id = self.vocab.word2id(data.STOP_DECODING)\n",
    "        self.pad_id = self.vocab.word2id(data.PAD_TOKEN)\n",
    "        self.unk_id = self.vocab.word2id(data.UNKNOWN_TOKEN)\n",
    "        time.sleep(5)\n",
    "\n",
    "    def save_model(self, iter):\n",
    "        save_path = config.save_model_path + \"/%07d.tar\" % iter\n",
    "        T.save({\n",
    "            \"iter\": iter + 1,\n",
    "            \"model_dict\": self.model.state_dict(),\n",
    "            \"trainer_dict\": self.trainer.state_dict()\n",
    "        }, save_path)\n",
    "\n",
    "    def setup_train(self):\n",
    "        self.model = Model()\n",
    "        self.model = get_cuda(self.model)\n",
    "        self.trainer = T.optim.Adam(self.model.parameters(), lr=config.lr)\n",
    "        start_iter = 0\n",
    "        if self.opt.load_model is not None:\n",
    "            load_model_path = os.path.join(config.save_model_path, self.opt.load_model)\n",
    "            checkpoint = T.load(load_model_path)\n",
    "            start_iter = checkpoint[\"iter\"]\n",
    "            self.model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "            self.trainer.load_state_dict(checkpoint[\"trainer_dict\"])\n",
    "            print(\"Loaded model at \" + load_model_path)\n",
    "        if self.opt.new_lr is not None:\n",
    "            self.trainer = T.optim.Adam(self.model.parameters(), lr=self.opt.new_lr)\n",
    "        return start_iter\n",
    "\n",
    "    def train_batch_MLE(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, batch):\n",
    "        ''' Calculate Negative Log Likelihood Loss for the given batch. In order to reduce exposure bias,\n",
    "                pass the previous generated token as input with a probability of 0.25 instead of ground truth label\n",
    "        Args:\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param batch: batch object\n",
    "        '''\n",
    "        dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)                        #Get input and target batchs for training decoder\n",
    "        step_losses = []\n",
    "        s_t = (enc_hidden[0], enc_hidden[1])                                                        #Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))                             #Input to the decoder\n",
    "        prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None                                                                    #Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "            use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()                        #Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "            x_t = use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t                   #Select decoder input based on use_ground_truth probabilities\n",
    "            x_t = self.model.embeds(x_t)\n",
    "            final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
    "            target = target_batch[:, t]\n",
    "            log_probs = T.log(final_dist + config.eps)\n",
    "            step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=self.pad_id)\n",
    "            step_losses.append(step_loss)\n",
    "            x_t = T.multinomial(final_dist, 1).squeeze()                                            #Sample words from final distribution which can be used as input in next time step\n",
    "            is_oov = (x_t >= config.vocab_size).long()                                              #Mask indicating whether sampled word is OOV\n",
    "            x_t = (1 - is_oov) * x_t.detach() + (is_oov) * self.unk_id                              #Replace OOVs with [UNK] token\n",
    "\n",
    "        losses = T.sum(T.stack(step_losses, 1), 1)                                                  #unnormalized losses for each example in the batch; (batch_size)\n",
    "        batch_avg_loss = losses / dec_lens                                                          #Normalized losses; (batch_size)\n",
    "        mle_loss = T.mean(batch_avg_loss)                                                           #Average batch loss\n",
    "        return mle_loss\n",
    "\n",
    "    def train_batch_RL(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, article_oovs, greedy):\n",
    "        '''Generate sentences from decoder entirely using sampled tokens as input. These sentences are used for ROUGE evaluation\n",
    "        Args\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param article_oovs: Batch containing list of OOVs in each example\n",
    "        :param greedy: If true, performs greedy based sampling, else performs multinomial sampling\n",
    "        Returns:\n",
    "        :decoded_strs: List of decoded sentences\n",
    "        :log_probs: Log probabilities of sampled words\n",
    "        '''\n",
    "        s_t = enc_hidden                                                                            #Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))                             #Input to the decoder\n",
    "        prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None                                                                    #Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        inds = []                                                                                   #Stores sampled indices for each time step\n",
    "        decoder_padding_mask = []                                                                   #Stores padding masks of generated samples\n",
    "        log_probs = []                                                                              #Stores log probabilites of generated samples\n",
    "        mask = get_cuda(T.LongTensor(len(enc_out)).fill_(1))                                        #Values that indicate whether [STOP] token has already been encountered; 1 => Not encountered, 0 otherwise\n",
    "\n",
    "        for t in range(config.max_dec_steps):\n",
    "            x_t = self.model.embeds(x_t)\n",
    "            probs, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
    "            if greedy is False:\n",
    "                multi_dist = Categorical(probs)\n",
    "                x_t = multi_dist.sample()                                                           #perform multinomial sampling\n",
    "                log_prob = multi_dist.log_prob(x_t)\n",
    "                log_probs.append(log_prob)\n",
    "            else:\n",
    "                _, x_t = T.max(probs, dim=1)                                                        #perform greedy sampling\n",
    "            x_t = x_t.detach()\n",
    "            inds.append(x_t)\n",
    "            mask_t = get_cuda(T.zeros(len(enc_out)))                                                #Padding mask of batch for current time step\n",
    "            mask_t[mask == 1] = 1                                                                   #If [STOP] is not encountered till previous time step, mask_t = 1 else mask_t = 0\n",
    "            mask[(mask == 1) + (x_t == self.end_id) == 2] = 0                                       #If [STOP] is not encountered till previous time step and current word is [STOP], make mask = 0\n",
    "            decoder_padding_mask.append(mask_t)\n",
    "            is_oov = (x_t>=config.vocab_size).long()                                                #Mask indicating whether sampled word is OOV\n",
    "            x_t = (1-is_oov)*x_t + (is_oov)*self.unk_id                                             #Replace OOVs with [UNK] token\n",
    "\n",
    "        inds = T.stack(inds, dim=1)\n",
    "        decoder_padding_mask = T.stack(decoder_padding_mask, dim=1)\n",
    "        if greedy is False:                                                                         #If multinomial based sampling, compute log probabilites of sampled words\n",
    "            log_probs = T.stack(log_probs, dim=1)\n",
    "            log_probs = log_probs * decoder_padding_mask                                            #Not considering sampled words with padding mask = 0\n",
    "            lens = T.sum(decoder_padding_mask, dim=1)                                               #Length of sampled sentence\n",
    "            log_probs = T.sum(log_probs, dim=1) / lens  # (bs,)                                     #compute normalizied log probability of a sentence\n",
    "        decoded_strs = []\n",
    "        for i in range(len(enc_out)):\n",
    "            id_list = inds[i].cpu().numpy()\n",
    "            oovs = article_oovs[i]\n",
    "            S = data.outputids2words(id_list, self.vocab, oovs)                                     #Generate sentence corresponding to sampled words\n",
    "            try:\n",
    "                end_idx = S.index(data.STOP_DECODING)\n",
    "                S = S[:end_idx]\n",
    "            except ValueError:\n",
    "                S = S\n",
    "            if len(S) < 2:                                                                           #If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
    "                S = [\"xxx\"]\n",
    "            S = \" \".join(S)\n",
    "            decoded_strs.append(S)\n",
    "\n",
    "        return decoded_strs, log_probs\n",
    "\n",
    "    def reward_function(self, decoded_sents, original_sents):\n",
    "        rouge = Rouge()\n",
    "        try:\n",
    "            scores = rouge.get_scores(decoded_sents, original_sents)\n",
    "        except Exception:\n",
    "            print(\"Rouge failed for multi sentence evaluation.. Finding exact pair\")\n",
    "            scores = []\n",
    "            for i in range(len(decoded_sents)):\n",
    "                try:\n",
    "                    score = rouge.get_scores(decoded_sents[i], original_sents[i])\n",
    "                except Exception:\n",
    "                    print(\"Error occured at:\")\n",
    "                    print(\"decoded_sents:\", decoded_sents[i])\n",
    "                    print(\"original_sents:\", original_sents[i])\n",
    "                    score = [{\"rouge-l\":{\"f\":0.0}}]\n",
    "                scores.append(score[0])\n",
    "        rouge_l_f1 = [score[\"rouge-l\"][\"f\"] for score in scores]\n",
    "        rouge_l_f1 = get_cuda(T.FloatTensor(rouge_l_f1))\n",
    "        return rouge_l_f1\n",
    "\n",
    "    # def write_to_file(self, decoded, max, original, sample_r, baseline_r, iter):\n",
    "    #     with open(\"temp.txt\", \"w\") as f:\n",
    "    #         f.write(\"iter:\"+str(iter)+\"\\n\")\n",
    "    #         for i in range(len(original)):\n",
    "    #             f.write(\"dec: \"+decoded[i]+\"\\n\")\n",
    "    #             f.write(\"max: \"+max[i]+\"\\n\")\n",
    "    #             f.write(\"org: \"+original[i]+\"\\n\")\n",
    "    #             f.write(\"Sample_R: %.4f, Baseline_R: %.4f\\n\\n\"%(sample_r[i].item(), baseline_r[i].item()))\n",
    "\n",
    "\n",
    "    def train_one_batch(self, batch, iter):\n",
    "        enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "\n",
    "        enc_batch = self.model.embeds(enc_batch)                                                    #Get embeddings for encoder input\n",
    "        enc_out, enc_hidden = self.model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "        # -------------------------------Summarization-----------------------\n",
    "        if self.opt.train_mle == \"yes\":                                                             #perform MLE training\n",
    "            mle_loss = self.train_batch_MLE(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, batch)\n",
    "        else:\n",
    "            mle_loss = get_cuda(T.FloatTensor([0]))\n",
    "        # --------------RL training-----------------------------------------------------\n",
    "        if self.opt.train_rl == \"yes\":                                                              #perform reinforcement learning training\n",
    "            # multinomial sampling\n",
    "            sample_sents, RL_log_probs = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, batch.art_oovs, greedy=False)\n",
    "            with T.autograd.no_grad():\n",
    "                # greedy sampling\n",
    "                greedy_sents, _ = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, batch.art_oovs, greedy=True)\n",
    "\n",
    "            sample_reward = self.reward_function(sample_sents, batch.original_abstracts)\n",
    "            baseline_reward = self.reward_function(greedy_sents, batch.original_abstracts)\n",
    "            # if iter%200 == 0:\n",
    "            #     self.write_to_file(sample_sents, greedy_sents, batch.original_abstracts, sample_reward, baseline_reward, iter)\n",
    "            rl_loss = -(sample_reward - baseline_reward) * RL_log_probs                             #Self-critic policy gradient training (eq 15 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "            rl_loss = T.mean(rl_loss)\n",
    "\n",
    "            batch_reward = T.mean(sample_reward).item()\n",
    "        else:\n",
    "            rl_loss = get_cuda(T.FloatTensor([0]))\n",
    "            batch_reward = 0\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "        self.trainer.zero_grad()\n",
    "        (self.opt.mle_weight * mle_loss + self.opt.rl_weight * rl_loss).backward()\n",
    "        self.trainer.step()\n",
    "\n",
    "        return mle_loss.item(), batch_reward\n",
    "\n",
    "    def trainIters(self):\n",
    "        iter = self.setup_train()\n",
    "        count = mle_total = r_total = 0\n",
    "        while iter <= config.max_iterations:\n",
    "            print(iter)\n",
    "            batch = self.batcher.next_batch()\n",
    "            try:\n",
    "                mle_loss, r = self.train_one_batch(batch, iter)\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"-------------------Keyboard Interrupt------------------\")\n",
    "                exit(0)\n",
    "\n",
    "            mle_total += mle_loss\n",
    "            r_total += r\n",
    "            count += 1\n",
    "            iter += 1\n",
    "\n",
    "            if iter % 1000 == 0:\n",
    "                mle_avg = mle_total / count\n",
    "                r_avg = r_total / count\n",
    "                print(\"iter:\", iter, \"mle_loss:\", \"%.3f\" % mle_avg, \"reward:\", \"%.4f\" % r_avg)\n",
    "                count = mle_total = r_total = 0\n",
    "\n",
    "            if iter % 5000 == 0:\n",
    "                self.save_model(iter)\n",
    "\n",
    "            self.save_model(iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Bucket queue size: 0, Input queue size: 0\n",
      "0\n",
      "INFO:tensorflow:Bucket queue size: 755, Input queue size: 0\n",
      "1\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 141209\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "2\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "3\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "4\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "5\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "6\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "7\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "8\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "9\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "10\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n",
      "INFO:tensorflow:Bucket queue size: 1000, Input queue size: 200000\n"
     ]
    }
   ],
   "source": [
    "train_mle = \"yes\"\n",
    "train_rl = \"no\"\n",
    "mle_weight = 1.0\n",
    "load_model = None\n",
    "new_lr = None\n",
    "rl_weight = 1 - mle_weight\n",
    "\n",
    "opt = Namespace(train_mle = train_mle, train_rl = train_rl, mle_weight = mle_weight, load_model = load_model,\n",
    "               new_lr = new_lr, rl_weight = rl_weight)\n",
    "train_processor = Train(opt)\n",
    "train_processor.trainIters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
