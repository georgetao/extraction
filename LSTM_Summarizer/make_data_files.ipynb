{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import tqdm\n",
    "from tensorflow.core.example import example_pb2\n",
    "import struct\n",
    "import random\n",
    "import shutil;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths and hyperparameters\n",
    "finished_path = \"data/finished\"\n",
    "unfinished_path = \"data/unfinished\"\n",
    "chunk_path = \"data/chunked\"\n",
    "\n",
    "vocab_path = \"data/vocab\"\n",
    "VOCAB_SIZE = 200000\n",
    "\n",
    "CHUNK_SIZE = 15000 # num examples per chunk, for the chunked data\n",
    "train_bin_path = os.path.join(finished_path, \"train.bin\")\n",
    "valid_bin_path = os.path.join(finished_path, \"valid.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_folder(folder_path):\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_text_data(unshuffled_art, unshuffled_abs, shuffled_art, shuffled_abs):\n",
    "    article_itr = open(os.path.join(unfinished_path, unshuffled_art), \"r\")\n",
    "    abstract_itr = open(os.path.join(unfinished_path, unshuffled_abs), \"r\")\n",
    "    list_of_pairs = []\n",
    "    for article in article_itr:\n",
    "        article = article.strip()\n",
    "        abstract = next(abstract_itr).strip()\n",
    "        list_of_pairs.append((article, abstract))\n",
    "    article_itr.close()\n",
    "    abstract_itr.close()\n",
    "    random.shuffle(list_of_pairs)\n",
    "    article_itr = open(os.path.join(unfinished_path, shuffled_art), \"w\")\n",
    "    abstract_itr = open(os.path.join(unfinished_path, shuffled_abs), \"w\")\n",
    "    for pair in list_of_pairs:\n",
    "        article_itr.write(pair[0]+\"\\n\")\n",
    "        abstract_itr.write(pair[1]+\"\\n\")\n",
    "    article_itr.close()\n",
    "    abstract_itr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_bin(article_path, abstract_path, out_file, vocab_counter = None):\n",
    "\n",
    "    with open(out_file, 'wb') as writer:\n",
    "\n",
    "        article_itr = open(article_path, 'r')\n",
    "        abstract_itr = open(abstract_path, 'r')\n",
    "        for article in tqdm.tqdm(article_itr):\n",
    "            article = article.strip()\n",
    "            abstract = next(abstract_itr).strip()\n",
    "\n",
    "            tf_example = example_pb2.Example()\n",
    "            tf_example.features.feature['article'].bytes_list.value.extend([str.encode(article)])\n",
    "            tf_example.features.feature['abstract'].bytes_list.value.extend([str.encode(abstract)])\n",
    "            tf_example_str = tf_example.SerializeToString()\n",
    "            str_len = len(tf_example_str)\n",
    "            writer.write(struct.pack('q', str_len))\n",
    "            writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "\n",
    "            if vocab_counter is not None:\n",
    "                art_tokens = article.split(' ')\n",
    "                abs_tokens = abstract.split(' ')\n",
    "                # abs_tokens = [t for t in abs_tokens if\n",
    "                #               t not in [SENTENCE_START, SENTENCE_END]]  # remove these tags from vocab\n",
    "                tokens = art_tokens + abs_tokens\n",
    "                tokens = [t.strip() for t in tokens]  # strip\n",
    "                tokens = [t for t in tokens if t != \"\"]  # remove empty\n",
    "                vocab_counter.update(tokens)\n",
    "\n",
    "    if vocab_counter is not None:\n",
    "        with open(vocab_path, 'w') as writer:\n",
    "            for word, count in vocab_counter.most_common(VOCAB_SIZE):\n",
    "                writer.write(word + ' ' + str(count) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_finished_data():\n",
    "    make_folder(finished_path)\n",
    "\n",
    "    vocab_counter = collections.Counter()\n",
    "\n",
    "    write_to_bin(os.path.join(unfinished_path, \"train.art.shuf.txt\"), os.path.join(unfinished_path, \"train.abs.shuf.txt\"), train_bin_path, vocab_counter)\n",
    "    write_to_bin(os.path.join(unfinished_path, \"valid.art.shuf.txt\"), os.path.join(unfinished_path, \"valid.abs.shuf.txt\"), valid_bin_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_file(set_name, chunks_dir, bin_file):\n",
    "    make_folder(chunks_dir)\n",
    "    reader = open(bin_file, \"rb\")\n",
    "    chunk = 0\n",
    "    finished = False\n",
    "    while not finished:\n",
    "        chunk_fname = os.path.join(chunks_dir, '%s_%04d.bin' % (set_name, chunk)) # new chunk\n",
    "        with open(chunk_fname, 'wb') as writer:\n",
    "            for _ in range(CHUNK_SIZE):\n",
    "                len_bytes = reader.read(8)\n",
    "                if not len_bytes:\n",
    "                    finished = True\n",
    "                    break\n",
    "                str_len = struct.unpack('q', len_bytes)[0]\n",
    "                example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "                writer.write(struct.pack('q', str_len))\n",
    "                writer.write(struct.pack('%ds' % str_len, example_str))\n",
    "            chunk += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling data\n",
    "shuffle_text_data(\"train.article.txt\", \"train.title.txt\", \"train.art.shuf.txt\", \"train.abs.shuf.txt\")\n",
    "shuffle_text_data(\"valid.article.filter.txt\", \"valid.title.filter.txt\", \"valid.art.shuf.txt\", \"valid.abs.shuf.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3803957it [16:16, 3894.26it/s] \n",
      "189651it [00:37, 5020.13it/s]\n"
     ]
    }
   ],
   "source": [
    "#creating bin files\n",
    "delete_folder(finished_path)\n",
    "creating_finished_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunking main bin files into smaller ones\n",
    "delete_folder(chunk_path)\n",
    "chunk_file(\"train\", os.path.join(chunk_path, \"train\"), train_bin_path)\n",
    "chunk_file(\"valid\", os.path.join(chunk_path, \"main_valid\"), valid_bin_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/chunked\\\\test\\\\test_00.bin'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performing rouge evaluation on 1.9 lakh sentences takes lot of time. \n",
    "#So, create mini validation set & test set by borrowing 15k samples each from these 1.9 lakh sentences\n",
    "make_folder(os.path.join(chunk_path, \"valid\"))\n",
    "make_folder(os.path.join(chunk_path, \"test\"))\n",
    "bin_chunks = os.listdir(os.path.join(chunk_path, \"main_valid\"))\n",
    "bin_chunks.sort()\n",
    "samples = random.sample(set(bin_chunks[:-1]), 2)      #Exclude last bin file; contains only 9k sentences\n",
    "valid_chunk, test_chunk = samples[0], samples[1]\n",
    "shutil.copyfile(os.path.join(chunk_path, \"main_valid\", valid_chunk), os.path.join(chunk_path, \"valid\", \"valid_00.bin\"))\n",
    "shutil.copyfile(os.path.join(chunk_path, \"main_valid\", test_chunk), os.path.join(chunk_path, \"test\", \"test_00.bin\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
