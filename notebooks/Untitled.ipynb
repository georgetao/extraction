{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The great bamino was a fantastic tasty man immediately with many documents, and don't forget the juice\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)\n",
    "#     print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The great bamino was a fantastic tasty man immediately with many documents, and don't forget the juice\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)\n",
    "#     print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# CREATE CUSTOM NLP function from SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\"])\n",
    "# nlp.add_pipe(custom_ents, name = 'custom', first=True)\n",
    "nlp.add_pipe(nlp.create_pipe(\"merge_entities\"))\n",
    "nlp.add_pipe(nlp.create_pipe(\"merge_noun_chunks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(nlp.create_pipe(\"merge_noun_chunks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Hello, I am a sarcastic gambler. Please walk to the famous Google.com and visit 458-972-9648')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I, a sarcastic gambler, the famous Google.com]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Hello,\n",
       " ,,\n",
       " I,\n",
       " am,\n",
       " a sarcastic gambler,\n",
       " .,\n",
       " Please,\n",
       " walk,\n",
       " to,\n",
       " the famous Google.com,\n",
       " and,\n",
       " visit,\n",
       " 458,\n",
       " -,\n",
       " 972,\n",
       " -,\n",
       " 9648]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I | I | nsubj | am\n",
      "a sarcastic gambler | a sarcastic gambler | attr | am\n",
      "the famous Google.com | the famous Google.com | pobj | to\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(nlp.create_pipe(\"merge_noun_chunks\"))\n",
    "nlp.add_pipe(nlp.create_pipe(\"merge_entities\"))\n",
    "doc = nlp('Hello, I am a sarcastic gambler. Please walk to the famous Google.com and visit 458-972-9648')\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, '|', chunk.root.text, '|', chunk.root.dep_, '|', chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "texts = [t.text for t in nlp(\"I have a blue car\")]\n",
    "assert texts == [\"I\", \"have\", \"a\", \"blue\", \"car\"]\n",
    "\n",
    "nlp.add_pipe(nlp.create_pipe(\"merge_entities\"))\n",
    "nlp.add_pipe(nlp.create_pipe(\"merge_noun_chunks\"))\n",
    "\n",
    "texts = [t.text for t in nlp(\"I have a blue car\")]\n",
    "assert texts == [\"I\", \"have\", \"a blue car\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Dear Vince, I hope your trip to Australia was successful. It's one of SENDER's favorite places to go. I've copied you on the email to Mike initiating Enron's trial service to Energycast. Thanks for helping to set this up. Would you ask the authorities in Enron to refresh my access to Enrononline?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text + '\\t\\t\\t\\t' + chunk.root.text + '\\t\\t\\t\\t' + chunk.root.dep_ + '\\t' + chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.t.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in doc:\n",
    "    print(t.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lookups import Lookups\n",
    "lookups = Lookups()\n",
    "lookups.add_table(\"lemma_rules\", {\"noun\": [[\"s\", \"\"]]})\n",
    "lemmatizer = Lemmatizer(lookups)\n",
    "lemmas = lemmatizer(\"ducks\", \"NOUN\")\n",
    "assert lemmas == [\"duck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer(\"my\", \"ADJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
