{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# flair model stuff\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "# from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "\n",
    "# fastai ULMFiT model stuff\n",
    "import fastai\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "\n",
    "# fasttext model stuff\n",
    "import fasttext\n",
    "\n",
    "# keras for cnn stuff\n",
    "from nltk import word_tokenize\n",
    "import gensim\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data and format for model\n",
    "\n",
    "train = pd.read_csv('../data/email_intent_train.txt', header=None, sep='\\t')\n",
    "train.columns = [\"label\", \"text\"]\n",
    "train['label'] = '__label__' + train['label'].astype(str)\n",
    "\n",
    "test = pd.read_csv('../data/email_intent_test.txt', header=None, sep='\\t')\n",
    "test.columns = [\"label\", \"text\"]\n",
    "test['label'] = '__label__' + test['label'].astype(str)\n",
    "\n",
    "data = train.append(test)\n",
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, dev, and test\n",
    "\n",
    "data.iloc[0:int(len(data)*0.8)].to_csv('../data/practice_model/train.csv', sep='\\t', index = False, header = None)\n",
    "data.iloc[int(len(data)*0.8):int(len(data)*0.9)].to_csv('../data/practice_model/test.csv', sep='\\t', index = False, header = False)\n",
    "data.iloc[int(len(data)*0.9):].to_csv('../data/practice_model/dev.csv', sep='\\t', index = False, header = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Naive Bayes, Linear SVM, Logistic Regression\n",
    "\n",
    "* Easy Implementation - https://medium.com/data-from-the-trenches/text-classification-the-first-step-toward-nlp-mastery-f5f95d525d73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    # strip whitespaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    # remove numbers\n",
    "    text = re.sub(\"(\\d)+\", \"\", text)\n",
    "    \n",
    "    # lower case everything\n",
    "    text = text.lower()\n",
    "    \n",
    "    # replace punctuation characters with spaces\n",
    "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "    \n",
    "    return text\n",
    "\n",
    "data[\"text\"] = data[\"text\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "\n",
    "shuffled_data = data.sample(frac=1).reset_index(drop=True)\n",
    "s_train = shuffled_data.iloc[0:int(len(data)*0.9)]\n",
    "s_test = shuffled_data.iloc[int(len(data)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words vectorization\n",
    "\n",
    "# this vectorizer will skip stop words\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    preprocessor=preprocess\n",
    ")\n",
    "\n",
    "# fit the vectorizer on the training text\n",
    "training_features = vectorizer.fit_transform(s_train[\"text\"])\n",
    "\n",
    "# Transform each text into a vector of word counts\n",
    "test_features = vectorizer.transform(s_test[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency vectorization\n",
    "# TODO Different kinds of tf\n",
    "\n",
    "# this vectorizer will skip stop words\n",
    "tf_vectorizer = TfidfVectorizer(stop_words=\"english\",\n",
    "                             preprocessor=preprocess,\n",
    "                             ngram_range=(1, 2))\n",
    "\n",
    "\n",
    "# fit the vectorizer on the training text\n",
    "tf_training_features = tf_vectorizer.fit_transform(s_train[\"text\"])\n",
    "\n",
    "# Transform each text into a vector of word counts\n",
    "tf_test_features = tf_vectorizer.transform(s_test[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset with BOW vectorization: 55.27\n",
      "Accuracy on the test dataset with TF vectorization: 59.35\n"
     ]
    }
   ],
   "source": [
    "# Training Naive Bayes\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(training_features.toarray(), s_train[\"label\"])\n",
    "nb_acc = nb_model.score(test_features.toarray(), s_test[\"label\"])\n",
    "\n",
    "print(\"Accuracy on the test dataset with BOW vectorization: {:.2f}\".format(nb_acc*100))\n",
    "\n",
    "nb_tf_model = GaussianNB()\n",
    "nb_tf_model.fit(tf_training_features.toarray(), s_train[\"label\"])\n",
    "nb_tf_acc = nb_tf_model.score(tf_test_features.toarray(), s_test[\"label\"])\n",
    "\n",
    "print(\"Accuracy on the test dataset with TF vectorization: {:.2f}\".format(nb_tf_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset with BOW vectorization: 71.18\n",
      "Accuracy on the test dataset with TF vectorization: 72.90\n"
     ]
    }
   ],
   "source": [
    "# Training Linear SVM\n",
    "\n",
    "model = LinearSVC()\n",
    "model.fit(training_features, s_train[\"label\"])\n",
    "y_pred = model.predict(test_features)\n",
    "\n",
    "# Evaluation\n",
    "acc = accuracy_score(s_test[\"label\"], y_pred)\n",
    "\n",
    "print(\"Accuracy on the test dataset with BOW vectorization: {:.2f}\".format(acc*100))\n",
    "\n",
    "tf_model = LinearSVC()\n",
    "tf_model.fit(tf_training_features, s_train[\"label\"])\n",
    "tf_y_pred = tf_model.predict(tf_test_features)\n",
    "\n",
    "# Evaluation\n",
    "tf_acc = accuracy_score(s_test[\"label\"], tf_y_pred)\n",
    "\n",
    "print(\"Accuracy on the test dataset with TF vectorization: {:.2f}\".format(tf_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset with BOW vectorization: 71.18\n",
      "Accuracy on the test dataset with TF vectorization: 70.54\n"
     ]
    }
   ],
   "source": [
    "# Training Logistic Regression\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(training_features, s_train[\"label\"])\n",
    "log_acc = log_model.score(test_features, s_test[\"label\"])\n",
    "\n",
    "print(\"Accuracy on the test dataset with BOW vectorization: {:.2f}\".format(log_acc*100))\n",
    "\n",
    "log_tf_model = LogisticRegression()\n",
    "log_tf_model.fit(tf_training_features, s_train[\"label\"])\n",
    "log_tf_acc = log_tf_model.score(tf_test_features, s_test[\"label\"])\n",
    "\n",
    "print(\"Accuracy on the test dataset with TF vectorization: {:.2f}\".format(log_tf_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flair Model: Allows combination of different kinds of word embeddings\n",
    "\n",
    "* Model - https://github.com/flairNLP/flair\n",
    "* Explanation - https://www.analyticsvidhya.com/blog/2019/02/flair-nlp-library-python/\n",
    "* Easy Implementation - https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-01 12:05:35,189 Reading data from ../data/practice_model\n",
      "2020-06-01 12:05:35,190 Train: ../data/practice_model/train.csv\n",
      "2020-06-01 12:05:35,191 Dev: ../data/practice_model/dev.csv\n",
      "2020-06-01 12:05:35,192 Test: ../data/practice_model/test.csv\n"
     ]
    }
   ],
   "source": [
    "# load in corpus\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = '../data/practice_model'\n",
    "\n",
    "# column format indicating which columns hold the text and label(s)\n",
    "column_name_map = {1: \"text\", 0: \"label_topic\"}\n",
    "\n",
    "# load corpus containing training, test and dev data\n",
    "corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
    "                                         column_name_map,\n",
    "                                         skip_header=False,\n",
    "                                         delimiter='\\t',    \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:36:44,102 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim.vectors.npy not found in cache, downloading to /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmpqk89ldj2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160000128/160000128 [03:39<00:00, 730082.18B/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:40:23,868 copying /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmpqk89ldj2 to cache at /Users/nataliewang/.flair/embeddings/glove.gensim.vectors.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:40:24,094 removing temp file /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmpqk89ldj2\n",
      "2020-05-24 16:40:24,815 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim not found in cache, downloading to /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmpvk4ktywm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21494764/21494764 [00:26<00:00, 812018.60B/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:40:52,148 copying /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmpvk4ktywm to cache at /Users/nataliewang/.flair/embeddings/glove.gensim\n",
      "2020-05-24 16:40:52,181 removing temp file /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmpvk4ktywm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:40:53,986 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-forward-1024-v0.2rc.pt not found in cache, downloading to /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmp3mjfp051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19689779/19689779 [00:24<00:00, 805576.13B/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:41:19,129 copying /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmp3mjfp051 to cache at /Users/nataliewang/.flair/embeddings/lm-news-english-forward-1024-v0.2rc.pt\n",
      "2020-05-24 16:41:19,162 removing temp file /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmp3mjfp051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:41:19,905 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-backward-1024-v0.2rc.pt not found in cache, downloading to /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmp6ci7guch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19689779/19689779 [00:20<00:00, 943009.28B/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:41:41,701 copying /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmp6ci7guch to cache at /Users/nataliewang/.flair/embeddings/lm-news-english-backward-1024-v0.2rc.pt\n",
      "2020-05-24 16:41:41,734 removing temp file /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmp6ci7guch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/nataliewang/.pyenv/versions/3.7.3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated method __init__. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:41:41,792 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4182/4182 [00:01<00:00, 3970.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:41:42,918 [b'__label__No', b'__label__Yes']\n",
      "2020-05-24 16:41:42,925 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:41:42,927 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentLSTMEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=2148, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2020-05-24 16:41:42,928 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:41:42,929 Corpus: \"Corpus: 3718 train + 464 dev + 464 test sentences\"\n",
      "2020-05-24 16:41:42,930 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:41:42,931 Parameters:\n",
      "2020-05-24 16:41:42,932  - learning_rate: \"0.1\"\n",
      "2020-05-24 16:41:42,933  - mini_batch_size: \"32\"\n",
      "2020-05-24 16:41:42,934  - patience: \"3\"\n",
      "2020-05-24 16:41:42,935  - anneal_factor: \"0.5\"\n",
      "2020-05-24 16:41:42,936  - max_epochs: \"10\"\n",
      "2020-05-24 16:41:42,937  - shuffle: \"True\"\n",
      "2020-05-24 16:41:42,938  - train_with_dev: \"False\"\n",
      "2020-05-24 16:41:42,938  - batch_growth_annealing: \"False\"\n",
      "2020-05-24 16:41:42,940 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:41:42,941 Model training base path: \".\"\n",
      "2020-05-24 16:41:42,942 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:41:42,942 Device: cpu\n",
      "2020-05-24 16:41:42,943 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:41:42,944 Embeddings storage mode: cpu\n",
      "2020-05-24 16:41:42,946 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:42:01,902 epoch 1 - iter 11/117 - loss 0.70836253 - samples/sec: 18.65\n",
      "2020-05-24 16:42:22,094 epoch 1 - iter 22/117 - loss 0.69428441 - samples/sec: 17.50\n",
      "2020-05-24 16:42:43,909 epoch 1 - iter 33/117 - loss 0.69242128 - samples/sec: 16.19\n",
      "2020-05-24 16:43:08,382 epoch 1 - iter 44/117 - loss 0.67784121 - samples/sec: 14.85\n",
      "2020-05-24 16:43:29,958 epoch 1 - iter 55/117 - loss 0.67401460 - samples/sec: 16.36\n",
      "2020-05-24 16:43:49,654 epoch 1 - iter 66/117 - loss 0.66823633 - samples/sec: 17.92\n",
      "2020-05-24 16:44:12,963 epoch 1 - iter 77/117 - loss 0.66822653 - samples/sec: 15.14\n",
      "2020-05-24 16:44:32,273 epoch 1 - iter 88/117 - loss 0.66663722 - samples/sec: 19.02\n",
      "2020-05-24 16:44:53,397 epoch 1 - iter 99/117 - loss 0.67022049 - samples/sec: 16.72\n",
      "2020-05-24 16:45:16,356 epoch 1 - iter 110/117 - loss 0.66924216 - samples/sec: 15.37\n",
      "2020-05-24 16:45:32,523 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:45:32,524 EPOCH 1 done: loss 0.6649 - lr 0.1000000\n",
      "2020-05-24 16:46:03,866 DEV : loss 0.7192593812942505 - score 0.5022\n",
      "2020-05-24 16:46:04,070 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-05-24 16:46:07,140 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:46:28,472 epoch 2 - iter 11/117 - loss 0.68297398 - samples/sec: 16.59\n",
      "2020-05-24 16:46:49,683 epoch 2 - iter 22/117 - loss 0.66689447 - samples/sec: 16.65\n",
      "2020-05-24 16:47:08,603 epoch 2 - iter 33/117 - loss 0.65276714 - samples/sec: 18.68\n",
      "2020-05-24 16:47:30,495 epoch 2 - iter 44/117 - loss 0.64982150 - samples/sec: 16.13\n",
      "2020-05-24 16:47:59,306 epoch 2 - iter 55/117 - loss 0.64710541 - samples/sec: 12.25\n",
      "2020-05-24 16:48:26,360 epoch 2 - iter 66/117 - loss 0.63973053 - samples/sec: 13.04\n",
      "2020-05-24 16:48:51,819 epoch 2 - iter 77/117 - loss 0.63352901 - samples/sec: 14.49\n",
      "2020-05-24 16:49:11,910 epoch 2 - iter 88/117 - loss 0.63049927 - samples/sec: 17.58\n",
      "2020-05-24 16:49:31,787 epoch 2 - iter 99/117 - loss 0.63561533 - samples/sec: 17.76\n",
      "2020-05-24 16:49:59,499 epoch 2 - iter 110/117 - loss 0.63469473 - samples/sec: 12.73\n",
      "2020-05-24 16:50:10,759 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:50:10,766 EPOCH 2 done: loss 0.6310 - lr 0.1000000\n",
      "2020-05-24 16:50:46,963 DEV : loss 0.7386118769645691 - score 0.5259\n",
      "2020-05-24 16:50:47,163 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-05-24 16:50:50,029 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:51:16,445 epoch 3 - iter 11/117 - loss 0.60101805 - samples/sec: 13.38\n",
      "2020-05-24 16:51:37,783 epoch 3 - iter 22/117 - loss 0.59084967 - samples/sec: 16.56\n",
      "2020-05-24 16:51:58,768 epoch 3 - iter 33/117 - loss 0.60476210 - samples/sec: 16.84\n",
      "2020-05-24 16:52:20,934 epoch 3 - iter 44/117 - loss 0.60295659 - samples/sec: 15.93\n",
      "2020-05-24 16:52:38,526 epoch 3 - iter 55/117 - loss 0.61635514 - samples/sec: 21.00\n",
      "2020-05-24 16:53:04,323 epoch 3 - iter 66/117 - loss 0.61293271 - samples/sec: 13.68\n",
      "2020-05-24 16:53:19,718 epoch 3 - iter 77/117 - loss 0.61093556 - samples/sec: 22.99\n",
      "2020-05-24 16:53:38,527 epoch 3 - iter 88/117 - loss 0.61266717 - samples/sec: 18.78\n",
      "2020-05-24 16:53:59,442 epoch 3 - iter 99/117 - loss 0.61210733 - samples/sec: 16.89\n",
      "2020-05-24 16:54:21,522 epoch 3 - iter 110/117 - loss 0.60566508 - samples/sec: 15.98\n",
      "2020-05-24 16:54:38,528 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:54:38,529 EPOCH 3 done: loss 0.6030 - lr 0.1000000\n",
      "2020-05-24 16:55:16,696 DEV : loss 0.9432142376899719 - score 0.4849\n",
      "2020-05-24 16:55:16,954 BAD EPOCHS (no improvement): 1\n",
      "2020-05-24 16:55:16,957 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:55:38,517 epoch 4 - iter 11/117 - loss 0.58918658 - samples/sec: 16.41\n",
      "2020-05-24 16:56:06,463 epoch 4 - iter 22/117 - loss 0.60190228 - samples/sec: 12.63\n",
      "2020-05-24 16:56:29,793 epoch 4 - iter 33/117 - loss 0.58647567 - samples/sec: 15.14\n",
      "2020-05-24 16:56:49,572 epoch 4 - iter 44/117 - loss 0.58039266 - samples/sec: 17.86\n",
      "2020-05-24 16:57:10,149 epoch 4 - iter 55/117 - loss 0.56594196 - samples/sec: 17.16\n",
      "2020-05-24 16:57:30,933 epoch 4 - iter 66/117 - loss 0.56998557 - samples/sec: 16.99\n",
      "2020-05-24 16:57:59,469 epoch 4 - iter 77/117 - loss 0.56593655 - samples/sec: 12.37\n",
      "2020-05-24 16:58:17,344 epoch 4 - iter 88/117 - loss 0.57704548 - samples/sec: 19.76\n",
      "2020-05-24 16:58:37,631 epoch 4 - iter 99/117 - loss 0.57861140 - samples/sec: 17.42\n",
      "2020-05-24 16:58:59,877 epoch 4 - iter 110/117 - loss 0.58184447 - samples/sec: 16.69\n",
      "2020-05-24 16:59:11,243 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:59:11,244 EPOCH 4 done: loss 0.5808 - lr 0.1000000\n",
      "2020-05-24 16:59:39,472 DEV : loss 0.8019097447395325 - score 0.625\n",
      "2020-05-24 16:59:39,668 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-05-24 16:59:42,297 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:00:06,145 epoch 5 - iter 11/117 - loss 0.56030329 - samples/sec: 14.83\n",
      "2020-05-24 17:00:26,824 epoch 5 - iter 22/117 - loss 0.56216718 - samples/sec: 17.10\n",
      "2020-05-24 17:00:51,419 epoch 5 - iter 33/117 - loss 0.57832237 - samples/sec: 14.83\n",
      "2020-05-24 17:01:15,031 epoch 5 - iter 44/117 - loss 0.56841880 - samples/sec: 14.95\n",
      "2020-05-24 17:01:41,658 epoch 5 - iter 55/117 - loss 0.56744353 - samples/sec: 13.26\n",
      "2020-05-24 17:01:59,948 epoch 5 - iter 66/117 - loss 0.56151756 - samples/sec: 19.32\n",
      "2020-05-24 17:02:20,369 epoch 5 - iter 77/117 - loss 0.56124652 - samples/sec: 17.29\n",
      "2020-05-24 17:02:39,199 epoch 5 - iter 88/117 - loss 0.56270077 - samples/sec: 18.76\n",
      "2020-05-24 17:03:02,254 epoch 5 - iter 99/117 - loss 0.55721527 - samples/sec: 16.26\n",
      "2020-05-24 17:03:25,205 epoch 5 - iter 110/117 - loss 0.55636753 - samples/sec: 15.38\n",
      "2020-05-24 17:03:39,546 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:03:39,547 EPOCH 5 done: loss 0.5558 - lr 0.1000000\n",
      "2020-05-24 17:04:12,533 DEV : loss 0.5636223554611206 - score 0.7069\n",
      "2020-05-24 17:04:12,778 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-05-24 17:04:15,799 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:04:41,763 epoch 6 - iter 11/117 - loss 0.59740614 - samples/sec: 13.62\n",
      "2020-05-24 17:05:06,271 epoch 6 - iter 22/117 - loss 0.57424033 - samples/sec: 14.40\n",
      "2020-05-24 17:05:33,836 epoch 6 - iter 33/117 - loss 0.56795000 - samples/sec: 13.20\n",
      "2020-05-24 17:05:55,459 epoch 6 - iter 44/117 - loss 0.57015063 - samples/sec: 16.34\n",
      "2020-05-24 17:06:13,505 epoch 6 - iter 55/117 - loss 0.55505213 - samples/sec: 19.58\n",
      "2020-05-24 17:06:43,486 epoch 6 - iter 66/117 - loss 0.55239158 - samples/sec: 11.77\n",
      "2020-05-24 17:07:13,778 epoch 6 - iter 77/117 - loss 0.55297939 - samples/sec: 11.65\n",
      "2020-05-24 17:07:35,311 epoch 6 - iter 88/117 - loss 0.55040927 - samples/sec: 16.40\n",
      "2020-05-24 17:07:56,617 epoch 6 - iter 99/117 - loss 0.54500690 - samples/sec: 16.58\n",
      "2020-05-24 17:08:20,952 epoch 6 - iter 110/117 - loss 0.54198349 - samples/sec: 14.51\n",
      "2020-05-24 17:08:33,172 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:08:33,174 EPOCH 6 done: loss 0.5466 - lr 0.1000000\n",
      "2020-05-24 17:09:05,636 DEV : loss 0.5493430495262146 - score 0.7263\n",
      "2020-05-24 17:09:05,956 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-05-24 17:09:08,878 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:09:29,317 epoch 7 - iter 11/117 - loss 0.52695005 - samples/sec: 17.32\n",
      "2020-05-24 17:09:53,262 epoch 7 - iter 22/117 - loss 0.51861469 - samples/sec: 15.12\n",
      "2020-05-24 17:10:12,674 epoch 7 - iter 33/117 - loss 0.52935068 - samples/sec: 18.19\n",
      "2020-05-24 17:10:36,982 epoch 7 - iter 44/117 - loss 0.51718497 - samples/sec: 14.52\n",
      "2020-05-24 17:11:00,699 epoch 7 - iter 55/117 - loss 0.51978967 - samples/sec: 14.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 17:11:24,005 epoch 7 - iter 66/117 - loss 0.51994762 - samples/sec: 15.92\n",
      "2020-05-24 17:11:49,245 epoch 7 - iter 77/117 - loss 0.51758517 - samples/sec: 13.98\n",
      "2020-05-24 17:12:09,247 epoch 7 - iter 88/117 - loss 0.52253921 - samples/sec: 17.65\n",
      "2020-05-24 17:12:32,898 epoch 7 - iter 99/117 - loss 0.52428676 - samples/sec: 14.92\n",
      "2020-05-24 17:12:51,971 epoch 7 - iter 110/117 - loss 0.51994464 - samples/sec: 18.52\n",
      "2020-05-24 17:13:12,028 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:13:12,035 EPOCH 7 done: loss 0.5197 - lr 0.1000000\n",
      "2020-05-24 17:13:48,998 DEV : loss 1.6004676818847656 - score 0.4181\n",
      "2020-05-24 17:13:49,217 BAD EPOCHS (no improvement): 1\n",
      "2020-05-24 17:13:49,224 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:14:12,126 epoch 8 - iter 11/117 - loss 0.64657290 - samples/sec: 15.46\n",
      "2020-05-24 17:14:30,924 epoch 8 - iter 22/117 - loss 0.56181443 - samples/sec: 18.81\n",
      "2020-05-24 17:14:48,683 epoch 8 - iter 33/117 - loss 0.58387581 - samples/sec: 19.91\n",
      "2020-05-24 17:15:06,931 epoch 8 - iter 44/117 - loss 0.56100546 - samples/sec: 19.37\n",
      "2020-05-24 17:15:27,183 epoch 8 - iter 55/117 - loss 0.54486730 - samples/sec: 18.17\n",
      "2020-05-24 17:15:47,222 epoch 8 - iter 66/117 - loss 0.54410925 - samples/sec: 17.63\n",
      "2020-05-24 17:16:14,600 epoch 8 - iter 77/117 - loss 0.53692037 - samples/sec: 12.89\n",
      "2020-05-24 17:16:33,999 epoch 8 - iter 88/117 - loss 0.53279875 - samples/sec: 18.21\n",
      "2020-05-24 17:16:50,790 epoch 8 - iter 99/117 - loss 0.53015632 - samples/sec: 21.05\n",
      "2020-05-24 17:17:09,914 epoch 8 - iter 110/117 - loss 0.53265487 - samples/sec: 19.33\n",
      "2020-05-24 17:17:19,820 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:17:19,820 EPOCH 8 done: loss 0.5333 - lr 0.1000000\n",
      "2020-05-24 17:17:48,455 DEV : loss 0.5582072734832764 - score 0.7328\n",
      "2020-05-24 17:17:48,657 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-05-24 17:17:51,276 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:18:09,347 epoch 9 - iter 11/117 - loss 0.50018499 - samples/sec: 19.59\n",
      "2020-05-24 17:18:25,731 epoch 9 - iter 22/117 - loss 0.50062181 - samples/sec: 21.59\n",
      "2020-05-24 17:18:45,046 epoch 9 - iter 33/117 - loss 0.50442699 - samples/sec: 18.29\n",
      "2020-05-24 17:19:04,858 epoch 9 - iter 44/117 - loss 0.50721900 - samples/sec: 18.65\n",
      "2020-05-24 17:19:26,173 epoch 9 - iter 55/117 - loss 0.51618802 - samples/sec: 16.57\n",
      "2020-05-24 17:19:53,708 epoch 9 - iter 66/117 - loss 0.51729644 - samples/sec: 12.82\n",
      "2020-05-24 17:20:21,409 epoch 9 - iter 77/117 - loss 0.51486435 - samples/sec: 12.74\n",
      "2020-05-24 17:20:40,147 epoch 9 - iter 88/117 - loss 0.51013853 - samples/sec: 18.85\n",
      "2020-05-24 17:21:04,510 epoch 9 - iter 99/117 - loss 0.51307353 - samples/sec: 15.02\n",
      "2020-05-24 17:21:29,544 epoch 9 - iter 110/117 - loss 0.51116739 - samples/sec: 14.10\n",
      "2020-05-24 17:21:39,236 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:21:39,237 EPOCH 9 done: loss 0.5090 - lr 0.1000000\n",
      "2020-05-24 17:22:13,398 DEV : loss 0.624588668346405 - score 0.6961\n",
      "2020-05-24 17:22:13,605 BAD EPOCHS (no improvement): 1\n",
      "2020-05-24 17:22:13,612 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:22:33,942 epoch 10 - iter 11/117 - loss 0.48576158 - samples/sec: 17.40\n",
      "2020-05-24 17:22:53,786 epoch 10 - iter 22/117 - loss 0.47863473 - samples/sec: 17.81\n",
      "2020-05-24 17:23:16,548 epoch 10 - iter 33/117 - loss 0.49771777 - samples/sec: 16.01\n",
      "2020-05-24 17:23:36,494 epoch 10 - iter 44/117 - loss 0.50350817 - samples/sec: 17.71\n",
      "2020-05-24 17:24:01,884 epoch 10 - iter 55/117 - loss 0.49575015 - samples/sec: 13.90\n",
      "2020-05-24 17:24:26,033 epoch 10 - iter 66/117 - loss 0.49612824 - samples/sec: 14.62\n",
      "2020-05-24 17:24:51,245 epoch 10 - iter 77/117 - loss 0.48928730 - samples/sec: 14.01\n",
      "2020-05-24 17:25:19,187 epoch 10 - iter 88/117 - loss 0.49164211 - samples/sec: 13.17\n",
      "2020-05-24 17:25:39,064 epoch 10 - iter 99/117 - loss 0.49286634 - samples/sec: 17.77\n",
      "2020-05-24 17:26:02,084 epoch 10 - iter 110/117 - loss 0.49714684 - samples/sec: 15.34\n",
      "2020-05-24 17:26:13,878 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:26:13,879 EPOCH 10 done: loss 0.4998 - lr 0.1000000\n",
      "2020-05-24 17:26:47,054 DEV : loss 0.529371976852417 - score 0.7414\n",
      "2020-05-24 17:26:47,285 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-05-24 17:26:53,073 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:26:53,074 Testing using best model ...\n",
      "2020-05-24 17:26:53,078 loading file best-model.pt\n",
      "2020-05-24 17:27:15,552 0.771551724137931\t0.771551724137931\t0.771551724137931\n",
      "2020-05-24 17:27:15,555 \n",
      "MICRO_AVG: acc 0.771551724137931 - f1-score 0.771551724137931\n",
      "MACRO_AVG: acc 0.771551724137931 - f1-score 0.7655895529501477\n",
      "__label__No tp: 216 - fp: 74 - fn: 32 - tn: 142 - precision: 0.7448 - recall: 0.8710 - accuracy: 0.7716 - f1-score: 0.8030\n",
      "__label__Yes tp: 142 - fp: 32 - fn: 74 - tn: 216 - precision: 0.8161 - recall: 0.6574 - accuracy: 0.7716 - f1-score: 0.7282\n",
      "2020-05-24 17:27:15,555 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.771551724137931,\n",
       " 'dev_score_history': [0.5021551724137931,\n",
       "  0.5258620689655172,\n",
       "  0.4849137931034483,\n",
       "  0.625,\n",
       "  0.7068965517241379,\n",
       "  0.7262931034482759,\n",
       "  0.41810344827586204,\n",
       "  0.7327586206896551,\n",
       "  0.6961206896551724,\n",
       "  0.7413793103448276],\n",
       " 'train_loss_history': [0.6649040309791892,\n",
       "  0.6310247648984958,\n",
       "  0.6030245005575001,\n",
       "  0.5808376736111112,\n",
       "  0.5557609664069282,\n",
       "  0.5466362495198209,\n",
       "  0.5196669252000303,\n",
       "  0.5333108894335918,\n",
       "  0.5089793034598359,\n",
       "  0.4998229920354664],\n",
       " 'dev_loss_history': [0.7192593812942505,\n",
       "  0.7386118769645691,\n",
       "  0.9432142376899719,\n",
       "  0.8019097447395325,\n",
       "  0.5636223554611206,\n",
       "  0.5493430495262146,\n",
       "  1.6004676818847656,\n",
       "  0.5582072734832764,\n",
       "  0.624588668346405,\n",
       "  0.529371976852417]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using Flair model\n",
    "\n",
    "word_embeddings = [WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')]\n",
    "document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "trainer.train('./', max_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flair Test Score with default params: 0.77155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastAI: ULMFiT - Discriminative fine-tuning, Slanted triangular learning rates, and Gradual unfreezing\n",
    "\n",
    "* Model Doc - https://docs.fast.ai/index.html\n",
    "* Explanation - https://arxiv.org/pdf/1801.06146.pdf\n",
    "* Easy Implementation - https://www.analyticsvidhya.com/blog/2018/11/tutorial-text-classification-ulmfit-fastai-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/practice_model/train.csv\", sep='\\t', header = None)\n",
    "dev = pd.read_csv(\"../data/practice_model/dev.csv\", sep='\\t', header = None)\n",
    "test = pd.read_csv(\"../data/practice_model/test.csv\", sep='\\t', header = None)\n",
    "\n",
    "# Language model data\n",
    "data_lm = TextLMDataBunch.from_df(train_df=train, valid_df=dev, test_df=test, path=\"\")\n",
    "\n",
    "# Classifier model data\n",
    "data_clas = TextClasDataBunch.from_df(path = \"\", train_df = train, valid_df = dev, vocab=data_lm.train_ds.vocab, bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune pre-trained language model\n",
    "\n",
    "learn = language_model_learner(data_lm, arch=AWD_LSTM, drop_mult=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.824266</td>\n",
       "      <td>4.155685</td>\n",
       "      <td>0.240179</td>\n",
       "      <td>00:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the learner object with learning rate = 1e-2\n",
    "\n",
    "learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save encoder for later\n",
    "\n",
    "learn.save_encoder('fastai_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (3719 items)\n",
       "x: TextList\n",
       "xxbos i do n't get to enjoy life 's pleasures .,xxbos xxmaj you do n't need to do this right away , when you get a chance go into xxmaj unify for xxmaj feb and show an actual volume of 0 for the 25th - 28th and a volume of xxunk for the 28th .,xxbos xxmaj in a recent discussion with one of xxmaj enron 's folks the xxmaj city ( or their lawyer ) raised the issue that xxmaj xxunk xxmaj xxunk had represented them in the past , and that there might be a potential xxunk .,xxbos i did n't get home until 10 and brian was on the phone xxunk 11 , so i figured you were xxunk by then .,xxbos i will be here all day tomorrow .\n",
       "y: CategoryList\n",
       "__label__No,__label__Yes,__label__No,__label__No,__label__No\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (465 items)\n",
       "x: TextList\n",
       "xxbos xxmaj please add xxmaj xxunk xxmaj xxunk to our bidweek index survey spreadsheet .,xxbos i would like to invite you to address the group on e - xxmaj xxunk issues at our next meeting ( xxmaj friday , xxmaj september 8th in xxmaj houston ) .,xxbos xxmaj check this xxunk for the resources to help you decide .,xxbos xxmaj looking forward to it .,xxbos xxmaj on the power plant , i 've got a couple of guys you need to talk to .\n",
       "y: CategoryList\n",
       "__label__Yes,__label__Yes,__label__Yes,__label__No,__label__Yes\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(3288, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(3288, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.27999999999999997, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x14c95e730>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(3288, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(3288, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.27999999999999997, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build classifier with encoder\n",
    "\n",
    "learn = text_classifier_learner(data_clas, arch=AWD_LSTM, drop_mult=0.7)\n",
    "learn.load_encoder('fastai_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.642192</td>\n",
       "      <td>0.537274</td>\n",
       "      <td>0.731183</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.591593</td>\n",
       "      <td>0.576960</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.600386</td>\n",
       "      <td>0.524698</td>\n",
       "      <td>0.724731</td>\n",
       "      <td>00:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.587529</td>\n",
       "      <td>0.505716</td>\n",
       "      <td>0.744086</td>\n",
       "      <td>00:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.567034</td>\n",
       "      <td>0.539854</td>\n",
       "      <td>0.720430</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.560789</td>\n",
       "      <td>0.520640</td>\n",
       "      <td>0.720430</td>\n",
       "      <td>00:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.567282</td>\n",
       "      <td>0.515134</td>\n",
       "      <td>0.718280</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.553607</td>\n",
       "      <td>0.495926</td>\n",
       "      <td>0.729032</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.548344</td>\n",
       "      <td>0.477352</td>\n",
       "      <td>0.769892</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.553365</td>\n",
       "      <td>0.492660</td>\n",
       "      <td>0.746237</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fit model\n",
    "\n",
    "learn.fit_one_cycle(10, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FB's FastText\n",
    "\n",
    "* Model - https://github.com/facebookresearch/fastText\n",
    "* Explanation - https://arxiv.org/pdf/1607.01759.pdf\n",
    "* Easy Implementation - https://idevji.com/2017/11/04/tutorial-text-classification-with-python-using-fasttext/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset with FastText: 80.65\n"
     ]
    }
   ],
   "source": [
    "ft_model = fasttext.train_supervised(\"../data/practice_model/train.csv\", epoch=10, loss='hs')\n",
    "results = ft_model.test(\"../data/practice_model/dev.csv\")\n",
    "\n",
    "# print(\"Precision on dev dataset: {:.2f}\".format(results[1]))\n",
    "# print(\"Recall on dev dataset: {:.2f}\".format(results[2]))\n",
    "\n",
    "test_data = pd.read_csv(\"../data/practice_model/test.csv\", sep='\\t', header=None)\n",
    "\n",
    "# Evaluation\n",
    "ft_y_pred = test_data[1].apply(lambda x: ft_model.predict(x)[0][0])\n",
    "ft_acc = accuracy_score(test_data[0], ft_y_pred)\n",
    "\n",
    "print(\"Accuracy on the test dataset with FastText: {:.2f}\".format(ft_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "\n",
    "* Explanation - https://www.aclweb.org/anthology/D14-1181.pdf\n",
    "* Easy Implementation - https://towardsdatascience.com/cnn-sentiment-analysis-1d16b7c5a0e7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/practice_model/train.csv\", sep='\\t', header = None)\n",
    "dev = pd.read_csv(\"../data/practice_model/dev.csv\", sep='\\t', header = None)\n",
    "test = pd.read_csv(\"../data/practice_model/test.csv\", sep='\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nataliewang/.pyenv/versions/3.7.3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/nataliewang/.pyenv/versions/3.7.3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "s_train[\"tokens\"] = [word_tokenize(sentence) for sentence in s_train[\"text\"]]\n",
    "s_test[\"tokens\"] = [word_tokenize(sentence) for sentence in s_test[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70297 words total, with a vocabulary size of 6820\n",
      "Max sentence length is 212\n"
     ]
    }
   ],
   "source": [
    "# training vocabulary\n",
    "\n",
    "all_training_words = [word for tokens in s_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in s_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7754 words total, with a vocabulary size of 1945\n",
      "Max sentence length is 89\n"
     ]
    }
   ],
   "source": [
    "# test vocabulary \n",
    "\n",
    "all_test_words = [word for tokens in s_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in s_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in google news word2vec model\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('models/GoogleNews-vectors-negative300.bin.gz', binary=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings = get_word2vec_embeddings(word2vec, s_train, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nataliewang/.pyenv/versions/3.7.3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/nataliewang/.pyenv/versions/3.7.3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/Users/nataliewang/.pyenv/versions/3.7.3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/nataliewang/.pyenv/versions/3.7.3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "train_nos = [1 if label == \"__label__No\" else 0 for label in s_train[\"label\"]]\n",
    "train_yes = [0 if label == \"__label__No\" else 1 for label in s_train[\"label\"]]\n",
    "\n",
    "s_train[\"pos\"] = train_yes\n",
    "s_train[\"neg\"] = train_nos\n",
    "\n",
    "test_nos = [1 if label == \"__label__No\" else 0 for label in s_test[\"label\"]]\n",
    "test_yes = [0 if label == \"__label__No\" else 1 for label in s_test[\"label\"]]\n",
    "\n",
    "s_test[\"pos\"] = test_yes\n",
    "s_test[\"neg\"] = test_nos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__No</td>\n",
       "      <td>quora keeps throwing mouth watering insults at...</td>\n",
       "      <td>[quora, keeps, throwing, mouth, watering, insu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__No</td>\n",
       "      <td>just after lunch if possible</td>\n",
       "      <td>[just, after, lunch, if, possible]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__No</td>\n",
       "      <td>parquet ever get in touch with you</td>\n",
       "      <td>[parquet, ever, get, in, touch, with, you]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__Yes</td>\n",
       "      <td>i wanted to see if we could set up a time to m...</td>\n",
       "      <td>[i, wanted, to, see, if, we, could, set, up, a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__Yes</td>\n",
       "      <td>chris you set up a meeting with me next week t...</td>\n",
       "      <td>[chris, you, set, up, a, meeting, with, me, ne...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                               text  \\\n",
       "0   __label__No  quora keeps throwing mouth watering insults at...   \n",
       "1   __label__No                      just after lunch if possible    \n",
       "2   __label__No                parquet ever get in touch with you    \n",
       "3  __label__Yes  i wanted to see if we could set up a time to m...   \n",
       "4  __label__Yes  chris you set up a meeting with me next week t...   \n",
       "\n",
       "                                              tokens  pos  neg  \n",
       "0  [quora, keeps, throwing, mouth, watering, insu...    0    1  \n",
       "1                 [just, after, lunch, if, possible]    0    1  \n",
       "2         [parquet, ever, get, in, touch, with, you]    0    1  \n",
       "3  [i, wanted, to, see, if, we, could, set, up, a...    1    0  \n",
       "4  [chris, you, set, up, a, meeting, with, me, ne...    1    0  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4184</th>\n",
       "      <td>__label__Yes</td>\n",
       "      <td>can you come and see me when you get in</td>\n",
       "      <td>[can, you, come, and, see, me, when, you, get,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4185</th>\n",
       "      <td>__label__Yes</td>\n",
       "      <td>if questions arise around an eol deal  please ...</td>\n",
       "      <td>[if, questions, arise, around, an, eol, deal, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4186</th>\n",
       "      <td>__label__No</td>\n",
       "      <td>if you re looking for work  or just want to ma...</td>\n",
       "      <td>[if, you, re, looking, for, work, or, just, wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4187</th>\n",
       "      <td>__label__No</td>\n",
       "      <td>when you sow a   faith seed  let me tell you w...</td>\n",
       "      <td>[when, you, sow, a, faith, seed, let, me, tell...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4188</th>\n",
       "      <td>__label__Yes</td>\n",
       "      <td>quick  give me a cheap stock that looks promis...</td>\n",
       "      <td>[quick, give, me, a, cheap, stock, that, looks...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label                                               text  \\\n",
       "4184  __label__Yes           can you come and see me when you get in    \n",
       "4185  __label__Yes  if questions arise around an eol deal  please ...   \n",
       "4186   __label__No  if you re looking for work  or just want to ma...   \n",
       "4187   __label__No  when you sow a   faith seed  let me tell you w...   \n",
       "4188  __label__Yes  quick  give me a cheap stock that looks promis...   \n",
       "\n",
       "                                                 tokens  pos  neg  \n",
       "4184  [can, you, come, and, see, me, when, you, get,...    1    0  \n",
       "4185  [if, questions, arise, around, an, eol, deal, ...    1    0  \n",
       "4186  [if, you, re, looking, for, work, or, just, wa...    0    1  \n",
       "4187  [when, you, sow, a, faith, seed, let, me, tell...    0    1  \n",
       "4188  [quick, give, me, a, cheap, stock, that, looks...    1    0  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6838 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(s_train[\"text\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(s_train[\"text\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6839, 300)\n"
     ]
    }
   ],
   "source": [
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(s_test[\"text\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['pos', 'neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = s_train[label_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       ...,\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_cnn_data\n",
    "y_tr = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 50, 300)      2051700     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 49, 200)      120200      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 48, 200)      180200      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 47, 200)      240200      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 46, 200)      300200      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 45, 200)      360200      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 200)          0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 200)          0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 200)          0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_24 (Global (None, 200)          0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_25 (Global (None, 200)          0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1000)         0           global_max_pooling1d_21[0][0]    \n",
      "                                                                 global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "                                                                 global_max_pooling1d_24[0][0]    \n",
      "                                                                 global_max_pooling1d_25[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 1000)         0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 128)          128128      dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 128)          0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 2)            258         dropout_13[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,381,086\n",
      "Trainable params: 1,329,386\n",
      "Non-trainable params: 2,051,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3765 samples, validate on 419 samples\n",
      "Epoch 1/10\n",
      "3765/3765 [==============================] - 10s 3ms/step - loss: 0.6265 - acc: 0.6651 - val_loss: 0.4785 - val_acc: 0.7971\n",
      "Epoch 2/10\n",
      "3765/3765 [==============================] - 9s 2ms/step - loss: 0.4533 - acc: 0.7888 - val_loss: 0.3939 - val_acc: 0.8043\n",
      "Epoch 3/10\n",
      "3765/3765 [==============================] - 9s 2ms/step - loss: 0.3297 - acc: 0.8570 - val_loss: 0.4170 - val_acc: 0.8174\n",
      "Epoch 4/10\n",
      "3765/3765 [==============================] - 9s 2ms/step - loss: 0.2584 - acc: 0.8980 - val_loss: 0.3959 - val_acc: 0.8126\n",
      "Epoch 5/10\n",
      "3765/3765 [==============================] - 9s 2ms/step - loss: 0.1463 - acc: 0.9486 - val_loss: 0.4471 - val_acc: 0.8126\n",
      "Epoch 6/10\n",
      "3765/3765 [==============================] - 9s 2ms/step - loss: 0.1075 - acc: 0.9594 - val_loss: 0.6475 - val_acc: 0.8186\n",
      "Epoch 7/10\n",
      "3765/3765 [==============================] - 10s 3ms/step - loss: 0.0587 - acc: 0.9810 - val_loss: 0.4978 - val_acc: 0.8222\n",
      "Epoch 8/10\n",
      "3765/3765 [==============================] - 10s 3ms/step - loss: 0.0279 - acc: 0.9919 - val_loss: 0.6685 - val_acc: 0.8317\n",
      "Epoch 9/10\n",
      "3765/3765 [==============================] - 9s 2ms/step - loss: 0.0202 - acc: 0.9950 - val_loss: 0.6574 - val_acc: 0.8162\n",
      "Epoch 10/10\n",
      "3765/3765 [==============================] - 9s 3ms/step - loss: 0.0123 - acc: 0.9977 - val_loss: 0.7453 - val_acc: 0.8305\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "hist = model.fit(x_train, y_train, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "465/465 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "\n",
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8344086021505376"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(s_test.pos==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__label__No     270\n",
       "__label__Yes    195\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_test.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Transformers\n",
    "\n",
    "* Model - https://github.com/huggingface/transformers\n",
    "* Explanation - \n",
    "* Easy Implementation - https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAM: Explicit Interaction Mechanism towards Text Classification (Encode, Interaction, Aggregation)\n",
    "\n",
    "* Model - https://github.com/NonvolatileMemory/AAAI_2019_EXAM\n",
    "* Explanation - https://arxiv.org/pdf/1811.09386.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
