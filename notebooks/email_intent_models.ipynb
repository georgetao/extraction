{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# flair model stuff\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "# from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "\n",
    "# fastai ULMFiT model stuff\n",
    "import fastai\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "\n",
    "# fasttext model stuff\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data and format for model\n",
    "\n",
    "train = pd.read_csv('../data/email_intent_train.txt', header=None, sep='\\t')\n",
    "train.columns = [\"label\", \"text\"]\n",
    "train['label'] = '__label__' + train['label'].astype(str)\n",
    "\n",
    "test = pd.read_csv('../data/email_intent_test.txt', header=None, sep='\\t')\n",
    "test.columns = [\"label\", \"text\"]\n",
    "test['label'] = '__label__' + test['label'].astype(str)\n",
    "\n",
    "data = train.append(test)\n",
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, dev, and test\n",
    "\n",
    "data.iloc[0:int(len(data)*0.8)].to_csv('../data/practice_model/train.csv', sep='\\t', index = False, header = None)\n",
    "data.iloc[int(len(data)*0.8):int(len(data)*0.9)].to_csv('../data/practice_model/test.csv', sep='\\t', index = False, header = False)\n",
    "data.iloc[int(len(data)*0.9):].to_csv('../data/practice_model/dev.csv', sep='\\t', index = False, header = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Naive Bayes, Linear SVM, Logistic Regression\n",
    "\n",
    "* Easy Implementation - https://medium.com/data-from-the-trenches/text-classification-the-first-step-toward-nlp-mastery-f5f95d525d73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    # strip whitespaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    # remove numbers\n",
    "    text = re.sub(\"(\\d)+\", \"\", text)\n",
    "    \n",
    "    # lower case everything\n",
    "    text = text.lower()\n",
    "    \n",
    "    # replace punctuation characters with spaces\n",
    "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "    \n",
    "    return text\n",
    "\n",
    "data[\"text\"] = data[\"text\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "\n",
    "shuffled_data = data.sample(frac=1).reset_index(drop=True)\n",
    "s_train = shuffled_data.iloc[0:int(len(data)*0.9)]\n",
    "s_test = shuffled_data.iloc[int(len(data)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words vectorization\n",
    "\n",
    "# this vectorizer will skip stop words\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    preprocessor=preprocess\n",
    ")\n",
    "\n",
    "# fit the vectorizer on the training text\n",
    "training_features = vectorizer.fit_transform(s_train[\"text\"])\n",
    "\n",
    "# Transform each text into a vector of word counts\n",
    "test_features = vectorizer.transform(s_test[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency vectorization\n",
    "\n",
    "# this vectorizer will skip stop words\n",
    "tf_vectorizer = TfidfVectorizer(stop_words=\"english\",\n",
    "                             preprocessor=preprocess,\n",
    "                             ngram_range=(1, 2))\n",
    "\n",
    "\n",
    "# fit the vectorizer on the training text\n",
    "tf_training_features = tf_vectorizer.fit_transform(s_train[\"text\"])\n",
    "\n",
    "# Transform each text into a vector of word counts\n",
    "tf_test_features = tf_vectorizer.transform(s_test[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset with BOW vectorization: 58.06\n",
      "Accuracy on the test dataset with TF vectorization: 60.00\n"
     ]
    }
   ],
   "source": [
    "# Training Naive Bayes\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(training_features.toarray(), s_train[\"label\"])\n",
    "nb_acc = nb_model.score(test_features.toarray(), s_test[\"label\"])\n",
    "\n",
    "print(\"Accuracy on the test dataset with BOW vectorization: {:.2f}\".format(nb_acc*100))\n",
    "\n",
    "nb_tf_model = GaussianNB()\n",
    "nb_tf_model.fit(tf_training_features.toarray(), s_train[\"label\"])\n",
    "nb_tf_acc = nb_tf_model.score(tf_test_features.toarray(), s_test[\"label\"])\n",
    "\n",
    "print(\"Accuracy on the test dataset with TF vectorization: {:.2f}\".format(nb_tf_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset with BOW vectorization: 67.53\n",
      "Accuracy on the test dataset with TF vectorization: 72.90\n"
     ]
    }
   ],
   "source": [
    "# Training Linear SVM\n",
    "\n",
    "model = LinearSVC()\n",
    "model.fit(training_features, s_train[\"label\"])\n",
    "y_pred = model.predict(test_features)\n",
    "\n",
    "# Evaluation\n",
    "acc = accuracy_score(s_test[\"label\"], y_pred)\n",
    "\n",
    "print(\"Accuracy on the test dataset with BOW vectorization: {:.2f}\".format(acc*100))\n",
    "\n",
    "tf_model = LinearSVC()\n",
    "tf_model.fit(tf_training_features, s_train[\"label\"])\n",
    "tf_y_pred = tf_model.predict(tf_test_features)\n",
    "\n",
    "# Evaluation\n",
    "tf_acc = accuracy_score(s_test[\"label\"], tf_y_pred)\n",
    "\n",
    "print(\"Accuracy on the test dataset with TF vectorization: {:.2f}\".format(tf_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset with BOW vectorization: 73.76\n",
      "Accuracy on the test dataset with TF vectorization: 72.69\n"
     ]
    }
   ],
   "source": [
    "# Training Logistic Regression\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(training_features, s_train[\"label\"])\n",
    "log_acc = log_model.score(test_features, s_test[\"label\"])\n",
    "\n",
    "print(\"Accuracy on the test dataset with BOW vectorization: {:.2f}\".format(log_acc*100))\n",
    "\n",
    "log_tf_model = LogisticRegression()\n",
    "log_tf_model.fit(tf_training_features, s_train[\"label\"])\n",
    "log_tf_acc = log_tf_model.score(tf_test_features, s_test[\"label\"])\n",
    "\n",
    "print(\"Accuracy on the test dataset with TF vectorization: {:.2f}\".format(log_tf_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flair Model: Allows combination of different kinds of word embeddings\n",
    "\n",
    "* Model - https://github.com/flairNLP/flair\n",
    "* Explanation - https://www.analyticsvidhya.com/blog/2019/02/flair-nlp-library-python/\n",
    "* Easy Implementation - https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in corpus\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = '../data/practice_model'\n",
    "\n",
    "# column format indicating which columns hold the text and label(s)\n",
    "column_name_map = {1: \"text\", 0: \"label_topic\"}\n",
    "\n",
    "# load corpus containing training, test and dev data\n",
    "corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
    "                                         column_name_map,\n",
    "                                         skip_header=False,\n",
    "                                         delimiter='\\t',    \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:36:44,102 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim.vectors.npy not found in cache, downloading to /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmpqk89ldj2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160000128/160000128 [03:39<00:00, 730082.18B/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:40:23,868 copying /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmpqk89ldj2 to cache at /Users/nataliewang/.flair/embeddings/glove.gensim.vectors.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:40:24,094 removing temp file /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmpqk89ldj2\n",
      "2020-05-24 16:40:24,815 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim not found in cache, downloading to /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmpvk4ktywm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21494764/21494764 [00:26<00:00, 812018.60B/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:40:52,148 copying /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmpvk4ktywm to cache at /Users/nataliewang/.flair/embeddings/glove.gensim\n",
      "2020-05-24 16:40:52,181 removing temp file /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmpvk4ktywm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:40:53,986 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-forward-1024-v0.2rc.pt not found in cache, downloading to /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmp3mjfp051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19689779/19689779 [00:24<00:00, 805576.13B/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:41:19,129 copying /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmp3mjfp051 to cache at /Users/nataliewang/.flair/embeddings/lm-news-english-forward-1024-v0.2rc.pt\n",
      "2020-05-24 16:41:19,162 removing temp file /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmp3mjfp051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:41:19,905 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-backward-1024-v0.2rc.pt not found in cache, downloading to /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmp6ci7guch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19689779/19689779 [00:20<00:00, 943009.28B/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:41:41,701 copying /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmp6ci7guch to cache at /Users/nataliewang/.flair/embeddings/lm-news-english-backward-1024-v0.2rc.pt\n",
      "2020-05-24 16:41:41,734 removing temp file /var/folders/__/6ygy68tn6c74340_57scyx500000gn/T/tmp6ci7guch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/nataliewang/.pyenv/versions/3.7.3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated method __init__. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:41:41,792 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4182/4182 [00:01<00:00, 3970.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:41:42,918 [b'__label__No', b'__label__Yes']\n",
      "2020-05-24 16:41:42,925 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:41:42,927 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentLSTMEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=2148, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2020-05-24 16:41:42,928 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:41:42,929 Corpus: \"Corpus: 3718 train + 464 dev + 464 test sentences\"\n",
      "2020-05-24 16:41:42,930 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:41:42,931 Parameters:\n",
      "2020-05-24 16:41:42,932  - learning_rate: \"0.1\"\n",
      "2020-05-24 16:41:42,933  - mini_batch_size: \"32\"\n",
      "2020-05-24 16:41:42,934  - patience: \"3\"\n",
      "2020-05-24 16:41:42,935  - anneal_factor: \"0.5\"\n",
      "2020-05-24 16:41:42,936  - max_epochs: \"10\"\n",
      "2020-05-24 16:41:42,937  - shuffle: \"True\"\n",
      "2020-05-24 16:41:42,938  - train_with_dev: \"False\"\n",
      "2020-05-24 16:41:42,938  - batch_growth_annealing: \"False\"\n",
      "2020-05-24 16:41:42,940 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:41:42,941 Model training base path: \".\"\n",
      "2020-05-24 16:41:42,942 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:41:42,942 Device: cpu\n",
      "2020-05-24 16:41:42,943 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:41:42,944 Embeddings storage mode: cpu\n",
      "2020-05-24 16:41:42,946 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:42:01,902 epoch 1 - iter 11/117 - loss 0.70836253 - samples/sec: 18.65\n",
      "2020-05-24 16:42:22,094 epoch 1 - iter 22/117 - loss 0.69428441 - samples/sec: 17.50\n",
      "2020-05-24 16:42:43,909 epoch 1 - iter 33/117 - loss 0.69242128 - samples/sec: 16.19\n",
      "2020-05-24 16:43:08,382 epoch 1 - iter 44/117 - loss 0.67784121 - samples/sec: 14.85\n",
      "2020-05-24 16:43:29,958 epoch 1 - iter 55/117 - loss 0.67401460 - samples/sec: 16.36\n",
      "2020-05-24 16:43:49,654 epoch 1 - iter 66/117 - loss 0.66823633 - samples/sec: 17.92\n",
      "2020-05-24 16:44:12,963 epoch 1 - iter 77/117 - loss 0.66822653 - samples/sec: 15.14\n",
      "2020-05-24 16:44:32,273 epoch 1 - iter 88/117 - loss 0.66663722 - samples/sec: 19.02\n",
      "2020-05-24 16:44:53,397 epoch 1 - iter 99/117 - loss 0.67022049 - samples/sec: 16.72\n",
      "2020-05-24 16:45:16,356 epoch 1 - iter 110/117 - loss 0.66924216 - samples/sec: 15.37\n",
      "2020-05-24 16:45:32,523 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:45:32,524 EPOCH 1 done: loss 0.6649 - lr 0.1000000\n",
      "2020-05-24 16:46:03,866 DEV : loss 0.7192593812942505 - score 0.5022\n",
      "2020-05-24 16:46:04,070 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-05-24 16:46:07,140 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:46:28,472 epoch 2 - iter 11/117 - loss 0.68297398 - samples/sec: 16.59\n",
      "2020-05-24 16:46:49,683 epoch 2 - iter 22/117 - loss 0.66689447 - samples/sec: 16.65\n",
      "2020-05-24 16:47:08,603 epoch 2 - iter 33/117 - loss 0.65276714 - samples/sec: 18.68\n",
      "2020-05-24 16:47:30,495 epoch 2 - iter 44/117 - loss 0.64982150 - samples/sec: 16.13\n",
      "2020-05-24 16:47:59,306 epoch 2 - iter 55/117 - loss 0.64710541 - samples/sec: 12.25\n",
      "2020-05-24 16:48:26,360 epoch 2 - iter 66/117 - loss 0.63973053 - samples/sec: 13.04\n",
      "2020-05-24 16:48:51,819 epoch 2 - iter 77/117 - loss 0.63352901 - samples/sec: 14.49\n",
      "2020-05-24 16:49:11,910 epoch 2 - iter 88/117 - loss 0.63049927 - samples/sec: 17.58\n",
      "2020-05-24 16:49:31,787 epoch 2 - iter 99/117 - loss 0.63561533 - samples/sec: 17.76\n",
      "2020-05-24 16:49:59,499 epoch 2 - iter 110/117 - loss 0.63469473 - samples/sec: 12.73\n",
      "2020-05-24 16:50:10,759 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:50:10,766 EPOCH 2 done: loss 0.6310 - lr 0.1000000\n",
      "2020-05-24 16:50:46,963 DEV : loss 0.7386118769645691 - score 0.5259\n",
      "2020-05-24 16:50:47,163 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-05-24 16:50:50,029 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:51:16,445 epoch 3 - iter 11/117 - loss 0.60101805 - samples/sec: 13.38\n",
      "2020-05-24 16:51:37,783 epoch 3 - iter 22/117 - loss 0.59084967 - samples/sec: 16.56\n",
      "2020-05-24 16:51:58,768 epoch 3 - iter 33/117 - loss 0.60476210 - samples/sec: 16.84\n",
      "2020-05-24 16:52:20,934 epoch 3 - iter 44/117 - loss 0.60295659 - samples/sec: 15.93\n",
      "2020-05-24 16:52:38,526 epoch 3 - iter 55/117 - loss 0.61635514 - samples/sec: 21.00\n",
      "2020-05-24 16:53:04,323 epoch 3 - iter 66/117 - loss 0.61293271 - samples/sec: 13.68\n",
      "2020-05-24 16:53:19,718 epoch 3 - iter 77/117 - loss 0.61093556 - samples/sec: 22.99\n",
      "2020-05-24 16:53:38,527 epoch 3 - iter 88/117 - loss 0.61266717 - samples/sec: 18.78\n",
      "2020-05-24 16:53:59,442 epoch 3 - iter 99/117 - loss 0.61210733 - samples/sec: 16.89\n",
      "2020-05-24 16:54:21,522 epoch 3 - iter 110/117 - loss 0.60566508 - samples/sec: 15.98\n",
      "2020-05-24 16:54:38,528 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:54:38,529 EPOCH 3 done: loss 0.6030 - lr 0.1000000\n",
      "2020-05-24 16:55:16,696 DEV : loss 0.9432142376899719 - score 0.4849\n",
      "2020-05-24 16:55:16,954 BAD EPOCHS (no improvement): 1\n",
      "2020-05-24 16:55:16,957 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:55:38,517 epoch 4 - iter 11/117 - loss 0.58918658 - samples/sec: 16.41\n",
      "2020-05-24 16:56:06,463 epoch 4 - iter 22/117 - loss 0.60190228 - samples/sec: 12.63\n",
      "2020-05-24 16:56:29,793 epoch 4 - iter 33/117 - loss 0.58647567 - samples/sec: 15.14\n",
      "2020-05-24 16:56:49,572 epoch 4 - iter 44/117 - loss 0.58039266 - samples/sec: 17.86\n",
      "2020-05-24 16:57:10,149 epoch 4 - iter 55/117 - loss 0.56594196 - samples/sec: 17.16\n",
      "2020-05-24 16:57:30,933 epoch 4 - iter 66/117 - loss 0.56998557 - samples/sec: 16.99\n",
      "2020-05-24 16:57:59,469 epoch 4 - iter 77/117 - loss 0.56593655 - samples/sec: 12.37\n",
      "2020-05-24 16:58:17,344 epoch 4 - iter 88/117 - loss 0.57704548 - samples/sec: 19.76\n",
      "2020-05-24 16:58:37,631 epoch 4 - iter 99/117 - loss 0.57861140 - samples/sec: 17.42\n",
      "2020-05-24 16:58:59,877 epoch 4 - iter 110/117 - loss 0.58184447 - samples/sec: 16.69\n",
      "2020-05-24 16:59:11,243 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 16:59:11,244 EPOCH 4 done: loss 0.5808 - lr 0.1000000\n",
      "2020-05-24 16:59:39,472 DEV : loss 0.8019097447395325 - score 0.625\n",
      "2020-05-24 16:59:39,668 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-05-24 16:59:42,297 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:00:06,145 epoch 5 - iter 11/117 - loss 0.56030329 - samples/sec: 14.83\n",
      "2020-05-24 17:00:26,824 epoch 5 - iter 22/117 - loss 0.56216718 - samples/sec: 17.10\n",
      "2020-05-24 17:00:51,419 epoch 5 - iter 33/117 - loss 0.57832237 - samples/sec: 14.83\n",
      "2020-05-24 17:01:15,031 epoch 5 - iter 44/117 - loss 0.56841880 - samples/sec: 14.95\n",
      "2020-05-24 17:01:41,658 epoch 5 - iter 55/117 - loss 0.56744353 - samples/sec: 13.26\n",
      "2020-05-24 17:01:59,948 epoch 5 - iter 66/117 - loss 0.56151756 - samples/sec: 19.32\n",
      "2020-05-24 17:02:20,369 epoch 5 - iter 77/117 - loss 0.56124652 - samples/sec: 17.29\n",
      "2020-05-24 17:02:39,199 epoch 5 - iter 88/117 - loss 0.56270077 - samples/sec: 18.76\n",
      "2020-05-24 17:03:02,254 epoch 5 - iter 99/117 - loss 0.55721527 - samples/sec: 16.26\n",
      "2020-05-24 17:03:25,205 epoch 5 - iter 110/117 - loss 0.55636753 - samples/sec: 15.38\n",
      "2020-05-24 17:03:39,546 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:03:39,547 EPOCH 5 done: loss 0.5558 - lr 0.1000000\n",
      "2020-05-24 17:04:12,533 DEV : loss 0.5636223554611206 - score 0.7069\n",
      "2020-05-24 17:04:12,778 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-05-24 17:04:15,799 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:04:41,763 epoch 6 - iter 11/117 - loss 0.59740614 - samples/sec: 13.62\n",
      "2020-05-24 17:05:06,271 epoch 6 - iter 22/117 - loss 0.57424033 - samples/sec: 14.40\n",
      "2020-05-24 17:05:33,836 epoch 6 - iter 33/117 - loss 0.56795000 - samples/sec: 13.20\n",
      "2020-05-24 17:05:55,459 epoch 6 - iter 44/117 - loss 0.57015063 - samples/sec: 16.34\n",
      "2020-05-24 17:06:13,505 epoch 6 - iter 55/117 - loss 0.55505213 - samples/sec: 19.58\n",
      "2020-05-24 17:06:43,486 epoch 6 - iter 66/117 - loss 0.55239158 - samples/sec: 11.77\n",
      "2020-05-24 17:07:13,778 epoch 6 - iter 77/117 - loss 0.55297939 - samples/sec: 11.65\n",
      "2020-05-24 17:07:35,311 epoch 6 - iter 88/117 - loss 0.55040927 - samples/sec: 16.40\n",
      "2020-05-24 17:07:56,617 epoch 6 - iter 99/117 - loss 0.54500690 - samples/sec: 16.58\n",
      "2020-05-24 17:08:20,952 epoch 6 - iter 110/117 - loss 0.54198349 - samples/sec: 14.51\n",
      "2020-05-24 17:08:33,172 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:08:33,174 EPOCH 6 done: loss 0.5466 - lr 0.1000000\n",
      "2020-05-24 17:09:05,636 DEV : loss 0.5493430495262146 - score 0.7263\n",
      "2020-05-24 17:09:05,956 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-05-24 17:09:08,878 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:09:29,317 epoch 7 - iter 11/117 - loss 0.52695005 - samples/sec: 17.32\n",
      "2020-05-24 17:09:53,262 epoch 7 - iter 22/117 - loss 0.51861469 - samples/sec: 15.12\n",
      "2020-05-24 17:10:12,674 epoch 7 - iter 33/117 - loss 0.52935068 - samples/sec: 18.19\n",
      "2020-05-24 17:10:36,982 epoch 7 - iter 44/117 - loss 0.51718497 - samples/sec: 14.52\n",
      "2020-05-24 17:11:00,699 epoch 7 - iter 55/117 - loss 0.51978967 - samples/sec: 14.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 17:11:24,005 epoch 7 - iter 66/117 - loss 0.51994762 - samples/sec: 15.92\n",
      "2020-05-24 17:11:49,245 epoch 7 - iter 77/117 - loss 0.51758517 - samples/sec: 13.98\n",
      "2020-05-24 17:12:09,247 epoch 7 - iter 88/117 - loss 0.52253921 - samples/sec: 17.65\n",
      "2020-05-24 17:12:32,898 epoch 7 - iter 99/117 - loss 0.52428676 - samples/sec: 14.92\n",
      "2020-05-24 17:12:51,971 epoch 7 - iter 110/117 - loss 0.51994464 - samples/sec: 18.52\n",
      "2020-05-24 17:13:12,028 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:13:12,035 EPOCH 7 done: loss 0.5197 - lr 0.1000000\n",
      "2020-05-24 17:13:48,998 DEV : loss 1.6004676818847656 - score 0.4181\n",
      "2020-05-24 17:13:49,217 BAD EPOCHS (no improvement): 1\n",
      "2020-05-24 17:13:49,224 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:14:12,126 epoch 8 - iter 11/117 - loss 0.64657290 - samples/sec: 15.46\n",
      "2020-05-24 17:14:30,924 epoch 8 - iter 22/117 - loss 0.56181443 - samples/sec: 18.81\n",
      "2020-05-24 17:14:48,683 epoch 8 - iter 33/117 - loss 0.58387581 - samples/sec: 19.91\n",
      "2020-05-24 17:15:06,931 epoch 8 - iter 44/117 - loss 0.56100546 - samples/sec: 19.37\n",
      "2020-05-24 17:15:27,183 epoch 8 - iter 55/117 - loss 0.54486730 - samples/sec: 18.17\n",
      "2020-05-24 17:15:47,222 epoch 8 - iter 66/117 - loss 0.54410925 - samples/sec: 17.63\n",
      "2020-05-24 17:16:14,600 epoch 8 - iter 77/117 - loss 0.53692037 - samples/sec: 12.89\n",
      "2020-05-24 17:16:33,999 epoch 8 - iter 88/117 - loss 0.53279875 - samples/sec: 18.21\n",
      "2020-05-24 17:16:50,790 epoch 8 - iter 99/117 - loss 0.53015632 - samples/sec: 21.05\n",
      "2020-05-24 17:17:09,914 epoch 8 - iter 110/117 - loss 0.53265487 - samples/sec: 19.33\n",
      "2020-05-24 17:17:19,820 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:17:19,820 EPOCH 8 done: loss 0.5333 - lr 0.1000000\n",
      "2020-05-24 17:17:48,455 DEV : loss 0.5582072734832764 - score 0.7328\n",
      "2020-05-24 17:17:48,657 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-05-24 17:17:51,276 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:18:09,347 epoch 9 - iter 11/117 - loss 0.50018499 - samples/sec: 19.59\n",
      "2020-05-24 17:18:25,731 epoch 9 - iter 22/117 - loss 0.50062181 - samples/sec: 21.59\n",
      "2020-05-24 17:18:45,046 epoch 9 - iter 33/117 - loss 0.50442699 - samples/sec: 18.29\n",
      "2020-05-24 17:19:04,858 epoch 9 - iter 44/117 - loss 0.50721900 - samples/sec: 18.65\n",
      "2020-05-24 17:19:26,173 epoch 9 - iter 55/117 - loss 0.51618802 - samples/sec: 16.57\n",
      "2020-05-24 17:19:53,708 epoch 9 - iter 66/117 - loss 0.51729644 - samples/sec: 12.82\n",
      "2020-05-24 17:20:21,409 epoch 9 - iter 77/117 - loss 0.51486435 - samples/sec: 12.74\n",
      "2020-05-24 17:20:40,147 epoch 9 - iter 88/117 - loss 0.51013853 - samples/sec: 18.85\n",
      "2020-05-24 17:21:04,510 epoch 9 - iter 99/117 - loss 0.51307353 - samples/sec: 15.02\n",
      "2020-05-24 17:21:29,544 epoch 9 - iter 110/117 - loss 0.51116739 - samples/sec: 14.10\n",
      "2020-05-24 17:21:39,236 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:21:39,237 EPOCH 9 done: loss 0.5090 - lr 0.1000000\n",
      "2020-05-24 17:22:13,398 DEV : loss 0.624588668346405 - score 0.6961\n",
      "2020-05-24 17:22:13,605 BAD EPOCHS (no improvement): 1\n",
      "2020-05-24 17:22:13,612 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:22:33,942 epoch 10 - iter 11/117 - loss 0.48576158 - samples/sec: 17.40\n",
      "2020-05-24 17:22:53,786 epoch 10 - iter 22/117 - loss 0.47863473 - samples/sec: 17.81\n",
      "2020-05-24 17:23:16,548 epoch 10 - iter 33/117 - loss 0.49771777 - samples/sec: 16.01\n",
      "2020-05-24 17:23:36,494 epoch 10 - iter 44/117 - loss 0.50350817 - samples/sec: 17.71\n",
      "2020-05-24 17:24:01,884 epoch 10 - iter 55/117 - loss 0.49575015 - samples/sec: 13.90\n",
      "2020-05-24 17:24:26,033 epoch 10 - iter 66/117 - loss 0.49612824 - samples/sec: 14.62\n",
      "2020-05-24 17:24:51,245 epoch 10 - iter 77/117 - loss 0.48928730 - samples/sec: 14.01\n",
      "2020-05-24 17:25:19,187 epoch 10 - iter 88/117 - loss 0.49164211 - samples/sec: 13.17\n",
      "2020-05-24 17:25:39,064 epoch 10 - iter 99/117 - loss 0.49286634 - samples/sec: 17.77\n",
      "2020-05-24 17:26:02,084 epoch 10 - iter 110/117 - loss 0.49714684 - samples/sec: 15.34\n",
      "2020-05-24 17:26:13,878 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:26:13,879 EPOCH 10 done: loss 0.4998 - lr 0.1000000\n",
      "2020-05-24 17:26:47,054 DEV : loss 0.529371976852417 - score 0.7414\n",
      "2020-05-24 17:26:47,285 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-05-24 17:26:53,073 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-24 17:26:53,074 Testing using best model ...\n",
      "2020-05-24 17:26:53,078 loading file best-model.pt\n",
      "2020-05-24 17:27:15,552 0.771551724137931\t0.771551724137931\t0.771551724137931\n",
      "2020-05-24 17:27:15,555 \n",
      "MICRO_AVG: acc 0.771551724137931 - f1-score 0.771551724137931\n",
      "MACRO_AVG: acc 0.771551724137931 - f1-score 0.7655895529501477\n",
      "__label__No tp: 216 - fp: 74 - fn: 32 - tn: 142 - precision: 0.7448 - recall: 0.8710 - accuracy: 0.7716 - f1-score: 0.8030\n",
      "__label__Yes tp: 142 - fp: 32 - fn: 74 - tn: 216 - precision: 0.8161 - recall: 0.6574 - accuracy: 0.7716 - f1-score: 0.7282\n",
      "2020-05-24 17:27:15,555 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.771551724137931,\n",
       " 'dev_score_history': [0.5021551724137931,\n",
       "  0.5258620689655172,\n",
       "  0.4849137931034483,\n",
       "  0.625,\n",
       "  0.7068965517241379,\n",
       "  0.7262931034482759,\n",
       "  0.41810344827586204,\n",
       "  0.7327586206896551,\n",
       "  0.6961206896551724,\n",
       "  0.7413793103448276],\n",
       " 'train_loss_history': [0.6649040309791892,\n",
       "  0.6310247648984958,\n",
       "  0.6030245005575001,\n",
       "  0.5808376736111112,\n",
       "  0.5557609664069282,\n",
       "  0.5466362495198209,\n",
       "  0.5196669252000303,\n",
       "  0.5333108894335918,\n",
       "  0.5089793034598359,\n",
       "  0.4998229920354664],\n",
       " 'dev_loss_history': [0.7192593812942505,\n",
       "  0.7386118769645691,\n",
       "  0.9432142376899719,\n",
       "  0.8019097447395325,\n",
       "  0.5636223554611206,\n",
       "  0.5493430495262146,\n",
       "  1.6004676818847656,\n",
       "  0.5582072734832764,\n",
       "  0.624588668346405,\n",
       "  0.529371976852417]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using Flair model\n",
    "\n",
    "word_embeddings = [WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')]\n",
    "document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "trainer.train('./', max_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flair Test Score with default params: 0.77155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastAI: ULMFiT - Discriminative fine-tuning, Slanted triangular learning rates, and Gradual unfreezing\n",
    "\n",
    "* Explanation - https://arxiv.org/pdf/1801.06146.pdf\n",
    "* Easy Implementation - https://www.analyticsvidhya.com/blog/2018/11/tutorial-text-classification-ulmfit-fastai-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FB's FastText\n",
    "\n",
    "* Model - https://github.com/facebookresearch/fastText\n",
    "* Explanation - https://arxiv.org/pdf/1607.01759.pdf\n",
    "* Easy Implementation - https://idevji.com/2017/11/04/tutorial-text-classification-with-python-using-fasttext/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset with FastText: 80.65\n"
     ]
    }
   ],
   "source": [
    "ft_model = fasttext.train_supervised(\"../data/practice_model/train.csv\", epoch=10, loss='hs')\n",
    "results = ft_model.test(\"../data/practice_model/dev.csv\")\n",
    "\n",
    "# print(\"Precision on dev dataset: {:.2f}\".format(results[1]))\n",
    "# print(\"Recall on dev dataset: {:.2f}\".format(results[2]))\n",
    "\n",
    "test_data = pd.read_csv(\"../data/practice_model/test.csv\", sep='\\t', header=None)\n",
    "\n",
    "# Evaluation\n",
    "ft_y_pred = test_data[1].apply(lambda x: ft_model.predict(x)[0][0])\n",
    "ft_acc = accuracy_score(test_data[0], ft_y_pred)\n",
    "\n",
    "print(\"Accuracy on the test dataset with FastText: {:.2f}\".format(ft_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Transformers\n",
    "\n",
    "* Model - https://github.com/huggingface/transformers\n",
    "* Explanation - \n",
    "* Easy Implementation - https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAM: Explicit Interaction Mechanism towards Text Classification (Encode, Interaction, Aggregation)\n",
    "\n",
    "* Model - https://github.com/NonvolatileMemory/AAAI_2019_EXAM\n",
    "* Explanation - https://arxiv.org/pdf/1811.09386.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
